#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ” ì´ìƒíƒì§€ ë° ì˜ˆì¸¡ API ì—”ë“œí¬ì¸íŠ¸ (í•œêµ­ì–´ ì£¼ì„ í¬í•¨)

FastAPI ë¼ìš°í„°ë¡œ ì´ìƒíƒì§€ ë¶„ì„ ê²°ê³¼ë¥¼ RESTful APIë¡œ ì œê³µ
ìºì‹œ ë° RBAC(Role-Based Access Control) ì ìš©

ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸:
- GET /api/v1/anomaly/summary          # ìµœì‹  ìš”ì•½ ì •ë³´
- GET /api/v1/anomaly/timeseries       # ì‹œê³„ì—´ ì´ìƒì¹˜ ë§ˆí‚¹ ë°ì´í„°
- GET /api/v1/anomaly/forecast         # 7ì¼ ì˜ˆì¸¡ ì •ë³´
- POST /api/v1/anomaly/run             # ë¶„ì„ ì‹¤í–‰ íŠ¸ë¦¬ê±°
- GET /api/v1/anomaly/report.md        # Markdown ë¦¬í¬íŠ¸
- GET /api/v1/anomaly/health           # í—¬ìŠ¤ì²´í¬

ì‘ì„±ì: MCP Map Company ìš´ì˜íŒ€
ìƒì„±ì¼: 2024ë…„ ì´ìƒíƒì§€ ì‹œìŠ¤í…œ í”„ë¡œì íŠ¸
"""

import asyncio
import json
import logging
import os
import subprocess
import tempfile
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from fastapi import APIRouter, Depends, HTTPException, Query, Request, Response
from fastapi.responses import PlainTextResponse, JSONResponse
from pydantic import BaseModel, Field
import aiofiles

# ë‚´ë¶€ ëª¨ë“ˆ
from .auth import require_role, get_current_user, RoleEnum
from .cache import cache_response, get_cached_data, set_cache_data

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ================================================================
# ë°ì´í„° ëª¨ë¸ (Pydantic Models) - í•œêµ­ì–´ ì£¼ì„ í¬í•¨
# ================================================================

class AnomalySummary(BaseModel):
    """ì´ìƒíƒì§€ ìš”ì•½ ì •ë³´ ëª¨ë¸"""
    total_anomalies: int = Field(..., description="ì´ ì´ìƒì¹˜ ê°œìˆ˜")
    high_risk_metrics: int = Field(..., description="ê³ ìœ„í—˜ ë©”íŠ¸ë¦­ ìˆ˜")
    future_risk_periods: int = Field(..., description="7ì¼ ì˜ˆì¸¡ ìœ„í—˜ êµ¬ê°„ ìˆ˜")
    confidence_score: float = Field(..., ge=0, le=1, description="ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜")
    last_updated: str = Field(..., description="ë§ˆì§€ë§‰ ë¶„ì„ ì‹œê°„ (ISO í˜•ì‹)")
    analysis_period_days: int = Field(..., description="ë¶„ì„ ê¸°ê°„ (ì¼)")
    metrics_analyzed: int = Field(..., description="ë¶„ì„ëœ ë©”íŠ¸ë¦­ ìˆ˜")

class AnomalyPoint(BaseModel):
    """ê°œë³„ ì´ìƒì¹˜ í¬ì¸íŠ¸ ëª¨ë¸"""
    timestamp: str = Field(..., description="ì´ìƒì¹˜ ë°œìƒ ì‹œê°„")
    metric: str = Field(..., description="ë©”íŠ¸ë¦­ ì´ë¦„")
    value: float = Field(..., description="ì‹¤ì œ ê°’")
    expected_value: float = Field(..., description="ì˜ˆìƒ ê°’ (EWMA)")
    z_score: float = Field(..., description="Z-score ê°’")
    severity: str = Field(..., description="ì‹¬ê°ë„ (low/medium/high/critical)")
    direction: str = Field(..., description="ë°©í–¥ (increase/decrease)")

class TimeSeriesData(BaseModel):
    """ì‹œê³„ì—´ ë°ì´í„° ëª¨ë¸"""
    metric: str = Field(..., description="ë©”íŠ¸ë¦­ ì´ë¦„")
    timestamps: List[str] = Field(..., description="ì‹œê°„ ë°°ì—´")
    values: List[float] = Field(..., description="ê°’ ë°°ì—´")
    expected_values: List[float] = Field(..., description="ì˜ˆìƒ ê°’ ë°°ì—´ (EWMA)")
    anomaly_points: List[AnomalyPoint] = Field(..., description="ì´ìƒì¹˜ í¬ì¸íŠ¸ë“¤")
    risk_level: str = Field(..., description="ìœ„í—˜ë„ ìˆ˜ì¤€")

class ForecastPoint(BaseModel):
    """ì˜ˆì¸¡ í¬ì¸íŠ¸ ëª¨ë¸"""
    date: str = Field(..., description="ì˜ˆì¸¡ ë‚ ì§œ")
    predicted_value: float = Field(..., description="ì˜ˆì¸¡ ê°’")
    lower_bound: float = Field(..., description="ì‹ ë¢°êµ¬ê°„ í•˜í•œ")
    upper_bound: float = Field(..., description="ì‹ ë¢°êµ¬ê°„ ìƒí•œ")
    confidence: float = Field(..., ge=0, le=1, description="ì‹ ë¢°ë„")

class ForecastData(BaseModel):
    """ì˜ˆì¸¡ ë°ì´í„° ëª¨ë¸"""
    metric: str = Field(..., description="ë©”íŠ¸ë¦­ ì´ë¦„")
    forecast_points: List[ForecastPoint] = Field(..., description="ì˜ˆì¸¡ í¬ì¸íŠ¸ë“¤")
    trend_direction: str = Field(..., description="ì¶”ì„¸ ë°©í–¥ (increasing/decreasing)")
    trend_strength: float = Field(..., description="ì¶”ì„¸ ê°•ë„")
    model_accuracy: float = Field(..., ge=0, le=1, description="ëª¨ë¸ ì •í™•ë„")
    risk_periods: List[Dict[str, Any]] = Field(..., description="ìœ„í—˜ ì˜ˆì¸¡ êµ¬ê°„ë“¤")

class AnalysisRequest(BaseModel):
    """ë¶„ì„ ì‹¤í–‰ ìš”ì²­ ëª¨ë¸"""
    days: Optional[int] = Field(30, ge=1, le=365, description="ë¶„ì„ ê¸°ê°„ (ì¼)")
    threshold: Optional[float] = Field(3.0, ge=1.0, le=5.0, description="Z-score ì„ê³„ì¹˜")
    window_size: Optional[int] = Field(7, ge=3, le=30, description="ë¡¤ë§ ìœˆë„ìš° í¬ê¸°")
    forecast_days: Optional[int] = Field(7, ge=1, le=30, description="ì˜ˆì¸¡ ê¸°ê°„ (ì¼)")
    force_refresh: Optional[bool] = Field(False, description="ìºì‹œ ë¬´ì‹œ ê°•ì œ ì‹¤í–‰")

class HealthStatus(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ìƒíƒœ ëª¨ë¸"""
    status: str = Field(..., description="ì„œë¹„ìŠ¤ ìƒíƒœ (healthy/degraded/unhealthy)")
    last_analysis_time: Optional[str] = Field(None, description="ë§ˆì§€ë§‰ ë¶„ì„ ì‹œê°„")
    data_sources_available: Dict[str, bool] = Field(..., description="ë°ì´í„° ì†ŒìŠ¤ë³„ ê°€ìš©ì„±")
    cache_hit_rate: float = Field(..., description="ìºì‹œ íˆíŠ¸ìœ¨")
    avg_response_time_ms: float = Field(..., description="í‰ê·  ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")

# ================================================================
# FastAPI ë¼ìš°í„° ìƒì„±
# ================================================================

router = APIRouter(
    prefix="/api/v1/anomaly",
    tags=["anomaly"],
    responses={
        404: {"description": "ìš”ì²­í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        403: {"description": "ì ‘ê·¼ ê¶Œí•œ ì—†ìŒ"},
        500: {"description": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜"}
    }
)

# ================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (í•œêµ­ì–´ ì£¼ì„ í¬í•¨)
# ================================================================

async def run_anomaly_script(
    days: int = 30,
    threshold: float = 3.0,
    window: int = 7,
    forecast: int = 7,
    output_format: str = "json"
) -> Optional[str]:
    """
    ì´ìƒíƒì§€ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰

    Args:
        days: ë¶„ì„ ê¸°ê°„ (ì¼)
        threshold: Z-score ì„ê³„ì¹˜
        window: ë¡¤ë§ ìœˆë„ìš° í¬ê¸°
        forecast: ì˜ˆì¸¡ ê¸°ê°„
        output_format: ì¶œë ¥ í˜•ì‹ (json/markdown)

    Returns:
        str: ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê²°ê³¼ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
    """
    try:
        script_path = Path(__file__).parent.parent / "scripts" / "anomaly_detect.py"

        if not script_path.exists():
            logger.error(f"ì´ìƒíƒì§€ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {script_path}")
            return None

        # ì„ì‹œ ì¶œë ¥ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(mode='w', suffix=f'.{output_format}', delete=False) as temp_file:
            temp_output_path = temp_file.name

        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ëª…ë ¹ êµ¬ì„±
        cmd = [
            "python3", str(script_path),
            "--days", str(days),
            "--threshold", str(threshold),
            "--window", str(window),
            "--forecast", str(forecast),
            "--output", temp_output_path
        ]

        if output_format == "json":
            cmd.append("--json")
        elif output_format == "markdown":
            cmd.append("--md")

        logger.info(f"ì´ìƒíƒì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: {' '.join(cmd)}")

        # ë¹„ë™ê¸° í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=script_path.parent.parent
        )

        # íƒ€ì„ì•„ì›ƒ 60ì´ˆë¡œ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
        try:
            stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=60.0)
        except asyncio.TimeoutError:
            process.kill()
            logger.error("ì´ìƒíƒì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ (60ì´ˆ)")
            return None

        if process.returncode == 0:
            # ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë¨ - ê²°ê³¼ íŒŒì¼ ì½ê¸°
            try:
                async with aiofiles.open(temp_output_path, 'r', encoding='utf-8') as f:
                    result = await f.read()

                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                os.unlink(temp_output_path)

                logger.info(f"ì´ìƒíƒì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì„±ê³µ ({len(result)} bytes)")
                return result

            except Exception as e:
                logger.error(f"ê²°ê³¼ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
                return None
        else:
            # ì‹¤í–‰ ì‹¤íŒ¨
            error_msg = stderr.decode('utf-8') if stderr else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
            logger.error(f"ì´ìƒíƒì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨ (ì½”ë“œ {process.returncode}): {error_msg}")
            return None

    except Exception as e:
        logger.error(f"ì´ìƒíƒì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return None

def parse_analysis_result(result_json: str) -> Optional[Dict[str, Any]]:
    """
    ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê²°ê³¼ JSONì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜

    Args:
        result_json: JSON í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼

    Returns:
        Dict: íŒŒì‹±ëœ ë¶„ì„ ê²°ê³¼
    """
    try:
        data = json.loads(result_json)

        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ['metadata', 'summary', 'top_anomalies', 'risk_levels']
        for field in required_fields:
            if field not in data:
                logger.warning(f"ë¶„ì„ ê²°ê³¼ì— í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                return None

        return data

    except json.JSONDecodeError as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return None
    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def check_data_source_availability() -> Dict[str, bool]:
    """
    ë°ì´í„° ì†ŒìŠ¤ë“¤ì˜ ê°€ìš©ì„± í™•ì¸

    Returns:
        Dict[str, bool]: ì†ŒìŠ¤ë³„ ê°€ìš©ì„± ìƒíƒœ
    """
    sources = {
        "metrics_json": False,
        "log_files": False,
        "ci_reports": False
    }

    try:
        base_path = Path(__file__).parent.parent

        # ë©”íŠ¸ë¦­ JSON íŒŒì¼ë“¤ í™•ì¸
        metrics_dir = base_path / "reports" / "metrics"
        if metrics_dir.exists() and any(metrics_dir.glob("*.json")):
            sources["metrics_json"] = True

        # ë¡œê·¸ íŒŒì¼ë“¤ í™•ì¸
        logs_dir = base_path / "logs"
        if logs_dir.exists() and any(logs_dir.glob("*.log")):
            sources["log_files"] = True

        # CI ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ í™•ì¸
        ci_dir = base_path / "reports" / "ci_reports"
        if ci_dir.exists() and any(ci_dir.glob("*.json")):
            sources["ci_reports"] = True

    except Exception as e:
        logger.warning(f"ë°ì´í„° ì†ŒìŠ¤ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    return sources

# ================================================================
# API ì—”ë“œí¬ì¸íŠ¸ë“¤ (í•œêµ­ì–´ ì£¼ì„ í¬í•¨)
# ================================================================

@router.get("/health", response_model=HealthStatus,
           summary="í—¬ìŠ¤ì²´í¬", description="ì´ìƒíƒì§€ ì„œë¹„ìŠ¤ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤")
async def health_check():
    """
    ì´ìƒíƒì§€ ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬

    ì ‘ê·¼ ê¶Œí•œ: ëª¨ë“  ì‚¬ìš©ì
    ìºì‹œ: 30ì´ˆ
    """
    try:
        # ìºì‹œ í™•ì¸
        cached_health = await get_cached_data("anomaly_health")
        if cached_health:
            return JSONResponse(cached_health)

        # ë°ì´í„° ì†ŒìŠ¤ ê°€ìš©ì„± í™•ì¸
        data_sources = check_data_source_availability()

        # ë§ˆì§€ë§‰ ë¶„ì„ ì‹œê°„ í™•ì¸
        last_analysis_cache = await get_cached_data("anomaly_last_analysis")
        last_analysis_time = last_analysis_cache.get("timestamp") if last_analysis_cache else None

        # ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚° (ê°„ë‹¨í•œ ì¶”ì •)
        cache_hit_rate = 0.75  # ì‹¤ì œë¡œëŠ” ìºì‹œ í†µê³„ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨

        # ì „ì²´ ìƒíƒœ íŒì •
        available_sources = sum(data_sources.values())
        if available_sources >= 2:
            status = "healthy"
        elif available_sources == 1:
            status = "degraded"
        else:
            status = "unhealthy"

        health_data = {
            "status": status,
            "last_analysis_time": last_analysis_time,
            "data_sources_available": data_sources,
            "cache_hit_rate": cache_hit_rate,
            "avg_response_time_ms": 150.0  # ì‹¤ì œë¡œëŠ” ë©”íŠ¸ë¦­ì—ì„œ ìˆ˜ì§‘
        }

        # ìºì‹œì— 30ì´ˆê°„ ì €ì¥
        await set_cache_data("anomaly_health", health_data, ttl=30)

        return JSONResponse(health_data)

    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail="í—¬ìŠ¤ì²´í¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

@router.get("/summary", response_model=AnomalySummary,
           summary="ì´ìƒíƒì§€ ìš”ì•½", description="ìµœì‹  ì´ìƒíƒì§€ ë¶„ì„ ìš”ì•½ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤")
async def get_anomaly_summary(
    current_user = Depends(require_role([RoleEnum.ADMIN, RoleEnum.OPERATOR]))
):
    """
    ì´ìƒíƒì§€ ë¶„ì„ ìš”ì•½ ì •ë³´ ì¡°íšŒ

    ì ‘ê·¼ ê¶Œí•œ: admin, operator
    ìºì‹œ: 5ë¶„
    """
    try:
        # ìºì‹œ í™•ì¸ (5ë¶„)
        cached_summary = await get_cached_data("anomaly_summary")
        if cached_summary:
            return JSONResponse(cached_summary)

        logger.info("ì´ìƒíƒì§€ ìš”ì•½ ì •ë³´ ìƒì„± ì‹œì‘")

        # ìµœì‹  ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        result_json = await run_anomaly_script(days=30, output_format="json")
        if not result_json:
            raise HTTPException(status_code=503, detail="ì´ìƒíƒì§€ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨")

        analysis_data = parse_analysis_result(result_json)
        if not analysis_data:
            raise HTTPException(status_code=502, detail="ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨")

        # ìš”ì•½ ì •ë³´ ì¶”ì¶œ
        metadata = analysis_data.get("metadata", {})
        summary = analysis_data.get("summary", {})

        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (ì˜ˆì¸¡ ëª¨ë¸ ì •í™•ë„ í‰ê· )
        forecasts = analysis_data.get("forecasts", {})
        accuracy_scores = [f.get("model_accuracy", 0.5) for f in forecasts.values()]
        confidence_score = sum(accuracy_scores) / len(accuracy_scores) if accuracy_scores else 0.5

        summary_data = {
            "total_anomalies": summary.get("total_anomalies", 0),
            "high_risk_metrics": summary.get("high_risk_metrics", 0),
            "future_risk_periods": summary.get("total_future_risk_periods", 0),
            "confidence_score": confidence_score,
            "last_updated": metadata.get("generated_at", datetime.now().isoformat()),
            "analysis_period_days": metadata.get("analysis_window_days", 30),
            "metrics_analyzed": metadata.get("total_metrics_analyzed", 0)
        }

        # ìºì‹œì— 5ë¶„ê°„ ì €ì¥
        await set_cache_data("anomaly_summary", summary_data, ttl=300)

        logger.info("ì´ìƒíƒì§€ ìš”ì•½ ì •ë³´ ìƒì„± ì™„ë£Œ")
        return JSONResponse(summary_data)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìš”ì•½ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail="ìš”ì•½ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

@router.get("/timeseries", response_model=List[TimeSeriesData],
           summary="ì‹œê³„ì—´ ì´ìƒì¹˜ ë°ì´í„°", description="ë©”íŠ¸ë¦­ë³„ ì‹œê³„ì—´ ë°ì´í„°ì™€ ì´ìƒì¹˜ ë§ˆí‚¹ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤")
async def get_timeseries_data(
    metric: Optional[str] = Query(None, description="íŠ¹ì • ë©”íŠ¸ë¦­ í•„í„°ë§"),
    days: int = Query(7, ge=1, le=30, description="ì¡°íšŒ ê¸°ê°„ (ì¼)"),
    current_user = Depends(require_role([RoleEnum.ADMIN, RoleEnum.OPERATOR]))
):
    """
    ì‹œê³„ì—´ ì´ìƒì¹˜ ë§ˆí‚¹ ë°ì´í„° ì¡°íšŒ

    ì ‘ê·¼ ê¶Œí•œ: admin, operator
    ìºì‹œ: 5ë¶„
    """
    try:
        cache_key = f"anomaly_timeseries_{metric or 'all'}_{days}"
        cached_data = await get_cached_data(cache_key)
        if cached_data:
            return JSONResponse(cached_data)

        logger.info(f"ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ: metric={metric}, days={days}")

        # ë¶„ì„ ì‹¤í–‰
        result_json = await run_anomaly_script(days=days, output_format="json")
        if not result_json:
            raise HTTPException(status_code=503, detail="ì´ìƒíƒì§€ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨")

        analysis_data = parse_analysis_result(result_json)
        if not analysis_data:
            raise HTTPException(status_code=502, detail="ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨")

        # ì‹œê³„ì—´ ë°ì´í„° êµ¬ì„±
        timeseries_list = []

        anomalies = analysis_data.get("detailed_anomalies", {})
        risk_levels = analysis_data.get("risk_levels", {})

        for metric_name, anomaly_list in anomalies.items():
            # íŠ¹ì • ë©”íŠ¸ë¦­ í•„í„°ë§
            if metric and metric not in metric_name:
                continue

            # ì´ìƒì¹˜ í¬ì¸íŠ¸ë“¤ì„ AnomalyPoint ëª¨ë¸ë¡œ ë³€í™˜
            anomaly_points = []
            timestamps = []
            values = []
            expected_values = []

            for anomaly in anomaly_list:
                point = AnomalyPoint(
                    timestamp=anomaly["timestamp"],
                    metric=metric_name,
                    value=anomaly["value"],
                    expected_value=anomaly["ewma"],
                    z_score=anomaly["z_score"],
                    severity=anomaly["severity"],
                    direction=anomaly["direction"]
                )
                anomaly_points.append(point)
                timestamps.append(anomaly["timestamp"])
                values.append(anomaly["value"])
                expected_values.append(anomaly["ewma"])

            if anomaly_points:  # ì´ìƒì¹˜ê°€ ìˆëŠ” ë©”íŠ¸ë¦­ë§Œ í¬í•¨
                timeseries_data = TimeSeriesData(
                    metric=metric_name,
                    timestamps=timestamps,
                    values=values,
                    expected_values=expected_values,
                    anomaly_points=anomaly_points,
                    risk_level=risk_levels.get(metric_name, "low")
                )
                timeseries_list.append(timeseries_data.dict())

        # ìºì‹œì— 5ë¶„ê°„ ì €ì¥
        await set_cache_data(cache_key, timeseries_list, ttl=300)

        logger.info(f"ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(timeseries_list)}ê°œ ë©”íŠ¸ë¦­")
        return JSONResponse(timeseries_list)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail="ì‹œê³„ì—´ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

@router.get("/forecast", response_model=List[ForecastData],
           summary="ì˜ˆì¸¡ ë°ì´í„°", description="í–¥í›„ 7ì¼ê°„ì˜ ë©”íŠ¸ë¦­ ì˜ˆì¸¡ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤")
async def get_forecast_data(
    metric: Optional[str] = Query(None, description="íŠ¹ì • ë©”íŠ¸ë¦­ í•„í„°ë§"),
    forecast_days: int = Query(7, ge=1, le=30, description="ì˜ˆì¸¡ ê¸°ê°„ (ì¼)"),
    current_user = Depends(require_role([RoleEnum.ADMIN, RoleEnum.OPERATOR]))
):
    """
    ë©”íŠ¸ë¦­ ì˜ˆì¸¡ ë°ì´í„° ì¡°íšŒ

    ì ‘ê·¼ ê¶Œí•œ: admin, operator
    ìºì‹œ: 10ë¶„
    """
    try:
        cache_key = f"anomaly_forecast_{metric or 'all'}_{forecast_days}"
        cached_data = await get_cached_data(cache_key)
        if cached_data:
            return JSONResponse(cached_data)

        logger.info(f"ì˜ˆì¸¡ ë°ì´í„° ì¡°íšŒ: metric={metric}, forecast_days={forecast_days}")

        # ë¶„ì„ ì‹¤í–‰
        result_json = await run_anomaly_script(forecast=forecast_days, output_format="json")
        if not result_json:
            raise HTTPException(status_code=503, detail="ì´ìƒíƒì§€ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨")

        analysis_data = parse_analysis_result(result_json)
        if not analysis_data:
            raise HTTPException(status_code=502, detail="ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨")

        # ì˜ˆì¸¡ ë°ì´í„° êµ¬ì„±
        forecast_list = []

        forecasts = analysis_data.get("forecasts", {})

        for metric_name, forecast_info in forecasts.items():
            # íŠ¹ì • ë©”íŠ¸ë¦­ í•„í„°ë§
            if metric and metric not in metric_name:
                continue

            # ì˜ˆì¸¡ í¬ì¸íŠ¸ë“¤ì„ ForecastPoint ëª¨ë¸ë¡œ ë³€í™˜
            forecast_points = []
            for point in forecast_info.get("forecast_points", []):
                forecast_point = ForecastPoint(
                    date=point["date"],
                    predicted_value=point["predicted_value"],
                    lower_bound=point["lower_bound"],
                    upper_bound=point["upper_bound"],
                    confidence=point["confidence"]
                )
                forecast_points.append(forecast_point)

            if forecast_points:  # ì˜ˆì¸¡ ë°ì´í„°ê°€ ìˆëŠ” ë©”íŠ¸ë¦­ë§Œ í¬í•¨
                forecast_data = ForecastData(
                    metric=metric_name,
                    forecast_points=[p.dict() for p in forecast_points],
                    trend_direction=forecast_info.get("trend_direction", "stable"),
                    trend_strength=forecast_info.get("trend_strength", 0.0),
                    model_accuracy=forecast_info.get("model_accuracy", 0.5),
                    risk_periods=forecast_info.get("risk_periods", [])
                )
                forecast_list.append(forecast_data.dict())

        # ìºì‹œì— 10ë¶„ê°„ ì €ì¥
        await set_cache_data(cache_key, forecast_list, ttl=600)

        logger.info(f"ì˜ˆì¸¡ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(forecast_list)}ê°œ ë©”íŠ¸ë¦­")
        return JSONResponse(forecast_list)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail="ì˜ˆì¸¡ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

@router.post("/run", summary="ë¶„ì„ ì‹¤í–‰", description="ì´ìƒíƒì§€ ë¶„ì„ì„ ì¦‰ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤")
async def run_analysis(
    request: AnalysisRequest,
    current_user = Depends(require_role([RoleEnum.ADMIN, RoleEnum.OPERATOR]))
):
    """
    ì´ìƒíƒì§€ ë¶„ì„ ì¦‰ì‹œ ì‹¤í–‰

    ì ‘ê·¼ ê¶Œí•œ: admin, operator
    ë¹„ë™ê¸° ì‹¤í–‰ìœ¼ë¡œ ë¹ ë¥¸ ì‘ë‹µ
    """
    try:
        logger.info(f"ì´ìƒíƒì§€ ë¶„ì„ ì‹¤í–‰ ìš”ì²­: {request.dict()}")

        # ìºì‹œ ë¬´íš¨í™” (force_refreshê°€ Trueì¸ ê²½ìš°)
        if request.force_refresh:
            cache_keys = [
                "anomaly_summary",
                "anomaly_timeseries_all_*",
                "anomaly_forecast_all_*"
            ]
            for key_pattern in cache_keys:
                # ì‹¤ì œë¡œëŠ” íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìºì‹œ ì‚­ì œí•´ì•¼ í•¨
                pass

        # ë¶„ì„ ì‹¤í–‰ (ë¹„ë™ê¸°)
        result_json = await run_anomaly_script(
            days=request.days,
            threshold=request.threshold,
            window=request.window_size,
            forecast=request.forecast_days,
            output_format="json"
        )

        if not result_json:
            raise HTTPException(status_code=503, detail="ì´ìƒíƒì§€ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨")

        analysis_data = parse_analysis_result(result_json)
        if not analysis_data:
            raise HTTPException(status_code=502, detail="ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨")

        # ë¶„ì„ ì™„ë£Œ ì‹œê°„ ìºì‹œ ì—…ë°ì´íŠ¸
        await set_cache_data("anomaly_last_analysis", {
            "timestamp": datetime.now().isoformat(),
            "parameters": request.dict()
        }, ttl=3600)

        # ê²°ê³¼ ìš”ì•½ ë°˜í™˜
        summary = analysis_data.get("summary", {})

        return JSONResponse({
            "status": "completed",
            "message": "ì´ìƒíƒì§€ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
            "execution_time": datetime.now().isoformat(),
            "parameters": request.dict(),
            "results_preview": {
                "total_anomalies": summary.get("total_anomalies", 0),
                "high_risk_metrics": summary.get("high_risk_metrics", 0),
                "future_risk_periods": summary.get("total_future_risk_periods", 0)
            }
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail="ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

@router.get("/report.md", response_class=PlainTextResponse,
           summary="Markdown ë¦¬í¬íŠ¸", description="ìµœì‹  ì´ìƒíƒì§€ ë¶„ì„ ê²°ê³¼ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤")
async def get_markdown_report(
    days: int = Query(30, ge=1, le=365, description="ë¶„ì„ ê¸°ê°„ (ì¼)"),
    current_user = Depends(require_role([RoleEnum.ADMIN, RoleEnum.OPERATOR]))
):
    """
    Markdown í˜•ì‹ ë¶„ì„ ë¦¬í¬íŠ¸ ì¡°íšŒ

    ì ‘ê·¼ ê¶Œí•œ: admin, operator
    ìºì‹œ: 10ë¶„
    """
    try:
        cache_key = f"anomaly_report_md_{days}"
        cached_report = await get_cached_data(cache_key)
        if cached_report:
            return PlainTextResponse(cached_report, media_type="text/markdown")

        logger.info(f"Markdown ë¦¬í¬íŠ¸ ìƒì„±: days={days}")

        # Markdown í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ì‹¤í–‰
        result_md = await run_anomaly_script(days=days, output_format="markdown")
        if not result_md:
            raise HTTPException(status_code=503, detail="ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨")

        # ìºì‹œì— 10ë¶„ê°„ ì €ì¥
        await set_cache_data(cache_key, result_md, ttl=600)

        logger.info("Markdown ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        return PlainTextResponse(result_md, media_type="text/markdown")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Markdown ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail="ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

# ================================================================
# ì—ëŸ¬ í•¸ë“¤ëŸ¬ ë° ë¯¸ë“¤ì›¨ì–´ (í•œêµ­ì–´ ì£¼ì„ í¬í•¨)
# ================================================================

@router.middleware("http")
async def add_korean_error_headers(request: Request, call_next):
    """
    í•œêµ­ì–´ ì—ëŸ¬ ë©”ì‹œì§€ í—¤ë” ì¶”ê°€ ë¯¸ë“¤ì›¨ì–´
    """
    response = await call_next(request)

    # ì—ëŸ¬ ì‘ë‹µì— í•œêµ­ì–´ ì„¤ëª… í—¤ë” ì¶”ê°€
    if response.status_code >= 400:
        error_messages = {
            400: "ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤",
            401: "ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤",
            403: "ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤",
            404: "ìš”ì²­í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            429: "ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤",
            500: "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            502: "ì˜ëª»ëœ ê²Œì´íŠ¸ì›¨ì´ ì‘ë‹µì…ë‹ˆë‹¤",
            503: "ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        }

        korean_message = error_messages.get(response.status_code, "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
        response.headers["X-Error-Message-KR"] = korean_message

    return response

# ================================================================
# RCA ë° ê³„ì ˆì„± ë¶„í•´ ì—”ë“œí¬ì¸íŠ¸ (ìƒˆë¡œ ì¶”ê°€)
# ================================================================

class RCARequest(BaseModel):
    """ì›ì¸ë¶„ì„ ìš”ì²­ ëª¨ë¸"""
    target_metric: Optional[str] = Field(None, description="ëŒ€ìƒ ì§€í‘œëª… (ì—†ìœ¼ë©´ ìë™ ì„ íƒ)")
    anomaly_start: int = Field(..., ge=0, description="ì´ìƒ êµ¬ê°„ ì‹œì‘ ì¸ë±ìŠ¤")
    anomaly_end: int = Field(..., ge=0, description="ì´ìƒ êµ¬ê°„ ì¢…ë£Œ ì¸ë±ìŠ¤")
    baseline_start: int = Field(..., ge=0, description="ê¸°ì¤€ êµ¬ê°„ ì‹œì‘ ì¸ë±ìŠ¤")
    baseline_end: int = Field(..., ge=0, description="ê¸°ì¤€ êµ¬ê°„ ì¢…ë£Œ ì¸ë±ìŠ¤")
    metrics_data: Dict[str, List[float]] = Field(..., description="ì§€í‘œë³„ ì‹œê³„ì—´ ë°ì´í„°")
    top_n: int = Field(5, ge=1, le=20, description="ìƒìœ„ Nê°œ ì›ì¸")

    @validator('anomaly_end')
    def validate_anomaly_window(cls, v, values):
        if 'anomaly_start' in values and v <= values['anomaly_start']:
            raise ValueError('ì´ìƒ êµ¬ê°„ ì¢…ë£ŒëŠ” ì‹œì‘ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤')
        return v

    @validator('baseline_end')
    def validate_baseline_window(cls, v, values):
        if 'baseline_start' in values and v <= values['baseline_start']:
            raise ValueError('ê¸°ì¤€ êµ¬ê°„ ì¢…ë£ŒëŠ” ì‹œì‘ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤')
        return v

@router.post("/rca", summary="ì´ìƒíƒì§€ ì›ì¸ë¶„ì„(RCA)")
async def run_root_cause_analysis(
    rca_request: RCARequest,
    current_user = Depends(require_role([RoleEnum.ADMIN, RoleEnum.OPERATOR]))
):
    """
    ì´ìƒ êµ¬ê°„ì— ëŒ€í•œ ì›ì¸ë¶„ì„ ìˆ˜í–‰

    ì ‘ê·¼ ê¶Œí•œ: admin, operator
    ìºì‹œ: 5ë¶„
    """
    try:
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"rca_{hash(str(rca_request.dict()))}"
        cached_data = await get_cached_data(cache_key)
        if cached_data:
            return JSONResponse(cached_data)

        logger.info(f"ì›ì¸ë¶„ì„ ìš”ì²­: ì´ìƒêµ¬ê°„={rca_request.anomaly_start}-{rca_request.anomaly_end}")

        # RCA ë¶„ì„ ì‹¤í–‰
        from .anomaly_rca import analyze_root_causes

        rca_result = analyze_root_causes(
            series_dict=rca_request.metrics_data,
            anomaly_window=(rca_request.anomaly_start, rca_request.anomaly_end),
            baseline_window=(rca_request.baseline_start, rca_request.baseline_end),
            target_metric=rca_request.target_metric,
            top_n=rca_request.top_n
        )

        response_data = {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "analysis_results": rca_result,
            "request_params": {
                "anomaly_window": f"{rca_request.anomaly_start}-{rca_request.anomaly_end}",
                "baseline_window": f"{rca_request.baseline_start}-{rca_request.baseline_end}",
                "target_metric": rca_request.target_metric or "ìë™ ì„ íƒ",
                "top_n": rca_request.top_n
            }
        }

        # 5ë¶„ ìºì‹œ
        await set_cache_data(cache_key, response_data, ttl=300)

        return JSONResponse(response_data)

    except Exception as e:
        logger.error(f"ì›ì¸ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì›ì¸ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@router.get("/decompose", summary="ì‹œê³„ì—´ ê³„ì ˆì„± ë¶„í•´")
async def decompose_timeseries(
    metric: str = Query(..., description="ë¶„í•´í•  ì§€í‘œëª…"),
    days: int = Query(30, ge=14, le=365, description="ë¶„ì„ ê¸°ê°„ (ì¼)"),
    freq_hint: str = Query("D", regex="^(D|H|W)$", description="ì£¼ê¸° íŒíŠ¸ (D=ì¼ê°„, H=ì‹œê°„, W=ì£¼ê°„)"),
    current_user = Depends(require_role([RoleEnum.ADMIN, RoleEnum.OPERATOR]))
):
    """
    ì‹œê³„ì—´ ë°ì´í„°ì˜ ê³„ì ˆì„±, ì¶”ì„¸, ì”ì°¨ ë¶„í•´

    ì ‘ê·¼ ê¶Œí•œ: admin, operator
    ìºì‹œ: 5ë¶„
    """
    try:
        cache_key = f"decompose_{metric}_{days}_{freq_hint}"
        cached_data = await get_cached_data(cache_key)
        if cached_data:
            return JSONResponse(cached_data)

        logger.info(f"ê³„ì ˆì„± ë¶„í•´ ìš”ì²­: metric={metric}, days={days}, freq={freq_hint}")

        # ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰í•˜ì—¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        result_json = await run_anomaly_script(days=days, output_format="json")
        if not result_json:
            raise HTTPException(status_code=503, detail="ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

        analysis_data = parse_analysis_result(result_json)
        if not analysis_data:
            raise HTTPException(status_code=502, detail="ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨")

        # ì§€í‘œ ë°ì´í„° ì¶”ì¶œ
        metric_series = []
        timestamps = []

        # ìƒì„¸ ì´ìƒì¹˜ ë°ì´í„°ì—ì„œ ì‹œê³„ì—´ ì¶”ì¶œ
        detailed_anomalies = analysis_data.get("detailed_anomalies", {})
        if metric in detailed_anomalies:
            anomaly_data = detailed_anomalies[metric]
            for item in anomaly_data:
                metric_series.append(item.get("value", 0))
                timestamps.append(item.get("timestamp", ""))

        if len(metric_series) < 14:
            raise HTTPException(status_code=400, detail=f"ì§€í‘œ '{metric}'ì˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 14ê°œ í•„ìš”, í˜„ì¬ {len(metric_series)}ê°œ)")

        # ê³„ì ˆì„± ë¶„í•´ ì‹¤í–‰
        from .anomaly_rca import decompose_seasonality

        decomp_result = decompose_seasonality(
            series=metric_series,
            timestamps=timestamps,
            freq_hint=freq_hint
        )

        response_data = {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "metric_name": metric,
            "analysis_period_days": days,
            "frequency_hint": freq_hint,
            "decomposition": decomp_result,
            "data_points": len(metric_series)
        }

        # 5ë¶„ ìºì‹œ
        await set_cache_data(cache_key, response_data, ttl=300)

        return JSONResponse(response_data)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ê³„ì ˆì„± ë¶„í•´ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ê³„ì ˆì„± ë¶„í•´ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# ================================================================
# í´ë°± ë°ì´í„° ì œê³µ í•¨ìˆ˜ë“¤ (í•œêµ­ì–´ ì£¼ì„ í¬í•¨)
# ================================================================

def get_fallback_summary() -> Dict[str, Any]:
    """
    ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í´ë°± ìš”ì•½ ë°ì´í„°

    Returns:
        Dict: ê¸°ë³¸ ìš”ì•½ ì •ë³´
    """
    return {
        "total_anomalies": 0,
        "high_risk_metrics": 0,
        "future_risk_periods": 0,
        "confidence_score": 0.0,
        "last_updated": datetime.now().isoformat(),
        "analysis_period_days": 30,
        "metrics_analyzed": 0,
        "status": "ë¶„ì„ ë°ì´í„° ì—†ìŒ - ê¸°ë³¸ê°’ ë°˜í™˜"
    }

def get_fallback_health() -> Dict[str, Any]:
    """
    í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í´ë°± ìƒíƒœ ë°ì´í„°

    Returns:
        Dict: ê¸°ë³¸ í—¬ìŠ¤ ìƒíƒœ
    """
    return {
        "status": "unhealthy",
        "last_analysis_time": None,
        "data_sources_available": {
            "metrics_json": False,
            "log_files": False,
            "ci_reports": False
        },
        "cache_hit_rate": 0.0,
        "avg_response_time_ms": 0.0,
        "message": "ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ"
    }

# ================================================================
# ëª¨ë“ˆ ì´ˆê¸°í™” (í•œêµ­ì–´ ì£¼ì„ í¬í•¨)
# ================================================================

logger.info("ğŸ” ì´ìƒíƒì§€ API ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
logger.info("ì—”ë“œí¬ì¸íŠ¸: /api/v1/anomaly/* (admin, operator ê¶Œí•œ í•„ìš”)")
logger.info("ìºì‹œ TTL: ìš”ì•½ 5ë¶„, ì‹œê³„ì—´ 5ë¶„, ì˜ˆì¸¡ 10ë¶„, ë¦¬í¬íŠ¸ 10ë¶„")