# mcp/anomaly_rca.py
"""
ì´ìƒíƒì§€ ì›ì¸ë¶„ì„(RCA) ë° ê³„ì ˆì„± ë¶„í•´ ìœ í‹¸ë¦¬í‹°

ì£¼ìš” ê¸°ëŠ¥:
- ë‹¨ë³€ëŸ‰/ë‹¤ë³€ëŸ‰ ì§€í‘œ ìƒê´€ê´€ê³„ ë° ê¸°ì—¬ë„ ë¶„ì„
- ì´ìƒ êµ¬ê°„ê³¼ ê¸°ì¤€ êµ¬ê°„ ë¹„êµë¥¼ í†µí•œ ì›ì¸ í•­ëª© ì‹ë³„
- ê³„ì ˆì„±/ì¶”ì„¸ ë¶„í•´ë¥¼ í†µí•œ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„
- í•œêµ­ì–´ ê²°ê³¼ í¬ë§·íŒ… ë° ì„¤ëª…

ì‘ì„±ì: Claude Code Assistant
ìƒì„±ì¼: 2024-09-21
"""

from __future__ import annotations

import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional, Union
from datetime import datetime, timedelta
import json
from dataclasses import dataclass, asdict
from scipy import stats
from scipy.signal import periodogram
import warnings

# ë¡œê¹… ì„¤ì •
log = logging.getLogger("mcp.anomaly_rca")

# ê²½ê³  í•„í„°ë§ (í†µê³„ ê³„ì‚° ì‹œ ë°œìƒí•˜ëŠ” ë¶ˆí•„ìš”í•œ ê²½ê³  ì–µì œ)
warnings.filterwarnings("ignore", category=RuntimeWarning)

@dataclass
class RCACause:
    """ì›ì¸ë¶„ì„ ê²°ê³¼ í•­ëª©"""
    cause: str              # ì›ì¸ ì§€í‘œëª… (í•œêµ­ì–´)
    contribution: float     # ê¸°ì—¬ë„ (0-100%)
    delta: float           # ê¸°ì¤€ ëŒ€ë¹„ ë³€í™”ëŸ‰
    evidence: str          # ì¦ê±°/ì„¤ëª… (í•œêµ­ì–´)
    correlation: float     # ëŒ€ìƒ ì§€í‘œì™€ì˜ ìƒê´€ê³„ìˆ˜
    p_value: float        # í†µê³„ì  ìœ ì˜ì„± (p-value)

@dataclass
class SeasonalDecomposition:
    """ê³„ì ˆì„± ë¶„í•´ ê²°ê³¼"""
    trend: List[float]          # ì¶”ì„¸ ì„±ë¶„
    seasonal: List[float]       # ê³„ì ˆì„± ì„±ë¶„
    residual: List[float]       # ì”ì°¨ ì„±ë¶„
    timestamps: List[str]       # íƒ€ì„ìŠ¤íƒ¬í”„
    seasonality_strength: float # ê³„ì ˆì„± ê°•ë„ (0-1)
    trend_strength: float      # ì¶”ì„¸ ê°•ë„ (0-1)
    dominant_period: int       # ì£¼ìš” ì£¼ê¸° (ì¼/ì‹œê°„ ë‹¨ìœ„)

class AnomalyRCAAnalyzer:
    """ì´ìƒíƒì§€ ì›ì¸ë¶„ì„ê¸°"""

    def __init__(self, min_correlation_threshold: float = 0.3):
        """
        ì´ˆê¸°í™”

        Args:
            min_correlation_threshold: ìµœì†Œ ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)
        """
        self.min_correlation_threshold = min_correlation_threshold
        self.log = logging.getLogger(f"{__name__}.AnomalyRCAAnalyzer")

    def analyze_root_causes(
        self,
        series_dict: Dict[str, List[float]],
        anomaly_window: Tuple[int, int],
        baseline_window: Tuple[int, int],
        target_metric: str = None,
        top_n: int = 5
    ) -> Dict[str, Any]:
        """
        ì´ìƒ êµ¬ê°„ì˜ ì›ì¸ ë¶„ì„ ìˆ˜í–‰

        Args:
            series_dict: ì§€í‘œë³„ ì‹œê³„ì—´ ë°ì´í„° {"ì§€í‘œëª…": [ê°’ë“¤]}
            anomaly_window: ì´ìƒ êµ¬ê°„ (ì‹œì‘_ì¸ë±ìŠ¤, ë_ì¸ë±ìŠ¤)
            baseline_window: ê¸°ì¤€ êµ¬ê°„ (ì‹œì‘_ì¸ë±ìŠ¤, ë_ì¸ë±ìŠ¤)
            target_metric: ëŒ€ìƒ ì§€í‘œëª… (Noneì´ë©´ ìë™ ì„ íƒ)
            top_n: ìƒìœ„ Nê°œ ì›ì¸ ë°˜í™˜

        Returns:
            Dict: ì›ì¸ë¶„ì„ ê²°ê³¼
        """
        try:
            self.log.info(f"ì›ì¸ë¶„ì„ ì‹œì‘ - ì´ìƒêµ¬ê°„: {anomaly_window}, ê¸°ì¤€êµ¬ê°„: {baseline_window}")

            # ì…ë ¥ ê²€ì¦
            if not series_dict:
                raise ValueError("ì‹œê³„ì—´ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

            # ëŒ€ìƒ ì§€í‘œ ìë™ ì„ íƒ (ê°€ì¥ í° ë³€í™”ë¥¼ ë³´ì¸ ì§€í‘œ)
            if target_metric is None:
                target_metric = self._select_target_metric(series_dict, anomaly_window, baseline_window)

            if target_metric not in series_dict:
                raise ValueError(f"ëŒ€ìƒ ì§€í‘œ '{target_metric}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ê° ì§€í‘œë³„ ê¸°ì—¬ë„ ê³„ì‚°
            causes = []
            target_series = np.array(series_dict[target_metric])

            for metric_name, metric_series in series_dict.items():
                if metric_name == target_metric:
                    continue

                try:
                    cause = self._calculate_metric_contribution(
                        metric_name,
                        np.array(metric_series),
                        target_series,
                        anomaly_window,
                        baseline_window
                    )

                    if cause and abs(cause.correlation) >= self.min_correlation_threshold:
                        causes.append(cause)

                except Exception as e:
                    self.log.warning(f"ì§€í‘œ '{metric_name}' ê¸°ì—¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    continue

            # ê¸°ì—¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            causes.sort(key=lambda x: abs(x.contribution), reverse=True)
            top_causes = causes[:top_n]

            # ê²°ê³¼ êµ¬ì„±
            result = {
                "ë¶„ì„_ì‹œê°„": datetime.now().isoformat(),
                "ëŒ€ìƒ_ì§€í‘œ": target_metric,
                "ì´ìƒ_êµ¬ê°„": {
                    "ì‹œì‘": anomaly_window[0],
                    "ë": anomaly_window[1],
                    "ê¸°ê°„": anomaly_window[1] - anomaly_window[0] + 1
                },
                "ê¸°ì¤€_êµ¬ê°„": {
                    "ì‹œì‘": baseline_window[0],
                    "ë": baseline_window[1],
                    "ê¸°ê°„": baseline_window[1] - baseline_window[0] + 1
                },
                "ì£¼ìš”_ì›ì¸": [asdict(cause) for cause in top_causes],
                "ì „ì²´_ì›ì¸_ìˆ˜": len(causes),
                "ë¶„ì„_ìš”ì•½": self._generate_summary(target_metric, top_causes)
            }

            self.log.info(f"ì›ì¸ë¶„ì„ ì™„ë£Œ - ë°œê²¬ëœ ì›ì¸: {len(top_causes)}ê°œ")
            return result

        except Exception as e:
            self.log.error(f"ì›ì¸ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise

    def _select_target_metric(
        self,
        series_dict: Dict[str, List[float]],
        anomaly_window: Tuple[int, int],
        baseline_window: Tuple[int, int]
    ) -> str:
        """ê°€ì¥ í° ë³€í™”ë¥¼ ë³´ì¸ ì§€í‘œë¥¼ ëŒ€ìƒ ì§€í‘œë¡œ ì„ íƒ"""
        max_change = 0
        target_metric = list(series_dict.keys())[0]

        for metric_name, series in series_dict.items():
            series_arr = np.array(series)

            # ê¸°ì¤€ êµ¬ê°„ê³¼ ì´ìƒ êµ¬ê°„ì˜ í‰ê· ê°’ ì°¨ì´
            baseline_mean = np.mean(series_arr[baseline_window[0]:baseline_window[1]+1])
            anomaly_mean = np.mean(series_arr[anomaly_window[0]:anomaly_window[1]+1])

            if baseline_mean != 0:
                change_ratio = abs((anomaly_mean - baseline_mean) / baseline_mean)
                if change_ratio > max_change:
                    max_change = change_ratio
                    target_metric = metric_name

        return target_metric

    def _calculate_metric_contribution(
        self,
        metric_name: str,
        metric_series: np.ndarray,
        target_series: np.ndarray,
        anomaly_window: Tuple[int, int],
        baseline_window: Tuple[int, int]
    ) -> Optional[RCACause]:
        """ê°œë³„ ì§€í‘œì˜ ê¸°ì—¬ë„ ê³„ì‚°"""
        try:
            # ê¸°ì¤€ êµ¬ê°„ í†µê³„
            baseline_metric = metric_series[baseline_window[0]:baseline_window[1]+1]
            baseline_target = target_series[baseline_window[0]:baseline_window[1]+1]
            baseline_metric_mean = np.mean(baseline_metric)

            # ì´ìƒ êµ¬ê°„ í†µê³„
            anomaly_metric = metric_series[anomaly_window[0]:anomaly_window[1]+1]
            anomaly_target = target_series[anomaly_window[0]:anomaly_window[1]+1]
            anomaly_metric_mean = np.mean(anomaly_metric)

            # ë³€í™”ëŸ‰ ê³„ì‚°
            delta = anomaly_metric_mean - baseline_metric_mean
            delta_ratio = (delta / baseline_metric_mean * 100) if baseline_metric_mean != 0 else 0

            # ìƒê´€ê´€ê³„ ê³„ì‚° (ì „ì²´ ê¸°ê°„)
            correlation, p_value = stats.pearsonr(metric_series, target_series)

            # ê¸°ì—¬ë„ ê³„ì‚° (ë³€í™”ëŸ‰ê³¼ ìƒê´€ê´€ê³„ ê¸°ë°˜)
            contribution = abs(delta_ratio) * abs(correlation)

            # ì¦ê±° ë¬¸ì¥ ìƒì„±
            evidence = self._generate_evidence_text(
                metric_name, delta, delta_ratio, correlation, p_value
            )

            return RCACause(
                cause=metric_name,
                contribution=contribution,
                delta=delta,
                evidence=evidence,
                correlation=correlation,
                p_value=p_value
            )

        except Exception as e:
            self.log.warning(f"ì§€í‘œ '{metric_name}' ê¸°ì—¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return None

    def _generate_evidence_text(
        self,
        metric_name: str,
        delta: float,
        delta_ratio: float,
        correlation: float,
        p_value: float
    ) -> str:
        """ì¦ê±° í…ìŠ¤íŠ¸ ìƒì„± (í•œêµ­ì–´)"""
        direction = "ì¦ê°€" if delta > 0 else "ê°ì†Œ"
        correlation_strength = self._get_correlation_strength_korean(abs(correlation))
        significance = "ìœ ì˜í•¨" if p_value < 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"

        return (f"{metric_name}ê°€ {abs(delta_ratio):.1f}% {direction}í•˜ì—¬ "
                f"{correlation_strength} ìƒê´€ê´€ê³„ (r={correlation:.3f}, p={p_value:.3f}, {significance})")

    def _get_correlation_strength_korean(self, abs_correlation: float) -> str:
        """ìƒê´€ê´€ê³„ ê°•ë„ë¥¼ í•œêµ­ì–´ë¡œ ë³€í™˜"""
        if abs_correlation >= 0.8:
            return "ë§¤ìš° ê°•í•œ"
        elif abs_correlation >= 0.6:
            return "ê°•í•œ"
        elif abs_correlation >= 0.4:
            return "ë³´í†µ"
        elif abs_correlation >= 0.2:
            return "ì•½í•œ"
        else:
            return "ë§¤ìš° ì•½í•œ"

    def _generate_summary(self, target_metric: str, causes: List[RCACause]) -> str:
        """ë¶„ì„ ìš”ì•½ ìƒì„± (í•œêµ­ì–´)"""
        if not causes:
            return f"{target_metric}ì— ëŒ€í•œ ìœ ì˜ë¯¸í•œ ì›ì¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        top_cause = causes[0]
        summary = (f"{target_metric}ì˜ ì´ìƒ ì›ì¸: {top_cause.cause}ê°€ "
                  f"{top_cause.contribution:.1f}% ê¸°ì—¬ (ìƒê´€ê³„ìˆ˜: {top_cause.correlation:.3f})")

        if len(causes) > 1:
            summary += f". ì´ {len(causes)}ê°œ ê´€ë ¨ ìš”ì¸ ì‹ë³„ë¨."

        return summary

    def decompose_seasonality(
        self,
        series: List[float],
        timestamps: List[str] = None,
        freq_hint: str = "D"
    ) -> Dict[str, Any]:
        """
        ì‹œê³„ì—´ ê³„ì ˆì„± ë¶„í•´ (ê°„ë‹¨í•œ ì´ë™í‰ê·  ê¸°ë°˜)

        Args:
            series: ì‹œê³„ì—´ ë°ì´í„°
            timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            freq_hint: ì£¼ê¸° íŒíŠ¸ ("D"=ì¼ê°„, "H"=ì‹œê°„, "W"=ì£¼ê°„)

        Returns:
            Dict: ê³„ì ˆì„± ë¶„í•´ ê²°ê³¼
        """
        try:
            self.log.info(f"ê³„ì ˆì„± ë¶„í•´ ì‹œì‘ - ë°ì´í„° í¬ì¸íŠ¸: {len(series)}ê°œ")

            if len(series) < 14:
                raise ValueError("ê³„ì ˆì„± ë¶„í•´ë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 14ê°œ ë°ì´í„° í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")

            series_arr = np.array(series)

            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ì—†ëŠ” ê²½ìš°)
            if timestamps is None:
                timestamps = [f"T{i}" for i in range(len(series))]

            # ì£¼ê¸° ì¶”ì •
            dominant_period = self._estimate_dominant_period(series_arr, freq_hint)

            # ì¶”ì„¸ ì„±ë¶„ ì¶”ì¶œ (ì´ë™í‰ê· )
            window_size = min(dominant_period, len(series) // 4)
            trend = self._calculate_trend(series_arr, window_size)

            # ì¶”ì„¸ ì œê±°
            detrended = series_arr - trend

            # ê³„ì ˆì„± ì„±ë¶„ ì¶”ì¶œ
            seasonal = self._calculate_seasonal(detrended, dominant_period)

            # ì”ì°¨ ê³„ì‚°
            residual = series_arr - trend - seasonal

            # ê°•ë„ ê³„ì‚°
            seasonality_strength = self._calculate_seasonality_strength(seasonal, residual)
            trend_strength = self._calculate_trend_strength(trend, series_arr)

            result = SeasonalDecomposition(
                trend=trend.tolist(),
                seasonal=seasonal.tolist(),
                residual=residual.tolist(),
                timestamps=timestamps,
                seasonality_strength=seasonality_strength,
                trend_strength=trend_strength,
                dominant_period=dominant_period
            )

            # Dictë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
            decomp_dict = asdict(result)
            decomp_dict["ë¶„ì„_ìš”ì•½"] = self._generate_decomposition_summary(result)

            self.log.info(f"ê³„ì ˆì„± ë¶„í•´ ì™„ë£Œ - ì£¼ê¸°: {dominant_period}, ê³„ì ˆì„± ê°•ë„: {seasonality_strength:.3f}")
            return decomp_dict

        except Exception as e:
            self.log.error(f"ê³„ì ˆì„± ë¶„í•´ ì‹¤íŒ¨: {e}")
            raise

    def _estimate_dominant_period(self, series: np.ndarray, freq_hint: str) -> int:
        """ì£¼ìš” ì£¼ê¸° ì¶”ì •"""
        try:
            # FFT ê¸°ë°˜ ì£¼ê¸° ì¶”ì •
            if len(series) > 50:
                freqs, power = periodogram(series)
                dominant_freq_idx = np.argmax(power[1:]) + 1  # DC ì„±ë¶„ ì œì™¸
                period = int(1 / freqs[dominant_freq_idx]) if freqs[dominant_freq_idx] > 0 else 7
            else:
                period = 7  # ê¸°ë³¸ê°’

            # ì£¼ê¸° íŒíŠ¸ì— ë”°ë¥¸ ì¡°ì •
            if freq_hint == "H":  # ì‹œê°„ ë‹¨ìœ„
                period = min(max(period, 24), 168)  # 1ì¼~1ì£¼
            elif freq_hint == "W":  # ì£¼ê°„ ë‹¨ìœ„
                period = min(max(period, 4), 52)   # 1ë‹¬~1ë…„
            else:  # ì¼ê°„ ë‹¨ìœ„ (ê¸°ë³¸)
                period = min(max(period, 7), 365)  # 1ì£¼~1ë…„

            return period

        except Exception:
            return 7  # ê¸°ë³¸ê°’: 7ì¼ ì£¼ê¸°

    def _calculate_trend(self, series: np.ndarray, window_size: int) -> np.ndarray:
        """ì¶”ì„¸ ì„±ë¶„ ê³„ì‚° (ì¤‘ì•™ê°’ ê¸°ë°˜ ì´ë™í‰ê· )"""
        trend = np.zeros_like(series)
        half_window = window_size // 2

        for i in range(len(series)):
            start = max(0, i - half_window)
            end = min(len(series), i + half_window + 1)
            trend[i] = np.median(series[start:end])

        return trend

    def _calculate_seasonal(self, detrended: np.ndarray, period: int) -> np.ndarray:
        """ê³„ì ˆì„± ì„±ë¶„ ê³„ì‚°"""
        seasonal = np.zeros_like(detrended)

        for i in range(len(detrended)):
            # ê°™ì€ ê³„ì ˆ ìœ„ì¹˜ì˜ ê°’ë“¤ í‰ê· 
            seasonal_indices = list(range(i % period, len(detrended), period))
            seasonal[i] = np.mean(detrended[seasonal_indices])

        return seasonal

    def _calculate_seasonality_strength(self, seasonal: np.ndarray, residual: np.ndarray) -> float:
        """ê³„ì ˆì„± ê°•ë„ ê³„ì‚° (0-1)"""
        seasonal_var = np.var(seasonal)
        residual_var = np.var(residual)
        total_var = seasonal_var + residual_var

        return seasonal_var / total_var if total_var > 0 else 0

    def _calculate_trend_strength(self, trend: np.ndarray, original: np.ndarray) -> float:
        """ì¶”ì„¸ ê°•ë„ ê³„ì‚° (0-1)"""
        detrended_var = np.var(original - trend)
        original_var = np.var(original)

        return 1 - (detrended_var / original_var) if original_var > 0 else 0

    def _generate_decomposition_summary(self, decomp: SeasonalDecomposition) -> str:
        """ë¶„í•´ ìš”ì•½ ìƒì„± (í•œêµ­ì–´)"""
        summary_parts = []

        if decomp.seasonality_strength > 0.6:
            summary_parts.append(f"ê°•í•œ ê³„ì ˆì„± ({decomp.seasonality_strength:.1%})")
        elif decomp.seasonality_strength > 0.3:
            summary_parts.append(f"ë³´í†µ ê³„ì ˆì„± ({decomp.seasonality_strength:.1%})")
        else:
            summary_parts.append(f"ì•½í•œ ê³„ì ˆì„± ({decomp.seasonality_strength:.1%})")

        if decomp.trend_strength > 0.6:
            summary_parts.append(f"ëšœë ·í•œ ì¶”ì„¸ ({decomp.trend_strength:.1%})")
        elif decomp.trend_strength > 0.3:
            summary_parts.append(f"ë³´í†µ ì¶”ì„¸ ({decomp.trend_strength:.1%})")
        else:
            summary_parts.append(f"ì•½í•œ ì¶”ì„¸ ({decomp.trend_strength:.1%})")

        summary_parts.append(f"ì£¼ê¸° {decomp.dominant_period}ì¼")

        return ", ".join(summary_parts)


# í¸ì˜ í•¨ìˆ˜ë“¤
def analyze_root_causes(
    series_dict: Dict[str, List[float]],
    anomaly_window: Tuple[int, int],
    baseline_window: Tuple[int, int],
    target_metric: str = None,
    top_n: int = 5
) -> Dict[str, Any]:
    """
    ì´ìƒ êµ¬ê°„ ì›ì¸ë¶„ì„ ìˆ˜í–‰ (í¸ì˜ í•¨ìˆ˜)

    Args:
        series_dict: ì§€í‘œë³„ ì‹œê³„ì—´ ë°ì´í„°
        anomaly_window: ì´ìƒ êµ¬ê°„ (ì‹œì‘, ë)
        baseline_window: ê¸°ì¤€ êµ¬ê°„ (ì‹œì‘, ë)
        target_metric: ëŒ€ìƒ ì§€í‘œëª…
        top_n: ìƒìœ„ Nê°œ ì›ì¸

    Returns:
        Dict: ì›ì¸ë¶„ì„ ê²°ê³¼
    """
    analyzer = AnomalyRCAAnalyzer()
    return analyzer.analyze_root_causes(
        series_dict, anomaly_window, baseline_window, target_metric, top_n
    )

def decompose_seasonality(
    series: List[float],
    timestamps: List[str] = None,
    freq_hint: str = "D"
) -> Dict[str, Any]:
    """
    ì‹œê³„ì—´ ê³„ì ˆì„± ë¶„í•´ (í¸ì˜ í•¨ìˆ˜)

    Args:
        series: ì‹œê³„ì—´ ë°ì´í„°
        timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸
        freq_hint: ì£¼ê¸° íŒíŠ¸

    Returns:
        Dict: ê³„ì ˆì„± ë¶„í•´ ê²°ê³¼
    """
    analyzer = AnomalyRCAAnalyzer()
    return analyzer.decompose_seasonality(series, timestamps, freq_hint)


# ìƒ˜í”Œ ë°ì´í„° ìƒì„± í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
def generate_sample_data(days: int = 30) -> Dict[str, List[float]]:
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)

    # ê¸°ë³¸ ì‹œê°„ ì¶•
    t = np.arange(days)

    # CPU ì‚¬ìš©ë¥  (ì£¼ê°„ íŒ¨í„´ + ë…¸ì´ì¦ˆ)
    cpu_baseline = 50 + 20 * np.sin(2 * np.pi * t / 7) + np.random.normal(0, 5, days)

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (CPUì™€ ìƒê´€ê´€ê³„ + ì¶”ì„¸)
    memory_baseline = 40 + 0.8 * cpu_baseline + 0.3 * t + np.random.normal(0, 3, days)

    # ë„¤íŠ¸ì›Œí¬ ì§€ì—° (ë°˜ëŒ€ ìƒê´€ê´€ê³„)
    network_baseline = 100 - 0.5 * cpu_baseline + np.random.normal(0, 10, days)

    # ë””ìŠ¤í¬ I/O (ë…ë¦½ì  íŒ¨í„´)
    disk_baseline = 30 + 10 * np.sin(2 * np.pi * t / 3) + np.random.normal(0, 8, days)

    return {
        "cpu_usage": cpu_baseline.tolist(),
        "memory_usage": memory_baseline.tolist(),
        "network_latency": network_baseline.tolist(),
        "disk_io": disk_baseline.tolist()
    }


if __name__ == "__main__":
    # ìƒ˜í”Œ ì‹¤í–‰
    print("ğŸ” ì´ìƒíƒì§€ ì›ì¸ë¶„ì„ ìƒ˜í”Œ ì‹¤í–‰")

    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    sample_data = generate_sample_data(30)

    # ì›ì¸ë¶„ì„ ì‹¤í–‰
    anomaly_window = (20, 25)  # 20~25ì¼ì°¨ê°€ ì´ìƒ êµ¬ê°„
    baseline_window = (5, 15)  # 5~15ì¼ì°¨ê°€ ê¸°ì¤€ êµ¬ê°„

    print("\nğŸ“Š ì›ì¸ë¶„ì„ ì‹¤í–‰ ì¤‘...")
    rca_result = analyze_root_causes(
        sample_data, anomaly_window, baseline_window, top_n=3
    )

    print(json.dumps(rca_result, ensure_ascii=False, indent=2))

    # ê³„ì ˆì„± ë¶„í•´ ì‹¤í–‰
    print("\nğŸ”„ ê³„ì ˆì„± ë¶„í•´ ì‹¤í–‰ ì¤‘...")
    decomp_result = decompose_seasonality(sample_data["cpu_usage"])

    print(f"ê³„ì ˆì„± ê°•ë„: {decomp_result['seasonality_strength']:.3f}")
    print(f"ì¶”ì„¸ ê°•ë„: {decomp_result['trend_strength']:.3f}")
    print(f"ì£¼ìš” ì£¼ê¸°: {decomp_result['dominant_period']}ì¼")
    print(f"ìš”ì•½: {decomp_result['ë¶„ì„_ìš”ì•½']}")