#!/usr/bin/env python3
"""
StockPilot AI ì„±ëŠ¥ ë¶„ì„ ë„êµ¬
ì‹œìŠ¤í…œ ì„±ëŠ¥, API ì‘ë‹µì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë“±ì„ ì¢…í•© ë¶„ì„
"""

import os
import sys
import time
import psutil
import asyncio
import aiohttp
import json
import statistics
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
from datetime import datetime, timedelta
import subprocess
import sqlite3
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import matplotlib.pyplot as plt
import numpy as np

@dataclass
class PerformanceMetric:
    """ì„±ëŠ¥ ì§€í‘œ"""
    timestamp: str
    metric_name: str
    value: float
    unit: str
    category: str
    description: str = ""

@dataclass  
class LoadTestResult:
    """ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    endpoint: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    p95_response_time: float
    requests_per_second: float
    error_rate: float

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, project_root: str, base_url: str = "http://localhost:8000"):
        self.project_root = Path(project_root)
        self.base_url = base_url
        self.metrics: List[PerformanceMetric] = []
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì œì–´
        self.monitoring_active = False
        self.monitor_thread = None

    async def run_comprehensive_analysis(self) -> Dict:
        """ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
        self.logger.info("ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ì‹œì‘")
        
        results = {
            'analysis_info': {
                'timestamp': datetime.now().isoformat(),
                'base_url': self.base_url,
                'project_root': str(self.project_root)
            },
            'system_performance': {},
            'api_performance': {},
            'load_test_results': {},
            'database_performance': {},
            'resource_usage': {},
            'recommendations': []
        }
        
        try:
            # 1. ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„
            self.logger.info("ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
            results['system_performance'] = await self.analyze_system_performance()
            
            # 2. API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            self.logger.info("API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
            results['api_performance'] = await self.test_api_performance()
            
            # 3. ë¶€í•˜ í…ŒìŠ¤íŠ¸
            self.logger.info("ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
            results['load_test_results'] = await self.run_load_tests()
            
            # 4. ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„
            self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
            results['database_performance'] = await self.analyze_database_performance()
            
            # 5. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„
            self.logger.info("ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„ ì¤‘...")
            results['resource_usage'] = await self.analyze_resource_usage()
            
            # 6. ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„±
            results['recommendations'] = self.generate_recommendations(results)
            
        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            results['error'] = str(e)
        
        return results

    async def analyze_system_performance(self) -> Dict:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„"""
        # CPU ì •ë³´
        cpu_info = {
            'physical_cores': psutil.cpu_count(logical=False),
            'logical_cores': psutil.cpu_count(logical=True),
            'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
            'cpu_percent': psutil.cpu_percent(interval=1)
        }
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory = psutil.virtual_memory()
        memory_info = {
            'total': memory.total,
            'available': memory.available,
            'used': memory.used,
            'percentage': memory.percent
        }
        
        # ë””ìŠ¤í¬ ì •ë³´
        disk = psutil.disk_usage('/')
        disk_info = {
            'total': disk.total,
            'used': disk.used,
            'free': disk.free,
            'percentage': (disk.used / disk.total) * 100
        }
        
        # ë„¤íŠ¸ì›Œí¬ ì •ë³´
        net_io = psutil.net_io_counters()
        network_info = {
            'bytes_sent': net_io.bytes_sent,
            'bytes_recv': net_io.bytes_recv,
            'packets_sent': net_io.packets_sent,
            'packets_recv': net_io.packets_recv
        }
        
        # Python í”„ë¡œì„¸ìŠ¤ ì •ë³´
        python_processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
            try:
                if 'python' in proc.info['name'].lower():
                    python_processes.append(proc.info)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
        
        return {
            'cpu': cpu_info,
            'memory': memory_info,
            'disk': disk_info,
            'network': network_info,
            'python_processes': python_processes,
            'load_average': os.getloadavg() if hasattr(os, 'getloadavg') else None
        }

    async def test_api_performance(self) -> Dict:
        """API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        endpoints = [
            '/api/v1/health',
            '/api/v1/status', 
            '/api/v1/health/comprehensive',
            '/api/v1/usage/stats',
            '/api/v1/batch/jobs'
        ]
        
        results = {}
        
        async with aiohttp.ClientSession() as session:
            for endpoint in endpoints:
                url = f"{self.base_url}{endpoint}"
                response_times = []
                
                # ê° ì—”ë“œí¬ì¸íŠ¸ë¥¼ 10ë²ˆ í…ŒìŠ¤íŠ¸
                for _ in range(10):
                    start_time = time.time()
                    try:
                        async with session.get(url, timeout=30) as response:
                            await response.text()
                            response_time = (time.time() - start_time) * 1000  # ms
                            response_times.append(response_time)
                            
                    except Exception as e:
                        self.logger.warning(f"API í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ {url}: {e}")
                        response_times.append(30000)  # íƒ€ì„ì•„ì›ƒì„ 30ì´ˆë¡œ ê¸°ë¡
                
                if response_times:
                    results[endpoint] = {
                        'avg_response_time': statistics.mean(response_times),
                        'min_response_time': min(response_times),
                        'max_response_time': max(response_times),
                        'median_response_time': statistics.median(response_times),
                        'std_dev': statistics.stdev(response_times) if len(response_times) > 1 else 0,
                        'success_rate': sum(1 for t in response_times if t < 5000) / len(response_times) * 100
                    }
        
        return results

    async def run_load_tests(self) -> Dict:
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        test_configs = [
            {'concurrent_users': 10, 'duration': 30},
            {'concurrent_users': 50, 'duration': 60},
            {'concurrent_users': 100, 'duration': 30}
        ]
        
        results = {}
        
        for config in test_configs:
            test_name = f"{config['concurrent_users']}_users_{config['duration']}s"
            self.logger.info(f"ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {test_name}")
            
            result = await self._run_single_load_test(
                concurrent_users=config['concurrent_users'],
                duration=config['duration'],
                endpoint='/api/v1/health'
            )
            
            results[test_name] = asdict(result)
        
        return results

    async def _run_single_load_test(self, concurrent_users: int, duration: int, endpoint: str) -> LoadTestResult:
        """ë‹¨ì¼ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        url = f"{self.base_url}{endpoint}"
        response_times = []
        successful_requests = 0
        failed_requests = 0
        
        start_time = time.time()
        end_time = start_time + duration
        
        async def make_request(session):
            nonlocal successful_requests, failed_requests
            
            while time.time() < end_time:
                request_start = time.time()
                try:
                    async with session.get(url, timeout=10) as response:
                        await response.text()
                        response_time = (time.time() - request_start) * 1000
                        response_times.append(response_time)
                        
                        if response.status == 200:
                            successful_requests += 1
                        else:
                            failed_requests += 1
                            
                except Exception:
                    failed_requests += 1
                    response_times.append(10000)  # íƒ€ì„ì•„ì›ƒ/ì—ëŸ¬ë¥¼ 10ì´ˆë¡œ ê¸°ë¡
                
                # ì ì‹œ ëŒ€ê¸° (CPU ê³¼ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(0.01)
        
        # ë™ì‹œ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜
        connector = aiohttp.TCPConnector(limit=concurrent_users * 2)
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = [make_request(session) for _ in range(concurrent_users)]
            await asyncio.gather(*tasks, return_exceptions=True)
        
        total_requests = successful_requests + failed_requests
        actual_duration = time.time() - start_time
        
        # í†µê³„ ê³„ì‚°
        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            
            # 95 í¼ì„¼íƒ€ì¼ ê³„ì‚°
            sorted_times = sorted(response_times)
            p95_index = int(len(sorted_times) * 0.95)
            p95_response_time = sorted_times[p95_index] if p95_index < len(sorted_times) else max_response_time
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = 0
        
        return LoadTestResult(
            endpoint=endpoint,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            requests_per_second=total_requests / actual_duration if actual_duration > 0 else 0,
            error_rate=(failed_requests / total_requests * 100) if total_requests > 0 else 0
        )

    async def analyze_database_performance(self) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„"""
        results = {
            'connection_test': False,
            'query_performance': {},
            'connection_pool': {},
            'slow_queries': []
        }
        
        # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            # SQLite í…ŒìŠ¤íŠ¸ (í”„ë¡œì íŠ¸ì— SQLite íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
            sqlite_files = list(self.project_root.rglob("*.db"))
            if sqlite_files:
                db_path = sqlite_files[0]
                start_time = time.time()
                conn = sqlite3.connect(str(db_path))
                
                # ê°„ë‹¨í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table'")
                table_count = cursor.fetchone()[0]
                
                connection_time = (time.time() - start_time) * 1000
                
                results['connection_test'] = True
                results['query_performance'] = {
                    'connection_time_ms': connection_time,
                    'table_count': table_count
                }
                
                # í…Œì´ë¸”ë³„ í–‰ ìˆ˜ ì¡°íšŒ
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                
                table_stats = {}
                for (table_name,) in tables:
                    try:
                        start_time = time.time()
                        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                        row_count = cursor.fetchone()[0]
                        query_time = (time.time() - start_time) * 1000
                        
                        table_stats[table_name] = {
                            'row_count': row_count,
                            'query_time_ms': query_time
                        }
                    except Exception as e:
                        table_stats[table_name] = {'error': str(e)}
                
                results['table_statistics'] = table_stats
                conn.close()
                
        except Exception as e:
            results['database_error'] = str(e)
        
        return results

    async def analyze_resource_usage(self) -> Dict:
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        # 30ì´ˆê°„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        monitoring_duration = 30
        interval = 1
        
        cpu_usage = []
        memory_usage = []
        disk_io = []
        network_io = []
        
        initial_disk_io = psutil.disk_io_counters()
        initial_net_io = psutil.net_io_counters()
        
        self.logger.info(f"{monitoring_duration}ì´ˆê°„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§...")
        
        for _ in range(monitoring_duration):
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent()
            cpu_usage.append(cpu_percent)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            memory_usage.append(memory.percent)
            
            # ë””ìŠ¤í¬ I/O
            current_disk_io = psutil.disk_io_counters()
            if initial_disk_io:
                disk_io.append({
                    'read_bytes': current_disk_io.read_bytes - initial_disk_io.read_bytes,
                    'write_bytes': current_disk_io.write_bytes - initial_disk_io.write_bytes
                })
            
            # ë„¤íŠ¸ì›Œí¬ I/O
            current_net_io = psutil.net_io_counters()
            if initial_net_io:
                network_io.append({
                    'bytes_sent': current_net_io.bytes_sent - initial_net_io.bytes_sent,
                    'bytes_recv': current_net_io.bytes_recv - initial_net_io.bytes_recv
                })
            
            await asyncio.sleep(interval)
        
        return {
            'cpu_usage': {
                'avg': statistics.mean(cpu_usage) if cpu_usage else 0,
                'min': min(cpu_usage) if cpu_usage else 0,
                'max': max(cpu_usage) if cpu_usage else 0,
                'samples': cpu_usage
            },
            'memory_usage': {
                'avg': statistics.mean(memory_usage) if memory_usage else 0,
                'min': min(memory_usage) if memory_usage else 0,
                'max': max(memory_usage) if memory_usage else 0,
                'samples': memory_usage
            },
            'disk_io': disk_io,
            'network_io': network_io,
            'monitoring_duration': monitoring_duration
        }

    def generate_recommendations(self, results: Dict) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # API ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­
        api_perf = results.get('api_performance', {})
        slow_apis = [endpoint for endpoint, metrics in api_perf.items() 
                    if metrics.get('avg_response_time', 0) > 1000]
        
        if slow_apis:
            recommendations.append(f"âš¡ ëŠë¦° API ê°œì„  í•„ìš”: {', '.join(slow_apis)} (1ì´ˆ ì´ìƒ ì‘ë‹µì‹œê°„)")
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê¶Œì¥ì‚¬í•­
        sys_perf = results.get('system_performance', {})
        
        cpu_percent = sys_perf.get('cpu', {}).get('cpu_percent', 0)
        if cpu_percent > 80:
            recommendations.append(f"ğŸ”¥ CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({cpu_percent:.1f}%). ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        memory_percent = sys_perf.get('memory', {}).get('percentage', 0)
        if memory_percent > 80:
            recommendations.append(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({memory_percent:.1f}%). ë©”ëª¨ë¦¬ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        disk_percent = sys_perf.get('disk', {}).get('percentage', 0)
        if disk_percent > 80:
            recommendations.append(f"ğŸ’¿ ë””ìŠ¤í¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({disk_percent:.1f}%). ì •ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê¶Œì¥ì‚¬í•­
        load_test = results.get('load_test_results', {})
        for test_name, test_result in load_test.items():
            error_rate = test_result.get('error_rate', 0)
            if error_rate > 5:
                recommendations.append(f"ğŸš¨ {test_name} í…ŒìŠ¤íŠ¸ì—ì„œ ë†’ì€ ì—ëŸ¬ìœ¨ ({error_rate:.1f}%) ë°œìƒ")
            
            rps = test_result.get('requests_per_second', 0)
            if rps < 10:
                recommendations.append(f"âš¡ {test_name} í…ŒìŠ¤íŠ¸ì—ì„œ ë‚®ì€ ì²˜ë¦¬ëŸ‰ ({rps:.1f} RPS) ê¸°ë¡")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("âœ… ì „ë°˜ì ì¸ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        
        recommendations.extend([
            "ğŸ“Š ì •ê¸°ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
            "ğŸ” í”„ë¡œíŒŒì¼ë§ì„ í†µí•œ ë³‘ëª©ì  ë¶„ì„ì„ ê³ ë ¤í•˜ì„¸ìš”.",
            "âš¡ ìºì‹± ì „ëµì„ ê²€í† í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ì„¸ìš”.",
            "ğŸ“ˆ API ì‘ë‹µ ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.",
            "ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”ë¥¼ ê²€í† í•˜ì„¸ìš”."
        ])
        
        return recommendations

    def generate_performance_charts(self, results: Dict, output_dir: str = "performance_reports"):
        """ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„±"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # API ì‘ë‹µ ì‹œê°„ ì°¨íŠ¸
        api_perf = results.get('api_performance', {})
        if api_perf:
            endpoints = list(api_perf.keys())
            avg_times = [metrics.get('avg_response_time', 0) for metrics in api_perf.values()]
            
            plt.figure(figsize=(12, 6))
            plt.bar(endpoints, avg_times, color='skyblue')
            plt.title('API í‰ê·  ì‘ë‹µ ì‹œê°„')
            plt.xlabel('ì—”ë“œí¬ì¸íŠ¸')
            plt.ylabel('ì‘ë‹µ ì‹œê°„ (ms)')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(output_path / 'api_response_times.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì°¨íŠ¸
        resource_usage = results.get('resource_usage', {})
        if resource_usage:
            cpu_samples = resource_usage.get('cpu_usage', {}).get('samples', [])
            memory_samples = resource_usage.get('memory_usage', {}).get('samples', [])
            
            if cpu_samples and memory_samples:
                time_points = list(range(len(cpu_samples)))
                
                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
                
                # CPU ì‚¬ìš©ë¥ 
                ax1.plot(time_points, cpu_samples, label='CPU ì‚¬ìš©ë¥ ', color='red', linewidth=2)
                ax1.set_title('CPU ì‚¬ìš©ë¥  (ì‹œê°„ë³„)')
                ax1.set_xlabel('ì‹œê°„ (ì´ˆ)')
                ax1.set_ylabel('CPU ì‚¬ìš©ë¥  (%)')
                ax1.grid(True, alpha=0.3)
                ax1.legend()
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                ax2.plot(time_points, memory_samples, label='ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ', color='blue', linewidth=2)
                ax2.set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (ì‹œê°„ë³„)')
                ax2.set_xlabel('ì‹œê°„ (ì´ˆ)')
                ax2.set_ylabel('ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (%)')
                ax2.grid(True, alpha=0.3)
                ax2.legend()
                
                plt.tight_layout()
                plt.savefig(output_path / 'resource_usage.png', dpi=300, bbox_inches='tight')
                plt.close()
        
        # ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì°¨íŠ¸
        load_tests = results.get('load_test_results', {})
        if load_tests:
            test_names = list(load_tests.keys())
            rps_values = [test.get('requests_per_second', 0) for test in load_tests.values()]
            error_rates = [test.get('error_rate', 0) for test in load_tests.values()]
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # RPS ì°¨íŠ¸
            ax1.bar(test_names, rps_values, color='green', alpha=0.7)
            ax1.set_title('ì´ˆë‹¹ ìš”ì²­ ì²˜ë¦¬ëŸ‰ (RPS)')
            ax1.set_xlabel('í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤')
            ax1.set_ylabel('RPS')
            ax1.tick_params(axis='x', rotation=45)
            
            # ì—ëŸ¬ìœ¨ ì°¨íŠ¸
            ax2.bar(test_names, error_rates, color='red', alpha=0.7)
            ax2.set_title('ì—ëŸ¬ìœ¨ (%)')
            ax2.set_xlabel('í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤')
            ax2.set_ylabel('ì—ëŸ¬ìœ¨ (%)')
            ax2.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            plt.savefig(output_path / 'load_test_results.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        self.logger.info(f"ì„±ëŠ¥ ì°¨íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")

    def generate_html_report(self, results: Dict, output_file: str):
        """HTML ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        html_template = """
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>StockPilot AI ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸</title>
    <style>
        body {{ 
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            margin: 0; padding: 20px; background: #f5f5f5; 
        }}
        .container {{ 
            max-width: 1200px; margin: 0 auto; background: white; 
            border-radius: 8px; padding: 30px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        .header {{ text-align: center; margin-bottom: 30px; }}
        .score {{ font-size: 48px; font-weight: bold; margin: 10px 0; color: #28a745; }}
        .metrics-grid {{ 
            display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); 
            gap: 20px; margin-bottom: 30px; 
        }}
        .metric-card {{ 
            background: #f8f9fa; padding: 20px; border-radius: 6px; 
        }}
        .metric-title {{ font-size: 14px; color: #666; text-transform: uppercase; }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #007bff; }}
        .metric-unit {{ font-size: 14px; color: #666; }}
        .section {{ margin: 30px 0; }}
        .section h2 {{ color: #333; border-bottom: 2px solid #007bff; padding-bottom: 10px; }}
        .api-table {{ width: 100%; border-collapse: collapse; margin: 15px 0; }}
        .api-table th, .api-table td {{ 
            border: 1px solid #ddd; padding: 12px; text-align: left; 
        }}
        .api-table th {{ background: #f8f9fa; }}
        .load-test {{ 
            background: #e7f3ff; border: 1px solid #bee5eb; 
            border-radius: 6px; padding: 20px; margin: 15px 0; 
        }}
        .recommendations {{ 
            background: #d4edda; border: 1px solid #c3e6cb; 
            border-radius: 6px; padding: 20px; margin: 30px 0; 
        }}
        .chart {{ text-align: center; margin: 20px 0; }}
        .chart img {{ max-width: 100%; height: auto; border: 1px solid #ddd; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š StockPilot AI ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸</h1>
            <div class="score">{overall_score}</div>
            <p>ë¶„ì„ ì¼ì‹œ: {timestamp}</p>
            <p>ê¸°ì¤€ URL: {base_url}</p>
        </div>
        
        <div class="metrics-grid">
            {metrics_cards}
        </div>
        
        <div class="section">
            <h2>ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì„±ëŠ¥</h2>
            {system_performance}
        </div>
        
        <div class="section">
            <h2>âš¡ API ì„±ëŠ¥</h2>
            {api_performance}
        </div>
        
        <div class="section">
            <h2>ğŸš€ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼</h2>
            {load_test_results}
        </div>
        
        <div class="section">
            <h2>ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥</h2>
            {database_performance}
        </div>
        
        <div class="section">
            <h2>ğŸ“ˆ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰</h2>
            {resource_usage}
        </div>
        
        <div class="recommendations">
            <h2>ğŸ’¡ ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­</h2>
            {recommendations}
        </div>
    </div>
</body>
</html>
"""
        
        # ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        overall_score = self._calculate_overall_score(results)
        
        # ê° ì„¹ì…˜ì˜ HTML ìƒì„±
        metrics_cards = self._generate_metrics_cards(results)
        system_performance = self._generate_system_performance_html(results.get('system_performance', {}))
        api_performance = self._generate_api_performance_html(results.get('api_performance', {}))
        load_test_results = self._generate_load_test_html(results.get('load_test_results', {}))
        database_performance = self._generate_database_performance_html(results.get('database_performance', {}))
        resource_usage = self._generate_resource_usage_html(results.get('resource_usage', {}))
        recommendations = self._generate_recommendations_html(results.get('recommendations', []))
        
        # HTML ìƒì„±
        html_content = html_template.format(
            overall_score=overall_score,
            timestamp=results['analysis_info']['timestamp'],
            base_url=results['analysis_info']['base_url'],
            metrics_cards=metrics_cards,
            system_performance=system_performance,
            api_performance=api_performance,
            load_test_results=load_test_results,
            database_performance=database_performance,
            resource_usage=resource_usage,
            recommendations=recommendations
        )
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.logger.info(f"HTML ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")

    def _calculate_overall_score(self, results: Dict) -> int:
        """ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°"""
        score = 100
        
        # API ì„±ëŠ¥ ì ìˆ˜
        api_perf = results.get('api_performance', {})
        for metrics in api_perf.values():
            avg_time = metrics.get('avg_response_time', 0)
            if avg_time > 2000:
                score -= 15
            elif avg_time > 1000:
                score -= 10
            elif avg_time > 500:
                score -= 5
        
        # ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì ìˆ˜
        load_tests = results.get('load_test_results', {})
        for test_result in load_tests.values():
            error_rate = test_result.get('error_rate', 0)
            if error_rate > 10:
                score -= 20
            elif error_rate > 5:
                score -= 10
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì ìˆ˜
        sys_perf = results.get('system_performance', {})
        cpu_percent = sys_perf.get('cpu', {}).get('cpu_percent', 0)
        memory_percent = sys_perf.get('memory', {}).get('percentage', 0)
        
        if cpu_percent > 90:
            score -= 15
        elif cpu_percent > 80:
            score -= 10
        
        if memory_percent > 90:
            score -= 15
        elif memory_percent > 80:
            score -= 10
        
        return max(0, score)

    def _generate_metrics_cards(self, results: Dict) -> str:
        """ë©”íŠ¸ë¦­ ì¹´ë“œ HTML ìƒì„±"""
        cards = []
        
        # API í‰ê·  ì‘ë‹µì‹œê°„
        api_perf = results.get('api_performance', {})
        if api_perf:
            avg_times = [metrics.get('avg_response_time', 0) for metrics in api_perf.values()]
            overall_avg = sum(avg_times) / len(avg_times) if avg_times else 0
            
            cards.append(f"""
            <div class="metric-card">
                <div class="metric-title">í‰ê·  API ì‘ë‹µì‹œê°„</div>
                <div class="metric-value">{overall_avg:.1f}</div>
                <div class="metric-unit">ms</div>
            </div>
            """)
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
        sys_perf = results.get('system_performance', {})
        cpu_percent = sys_perf.get('cpu', {}).get('cpu_percent', 0)
        memory_percent = sys_perf.get('memory', {}).get('percentage', 0)
        
        cards.extend([
            f"""
            <div class="metric-card">
                <div class="metric-title">CPU ì‚¬ìš©ë¥ </div>
                <div class="metric-value">{cpu_percent:.1f}</div>
                <div class="metric-unit">%</div>
            </div>
            """,
            f"""
            <div class="metric-card">
                <div class="metric-title">ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ </div>
                <div class="metric-value">{memory_percent:.1f}</div>
                <div class="metric-unit">%</div>
            </div>
            """
        ])
        
        # ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        load_tests = results.get('load_test_results', {})
        if load_tests:
            # ìµœê³  RPS ì°¾ê¸°
            max_rps = max([test.get('requests_per_second', 0) for test in load_tests.values()], default=0)
            cards.append(f"""
            <div class="metric-card">
                <div class="metric-title">ìµœëŒ€ ì²˜ë¦¬ëŸ‰</div>
                <div class="metric-value">{max_rps:.1f}</div>
                <div class="metric-unit">RPS</div>
            </div>
            """)
        
        return ''.join(cards)

    def _generate_system_performance_html(self, sys_perf: Dict) -> str:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ HTML ìƒì„±"""
        if not sys_perf:
            return "<p>ì‹œìŠ¤í…œ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
        
        cpu_info = sys_perf.get('cpu', {})
        memory_info = sys_perf.get('memory', {})
        
        return f"""
        <p><strong>CPU:</strong> {cpu_info.get('physical_cores', 'N/A')}ì½”ì–´ (ë…¼ë¦¬ {cpu_info.get('logical_cores', 'N/A')}ì½”ì–´), ì‚¬ìš©ë¥  {cpu_info.get('cpu_percent', 0):.1f}%</p>
        <p><strong>ë©”ëª¨ë¦¬:</strong> {memory_info.get('used', 0) / (1024**3):.1f}GB / {memory_info.get('total', 0) / (1024**3):.1f}GB ({memory_info.get('percentage', 0):.1f}%)</p>
        <p><strong>ë””ìŠ¤í¬:</strong> {sys_perf.get('disk', {}).get('percentage', 0):.1f}% ì‚¬ìš©ì¤‘</p>
        """

    def _generate_api_performance_html(self, api_perf: Dict) -> str:
        """API ì„±ëŠ¥ HTML ìƒì„±"""
        if not api_perf:
            return "<p>API ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
        
        html = '<table class="api-table"><tr><th>ì—”ë“œí¬ì¸íŠ¸</th><th>í‰ê·  ì‘ë‹µì‹œê°„</th><th>ìµœì†Œ</th><th>ìµœëŒ€</th><th>ì„±ê³µë¥ </th></tr>'
        
        for endpoint, metrics in api_perf.items():
            html += f"""
            <tr>
                <td>{endpoint}</td>
                <td>{metrics.get('avg_response_time', 0):.1f}ms</td>
                <td>{metrics.get('min_response_time', 0):.1f}ms</td>
                <td>{metrics.get('max_response_time', 0):.1f}ms</td>
                <td>{metrics.get('success_rate', 0):.1f}%</td>
            </tr>
            """
        
        html += '</table>'
        return html

    def _generate_load_test_html(self, load_tests: Dict) -> str:
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸ HTML ìƒì„±"""
        if not load_tests:
            return "<p>ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
        
        html = ""
        for test_name, result in load_tests.items():
            html += f"""
            <div class="load-test">
                <h3>{test_name}</h3>
                <p><strong>ì²˜ë¦¬ëŸ‰:</strong> {result.get('requests_per_second', 0):.1f} RPS</p>
                <p><strong>í‰ê·  ì‘ë‹µì‹œê°„:</strong> {result.get('avg_response_time', 0):.1f}ms</p>
                <p><strong>ì—ëŸ¬ìœ¨:</strong> {result.get('error_rate', 0):.1f}%</p>
                <p><strong>ì„±ê³µ/ì‹¤íŒ¨:</strong> {result.get('successful_requests', 0)}/{result.get('failed_requests', 0)}</p>
            </div>
            """
        
        return html

    def _generate_database_performance_html(self, db_perf: Dict) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ HTML ìƒì„±"""
        if not db_perf:
            return "<p>ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
        
        html = f"<p><strong>ì—°ê²° í…ŒìŠ¤íŠ¸:</strong> {'ì„±ê³µ' if db_perf.get('connection_test') else 'ì‹¤íŒ¨'}</p>"
        
        if 'query_performance' in db_perf:
            qp = db_perf['query_performance']
            html += f"<p><strong>ì—°ê²° ì‹œê°„:</strong> {qp.get('connection_time_ms', 0):.1f}ms</p>"
            html += f"<p><strong>í…Œì´ë¸” ìˆ˜:</strong> {qp.get('table_count', 0)}ê°œ</p>"
        
        return html

    def _generate_resource_usage_html(self, resource_usage: Dict) -> str:
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ HTML ìƒì„±"""
        if not resource_usage:
            return "<p>ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
        
        cpu = resource_usage.get('cpu_usage', {})
        memory = resource_usage.get('memory_usage', {})
        
        return f"""
        <p><strong>ëª¨ë‹ˆí„°ë§ ì‹œê°„:</strong> {resource_usage.get('monitoring_duration', 0)}ì´ˆ</p>
        <p><strong>CPU ì‚¬ìš©ë¥ :</strong> í‰ê·  {cpu.get('avg', 0):.1f}%, ìµœëŒ€ {cpu.get('max', 0):.1f}%</p>
        <p><strong>ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ :</strong> í‰ê·  {memory.get('avg', 0):.1f}%, ìµœëŒ€ {memory.get('max', 0):.1f}%</p>
        """

    def _generate_recommendations_html(self, recommendations: List[str]) -> str:
        """ê¶Œì¥ì‚¬í•­ HTML ìƒì„±"""
        if not recommendations:
            return "<p>ê¶Œì¥ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.</p>"
        
        html = "<ul>"
        for rec in recommendations:
            html += f"<li>{rec}</li>"
        html += "</ul>"
        
        return html

    def save_results(self, results: Dict, output_prefix: str = "performance_analysis"):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ë³´ê³ ì„œ
        json_file = f"{output_prefix}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        self.logger.info(f"JSON ë¦¬í¬íŠ¸ ì €ì¥: {json_file}")
        
        # HTML ë³´ê³ ì„œ
        html_file = f"{output_prefix}_{timestamp}.html"
        self.generate_html_report(results, html_file)
        
        # ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„±
        self.generate_performance_charts(results, f"charts_{timestamp}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='StockPilot AI ì„±ëŠ¥ ë¶„ì„ ë„êµ¬')
    parser.add_argument('--project-root', '-p', default='.', help='í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬')
    parser.add_argument('--base-url', '-u', default='http://localhost:8000', help='API ê¸°ë³¸ URL')
    parser.add_argument('--output-prefix', '-o', default='performance_analysis', help='ì¶œë ¥ íŒŒì¼ ì ‘ë‘ì‚¬')
    parser.add_argument('--skip-load-test', action='store_true', help='ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ ë¶„ì„ (ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œê°„ ë‹¨ì¶•)')
    parser.add_argument('--verbose', '-v', action='store_true', help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
    analyzer = PerformanceAnalyzer(args.project_root, args.base_url)
    
    try:
        results = await analyzer.run_comprehensive_analysis()
        
        # ê²°ê³¼ ì €ì¥
        analyzer.save_results(results, args.output_prefix)
        
        # ì½˜ì†” ìš”ì•½ ì¶œë ¥
        overall_score = analyzer._calculate_overall_score(results)
        print(f"\nğŸ“Š ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ")
        print(f"ì „ì²´ ì ìˆ˜: {overall_score}/100")
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ìš”ì•½
        api_perf = results.get('api_performance', {})
        if api_perf:
            avg_times = [metrics.get('avg_response_time', 0) for metrics in api_perf.values()]
            overall_avg = sum(avg_times) / len(avg_times) if avg_times else 0
            print(f"í‰ê·  API ì‘ë‹µì‹œê°„: {overall_avg:.1f}ms")
        
        sys_perf = results.get('system_performance', {})
        if sys_perf:
            print(f"CPU ì‚¬ìš©ë¥ : {sys_perf.get('cpu', {}).get('cpu_percent', 0):.1f}%")
            print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {sys_perf.get('memory', {}).get('percentage', 0):.1f}%")
        
        return 0 if overall_score >= 70 else 1
        
    except Exception as e:
        logging.error(f"ì„±ëŠ¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(asyncio.run(main()))