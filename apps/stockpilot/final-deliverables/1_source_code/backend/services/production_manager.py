#!/usr/bin/env python3
"""
StockPilot í”„ë¡œë•ì…˜ í™˜ê²½ ê´€ë¦¬ì
SSL ì¸ì¦ì„œ ìë™ ê°±ì‹ , ë¡œë“œ ë°¸ëŸ°ì‹±, ë°±ì—… ë° ë³µêµ¬, ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë‹´ë‹¹
"""

import asyncio
import aiohttp
import json
import logging
import subprocess
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import ssl
import socket
import psutil
import time
import os
import sqlite3
import tarfile
import gzip

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class ServiceHealth(Enum):
    """ì„œë¹„ìŠ¤ ìƒíƒœ"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    DOWN = "down"

class BackupType(Enum):
    """ë°±ì—… íƒ€ì…"""
    FULL = "full"
    INCREMENTAL = "incremental"
    CONFIG = "config"
    DATABASE = "database"
    LOGS = "logs"

@dataclass
class SSLCertificateInfo:
    """SSL ì¸ì¦ì„œ ì •ë³´"""
    domain: str
    cert_path: str
    key_path: str
    ca_path: Optional[str] = None
    expiry_date: Optional[datetime] = None
    days_until_expiry: int = 0
    auto_renew: bool = True
    renewal_threshold_days: int = 30

@dataclass
class ServiceEndpoint:
    """ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸"""
    name: str
    url: str
    health_path: str = "/health"
    expected_status: int = 200
    timeout_seconds: int = 10
    critical: bool = True

@dataclass
class LoadBalancerConfig:
    """ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì •"""
    name: str
    frontend_port: int
    backend_servers: List[Dict[str, Any]]
    health_check_interval: int = 30
    max_retries: int = 3
    algorithm: str = "round_robin"  # round_robin, least_conn, ip_hash

@dataclass
class BackupConfig:
    """ë°±ì—… ì„¤ì •"""
    name: str
    backup_type: BackupType
    source_path: str
    destination_path: str
    retention_days: int = 7
    compression: bool = True
    encryption: bool = False
    schedule_cron: str = "0 2 * * *"  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    test_name: str
    endpoint: str
    timestamp: datetime
    total_requests: int
    successful_requests: int
    failed_requests: int
    average_response_time: float
    p95_response_time: float
    p99_response_time: float
    requests_per_second: float
    memory_usage_mb: float
    cpu_usage_percent: float

class ProductionManager:
    """í”„ë¡œë•ì…˜ í™˜ê²½ ê´€ë¦¬ì"""
    
    def __init__(self, config_path: str = "config/production.json"):
        self.config_path = Path(config_path)
        self.ssl_certificates: Dict[str, SSLCertificateInfo] = {}
        self.service_endpoints: List[ServiceEndpoint] = []
        self.load_balancers: Dict[str, LoadBalancerConfig] = {}
        self.backup_configs: Dict[str, BackupConfig] = {}
        self.benchmark_history: List[BenchmarkResult] = []
        
        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
        self.base_dir = Path("/opt/stockpilot")
        self.ssl_dir = self.base_dir / "ssl"
        self.backup_dir = self.base_dir / "backups"
        self.logs_dir = Path("/var/log/stockpilot")
        self.data_dir = self.base_dir / "data"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in [self.ssl_dir, self.backup_dir, self.logs_dir, self.data_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
        self._load_default_configurations()

    def _load_default_configurations(self):
        """ê¸°ë³¸ ì„¤ì • ë¡œë“œ"""
        try:
            # ê¸°ë³¸ SSL ì¸ì¦ì„œ ì„¤ì •
            default_domains = ["stockpilot.ai", "api.stockpilot.ai", "dashboard.stockpilot.ai"]
            for domain in default_domains:
                self.ssl_certificates[domain] = SSLCertificateInfo(
                    domain=domain,
                    cert_path=str(self.ssl_dir / f"{domain}.crt"),
                    key_path=str(self.ssl_dir / f"{domain}.key"),
                    ca_path=str(self.ssl_dir / "ca-bundle.crt"),
                    auto_renew=True,
                    renewal_threshold_days=30
                )
            
            # ê¸°ë³¸ ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸
            self.service_endpoints = [
                ServiceEndpoint("WebSocket Server", "ws://localhost:8765", "/", 101, 5, True),
                ServiceEndpoint("Auth API", "http://localhost:8002", "/health", 200, 10, True),
                ServiceEndpoint("Dashboard API", "http://localhost:8003", "/health", 200, 10, True),
                ServiceEndpoint("Cost Dashboard API", "http://localhost:8004", "/health", 200, 10, False),
            ]
            
            # ê¸°ë³¸ ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì •
            self.load_balancers["api_lb"] = LoadBalancerConfig(
                name="API Load Balancer",
                frontend_port=80,
                backend_servers=[
                    {"host": "localhost", "port": 8002, "weight": 1},
                    {"host": "localhost", "port": 8003, "weight": 1},
                    {"host": "localhost", "port": 8004, "weight": 1}
                ],
                health_check_interval=30,
                max_retries=3,
                algorithm="round_robin"
            )
            
            # ê¸°ë³¸ ë°±ì—… ì„¤ì •
            self.backup_configs = {
                "database_backup": BackupConfig(
                    name="Database Backup",
                    backup_type=BackupType.DATABASE,
                    source_path=str(self.data_dir),
                    destination_path=str(self.backup_dir / "database"),
                    retention_days=30,
                    compression=True,
                    schedule_cron="0 2 * * *"
                ),
                "config_backup": BackupConfig(
                    name="Configuration Backup",
                    backup_type=BackupType.CONFIG,
                    source_path="/opt/stockpilot/config",
                    destination_path=str(self.backup_dir / "config"),
                    retention_days=14,
                    compression=True,
                    schedule_cron="0 3 * * 0"  # ë§¤ì£¼ ì¼ìš”ì¼
                ),
                "logs_backup": BackupConfig(
                    name="Logs Backup",
                    backup_type=BackupType.LOGS,
                    source_path=str(self.logs_dir),
                    destination_path=str(self.backup_dir / "logs"),
                    retention_days=7,
                    compression=True,
                    schedule_cron="0 1 * * *"
                )
            }
            
            logger.info("ê¸°ë³¸ í”„ë¡œë•ì…˜ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì„¤ì • ë¡œë“œ ì˜¤ë¥˜: {e}")

    async def check_ssl_certificates(self) -> Dict[str, Any]:
        """SSL ì¸ì¦ì„œ ìƒíƒœ í™•ì¸"""
        try:
            certificate_status = {
                'total_certificates': len(self.ssl_certificates),
                'valid_certificates': 0,
                'expiring_soon': 0,
                'expired': 0,
                'certificates': {},
                'renewal_needed': [],
                'checked_at': datetime.now().isoformat()
            }
            
            for domain, cert_info in self.ssl_certificates.items():
                try:
                    cert_status = await self._check_single_certificate(cert_info)
                    certificate_status['certificates'][domain] = cert_status
                    
                    if cert_status['valid']:
                        certificate_status['valid_certificates'] += 1
                    
                    if cert_status['days_until_expiry'] <= 0:
                        certificate_status['expired'] += 1
                        certificate_status['renewal_needed'].append(domain)
                    elif cert_status['days_until_expiry'] <= cert_info.renewal_threshold_days:
                        certificate_status['expiring_soon'] += 1
                        if cert_info.auto_renew:
                            certificate_status['renewal_needed'].append(domain)
                    
                except Exception as e:
                    logger.error(f"ì¸ì¦ì„œ {domain} í™•ì¸ ì˜¤ë¥˜: {e}")
                    certificate_status['certificates'][domain] = {
                        'valid': False,
                        'error': str(e),
                        'days_until_expiry': -1
                    }
            
            return certificate_status
            
        except Exception as e:
            logger.error(f"SSL ì¸ì¦ì„œ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}

    async def _check_single_certificate(self, cert_info: SSLCertificateInfo) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¸ì¦ì„œ í™•ì¸"""
        try:
            cert_path = Path(cert_info.cert_path)
            
            if not cert_path.exists():
                return {
                    'valid': False,
                    'error': 'Certificate file not found',
                    'days_until_expiry': -1
                }
            
            # OpenSSLì„ ì‚¬ìš©í•œ ì¸ì¦ì„œ ì •ë³´ ì¶”ì¶œ
            result = subprocess.run([
                'openssl', 'x509', '-in', str(cert_path), 
                '-noout', '-dates', '-subject'
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                return {
                    'valid': False,
                    'error': f'OpenSSL error: {result.stderr}',
                    'days_until_expiry': -1
                }
            
            # ë§Œë£Œì¼ íŒŒì‹±
            output_lines = result.stdout.strip().split('\n')
            expiry_line = next((line for line in output_lines if line.startswith('notAfter=')), None)
            
            if not expiry_line:
                return {
                    'valid': False,
                    'error': 'Cannot parse expiry date',
                    'days_until_expiry': -1
                }
            
            # ë§Œë£Œì¼ ê³„ì‚°
            expiry_str = expiry_line.replace('notAfter=', '')
            try:
                expiry_date = datetime.strptime(expiry_str, '%b %d %H:%M:%S %Y %Z')
            except ValueError:
                # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                try:
                    expiry_date = datetime.strptime(expiry_str, '%b %d %H:%M:%S %Y GMT')
                except ValueError:
                    return {
                        'valid': False,
                        'error': f'Cannot parse expiry date format: {expiry_str}',
                        'days_until_expiry': -1
                    }
            
            # ì¸ì¦ì„œ ì •ë³´ ì—…ë°ì´íŠ¸
            cert_info.expiry_date = expiry_date
            cert_info.days_until_expiry = (expiry_date - datetime.now()).days
            
            return {
                'valid': cert_info.days_until_expiry > 0,
                'expiry_date': expiry_date.isoformat(),
                'days_until_expiry': cert_info.days_until_expiry,
                'auto_renew': cert_info.auto_renew
            }
            
        except Exception as e:
            logger.error(f"ì¸ì¦ì„œ í™•ì¸ ìƒì„¸ ì˜¤ë¥˜: {e}")
            return {
                'valid': False,
                'error': str(e),
                'days_until_expiry': -1
            }

    async def renew_ssl_certificates(self, domains: Optional[List[str]] = None) -> Dict[str, Any]:
        """SSL ì¸ì¦ì„œ ê°±ì‹ """
        try:
            renewal_results = {
                'total_renewals_attempted': 0,
                'successful_renewals': 0,
                'failed_renewals': 0,
                'results': {},
                'renewed_at': datetime.now().isoformat()
            }
            
            domains_to_renew = domains if domains else list(self.ssl_certificates.keys())
            
            for domain in domains_to_renew:
                if domain not in self.ssl_certificates:
                    continue
                
                cert_info = self.ssl_certificates[domain]
                renewal_results['total_renewals_attempted'] += 1
                
                try:
                    # Let's Encrypt Certbotì„ ì‚¬ìš©í•œ ê°±ì‹ 
                    success = await self._renew_certificate_with_certbot(cert_info)
                    
                    if success:
                        renewal_results['successful_renewals'] += 1
                        renewal_results['results'][domain] = {
                            'status': 'success',
                            'message': 'ì¸ì¦ì„œ ê°±ì‹  ì™„ë£Œ'
                        }
                        logger.info(f"ë„ë©”ì¸ {domain} ì¸ì¦ì„œ ê°±ì‹  ì„±ê³µ")
                    else:
                        renewal_results['failed_renewals'] += 1
                        renewal_results['results'][domain] = {
                            'status': 'failed',
                            'message': 'ì¸ì¦ì„œ ê°±ì‹  ì‹¤íŒ¨'
                        }
                        logger.error(f"ë„ë©”ì¸ {domain} ì¸ì¦ì„œ ê°±ì‹  ì‹¤íŒ¨")
                    
                except Exception as e:
                    renewal_results['failed_renewals'] += 1
                    renewal_results['results'][domain] = {
                        'status': 'error',
                        'message': str(e)
                    }
                    logger.error(f"ë„ë©”ì¸ {domain} ê°±ì‹  ì¤‘ ì˜¤ë¥˜: {e}")
            
            return renewal_results
            
        except Exception as e:
            logger.error(f"SSL ì¸ì¦ì„œ ê°±ì‹  ì˜¤ë¥˜: {e}")
            return {'error': str(e)}

    async def _renew_certificate_with_certbot(self, cert_info: SSLCertificateInfo) -> bool:
        """Certbotì„ ì‚¬ìš©í•œ ì¸ì¦ì„œ ê°±ì‹ """
        try:
            # Certbot ê°±ì‹  ëª…ë ¹
            cmd = [
                'certbot', 'renew',
                '--cert-name', cert_info.domain,
                '--quiet',
                '--no-self-upgrade'
            ]
            
            # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì›¹ì„œë²„ ì¤‘ë‹¨ ì—†ëŠ” ê°±ì‹  ë°©ì‹ ì‚¬ìš©
            if os.path.exists('/etc/nginx/nginx.conf'):
                cmd.extend(['--nginx'])
            elif os.path.exists('/etc/apache2/apache2.conf'):
                cmd.extend(['--apache'])
            else:
                # ë…ë¦½ ì‹¤í–‰í˜• ê°±ì‹ 
                cmd.extend(['--standalone', '--preferred-challenges', 'http'])
            
            # ê°±ì‹  ì‹¤í–‰
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"Certbot ê°±ì‹  ì„±ê³µ: {cert_info.domain}")
                return True
            else:
                logger.error(f"Certbot ê°±ì‹  ì‹¤íŒ¨ ({cert_info.domain}): {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"Certbot ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False

    async def check_service_health(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            health_status = {
                'overall_status': ServiceHealth.HEALTHY.value,
                'total_services': len(self.service_endpoints),
                'healthy_services': 0,
                'unhealthy_services': 0,
                'services': {},
                'checked_at': datetime.now().isoformat()
            }
            
            unhealthy_critical_services = 0
            
            for endpoint in self.service_endpoints:
                try:
                    service_health = await self._check_service_endpoint(endpoint)
                    health_status['services'][endpoint.name] = service_health
                    
                    if service_health['status'] == ServiceHealth.HEALTHY.value:
                        health_status['healthy_services'] += 1
                    else:
                        health_status['unhealthy_services'] += 1
                        if endpoint.critical:
                            unhealthy_critical_services += 1
                    
                except Exception as e:
                    logger.error(f"ì„œë¹„ìŠ¤ {endpoint.name} ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
                    health_status['services'][endpoint.name] = {
                        'status': ServiceHealth.DOWN.value,
                        'error': str(e),
                        'critical': endpoint.critical
                    }
                    health_status['unhealthy_services'] += 1
                    if endpoint.critical:
                        unhealthy_critical_services += 1
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            if unhealthy_critical_services > 0:
                health_status['overall_status'] = ServiceHealth.UNHEALTHY.value
            elif health_status['unhealthy_services'] > 0:
                health_status['overall_status'] = ServiceHealth.DEGRADED.value
            
            return health_status
            
        except Exception as e:
            logger.error(f"ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}

    async def _check_service_endpoint(self, endpoint: ServiceEndpoint) -> Dict[str, Any]:
        """ê°œë³„ ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸"""
        try:
            start_time = time.time()
            
            timeout = aiohttp.ClientTimeout(total=endpoint.timeout_seconds)
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                if endpoint.url.startswith('ws://') or endpoint.url.startswith('wss://'):
                    # WebSocket ì—°ê²° í…ŒìŠ¤íŠ¸
                    import aiohttp.client_ws
                    try:
                        async with session.ws_connect(endpoint.url) as ws:
                            await ws.ping()
                            status = ServiceHealth.HEALTHY.value
                            status_code = 101  # WebSocket ì—…ê·¸ë ˆì´ë“œ
                    except Exception as e:
                        status = ServiceHealth.UNHEALTHY.value
                        status_code = 0
                        response_text = str(e)
                else:
                    # HTTP ìš”ì²­ í…ŒìŠ¤íŠ¸
                    async with session.get(f"{endpoint.url}{endpoint.health_path}") as response:
                        status_code = response.status
                        response_text = await response.text()
                        
                        if status_code == endpoint.expected_status:
                            status = ServiceHealth.HEALTHY.value
                        else:
                            status = ServiceHealth.UNHEALTHY.value
            
            response_time = (time.time() - start_time) * 1000  # ms
            
            return {
                'status': status,
                'status_code': status_code,
                'response_time_ms': round(response_time, 2),
                'critical': endpoint.critical,
                'url': endpoint.url,
                'last_checked': datetime.now().isoformat()
            }
            
        except asyncio.TimeoutError:
            return {
                'status': ServiceHealth.DOWN.value,
                'error': 'Connection timeout',
                'critical': endpoint.critical,
                'url': endpoint.url,
                'last_checked': datetime.now().isoformat()
            }
        except Exception as e:
            return {
                'status': ServiceHealth.DOWN.value,
                'error': str(e),
                'critical': endpoint.critical,
                'url': endpoint.url,
                'last_checked': datetime.now().isoformat()
            }

    async def create_backup(self, backup_name: str) -> Dict[str, Any]:
        """ë°±ì—… ìƒì„±"""
        try:
            if backup_name not in self.backup_configs:
                return {'error': f'ë°±ì—… ì„¤ì • {backup_name}ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ'}
            
            config = self.backup_configs[backup_name]
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ë°±ì—… íŒŒì¼ëª… ìƒì„±
            backup_filename = f"{backup_name}_{timestamp}"
            if config.compression:
                backup_filename += ".tar.gz"
            else:
                backup_filename += ".tar"
            
            destination_dir = Path(config.destination_path)
            destination_dir.mkdir(parents=True, exist_ok=True)
            backup_file_path = destination_dir / backup_filename
            
            # ë°±ì—… ì‹¤í–‰
            success = await self._execute_backup(config, backup_file_path)
            
            if success:
                # ë°±ì—… íŒŒì¼ ê²€ì¦
                if backup_file_path.exists():
                    file_size = backup_file_path.stat().st_size
                    
                    # ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬
                    await self._cleanup_old_backups(config)
                    
                    return {
                        'status': 'success',
                        'backup_file': str(backup_file_path),
                        'file_size_bytes': file_size,
                        'file_size_mb': round(file_size / (1024 * 1024), 2),
                        'created_at': datetime.now().isoformat(),
                        'retention_days': config.retention_days
                    }
                else:
                    return {'status': 'failed', 'error': 'ë°±ì—… íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ'}
            else:
                return {'status': 'failed', 'error': 'ë°±ì—… ì‹¤í–‰ ì‹¤íŒ¨'}
            
        except Exception as e:
            logger.error(f"ë°±ì—… ìƒì„± ì˜¤ë¥˜ ({backup_name}): {e}")
            return {'status': 'error', 'error': str(e)}

    async def _execute_backup(self, config: BackupConfig, backup_file_path: Path) -> bool:
        """ë°±ì—… ì‹¤í–‰"""
        try:
            source_path = Path(config.source_path)
            
            if not source_path.exists():
                logger.error(f"ë°±ì—… ì†ŒìŠ¤ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {source_path}")
                return False
            
            # tar ëª…ë ¹ êµ¬ì„±
            tar_cmd = ['tar']
            
            if config.compression:
                tar_cmd.append('-czf')
            else:
                tar_cmd.append('-cf')
            
            tar_cmd.extend([str(backup_file_path), '-C', str(source_path.parent), source_path.name])
            
            # íŠ¹ì • ë°±ì—… íƒ€ì…ë³„ ì¶”ê°€ ì²˜ë¦¬
            if config.backup_type == BackupType.DATABASE:
                # ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì „ íŠ¹ë³„ ì²˜ë¦¬
                await self._prepare_database_backup(source_path)
            
            # ë°±ì—… ì‹¤í–‰
            result = subprocess.run(tar_cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"ë°±ì—… ìƒì„± ì™„ë£Œ: {backup_file_path}")
                return True
            else:
                logger.error(f"tar ë°±ì—… ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"ë°±ì—… ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False

    async def _prepare_database_backup(self, db_path: Path):
        """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì „ ì¤€ë¹„"""
        try:
            # SQLite ë°ì´í„°ë² ì´ìŠ¤ë“¤ì— ëŒ€í•´ VACUUM ì‹¤í–‰
            for db_file in db_path.glob("*.db"):
                try:
                    conn = sqlite3.connect(db_file)
                    conn.execute("VACUUM")
                    conn.close()
                    logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ {db_file.name} VACUUM ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ {db_file.name} VACUUM ì‹¤íŒ¨: {e}")
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì¤€ë¹„ ì˜¤ë¥˜: {e}")

    async def _cleanup_old_backups(self, config: BackupConfig):
        """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬"""
        try:
            destination_dir = Path(config.destination_path)
            if not destination_dir.exists():
                return
            
            cutoff_date = datetime.now() - timedelta(days=config.retention_days)
            
            # ë°±ì—… íŒŒì¼ íŒ¨í„´ ë§¤ì¹­
            pattern = f"{config.name.lower().replace(' ', '_')}_*"
            backup_files = list(destination_dir.glob(pattern))
            
            cleaned_files = 0
            for backup_file in backup_files:
                try:
                    file_mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
                    if file_mtime < cutoff_date:
                        backup_file.unlink()
                        cleaned_files += 1
                        logger.info(f"ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì‚­ì œ: {backup_file.name}")
                except Exception as e:
                    logger.error(f"ë°±ì—… íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜ ({backup_file}): {e}")
            
            if cleaned_files > 0:
                logger.info(f"{cleaned_files}ê°œ ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"ë°±ì—… íŒŒì¼ ì •ë¦¬ ì˜¤ë¥˜: {e}")

    async def restore_backup(self, backup_file_path: str, restore_path: Optional[str] = None) -> Dict[str, Any]:
        """ë°±ì—… ë³µì›"""
        try:
            backup_file = Path(backup_file_path)
            
            if not backup_file.exists():
                return {'status': 'error', 'error': 'ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ'}
            
            # ë³µì› ê²½ë¡œ ê²°ì •
            if restore_path:
                target_path = Path(restore_path)
            else:
                # ì›ë³¸ ê²½ë¡œë¡œ ë³µì› (ìœ„í—˜í•˜ë¯€ë¡œ í™•ì¸ í•„ìš”)
                target_path = self.base_dir / "restore" / datetime.now().strftime('%Y%m%d_%H%M%S')
            
            target_path.mkdir(parents=True, exist_ok=True)
            
            # tar ì••ì¶• í•´ì œ
            if backup_file.suffix == '.gz':
                tar_cmd = ['tar', '-xzf', str(backup_file), '-C', str(target_path)]
            else:
                tar_cmd = ['tar', '-xf', str(backup_file), '-C', str(target_path)]
            
            result = subprocess.run(tar_cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                # ë³µì›ëœ íŒŒì¼ë“¤ í™•ì¸
                restored_files = list(target_path.rglob('*'))
                file_count = len([f for f in restored_files if f.is_file()])
                
                return {
                    'status': 'success',
                    'restored_to': str(target_path),
                    'restored_files': file_count,
                    'restored_at': datetime.now().isoformat()
                }
            else:
                return {
                    'status': 'failed',
                    'error': f'ë³µì› ì‹¤íŒ¨: {result.stderr}'
                }
                
        except Exception as e:
            logger.error(f"ë°±ì—… ë³µì› ì˜¤ë¥˜: {e}")
            return {'status': 'error', 'error': str(e)}

    async def run_performance_benchmark(self, test_duration: int = 60) -> List[BenchmarkResult]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            benchmark_results = []
            test_scenarios = [
                {
                    'name': 'API Health Check Load Test',
                    'endpoint': 'http://localhost:8002/health',
                    'concurrent_users': 10,
                    'requests_per_user': 100
                },
                {
                    'name': 'Dashboard API Load Test',
                    'endpoint': 'http://localhost:8003/health',
                    'concurrent_users': 20,
                    'requests_per_user': 50
                },
                {
                    'name': 'Cost Dashboard Stress Test',
                    'endpoint': 'http://localhost:8004/health',
                    'concurrent_users': 5,
                    'requests_per_user': 200
                }
            ]
            
            for scenario in test_scenarios:
                logger.info(f"ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘: {scenario['name']}")
                
                try:
                    result = await self._run_single_benchmark(scenario, test_duration)
                    if result:
                        benchmark_results.append(result)
                        logger.info(f"ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {scenario['name']} - "
                                  f"RPS: {result.requests_per_second:.2f}")
                except Exception as e:
                    logger.error(f"ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ({scenario['name']}): {e}")
            
            # ê²°ê³¼ë¥¼ íˆìŠ¤í† ë¦¬ì— ì €ì¥
            self.benchmark_history.extend(benchmark_results)
            
            return benchmark_results
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return []

    async def _run_single_benchmark(self, scenario: Dict[str, Any], duration: int) -> Optional[BenchmarkResult]:
        """ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            endpoint = scenario['endpoint']
            concurrent_users = scenario['concurrent_users']
            requests_per_user = scenario['requests_per_user']
            
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¸¡ì • ì‹œì‘
            process = psutil.Process()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB
            start_cpu = psutil.cpu_percent(interval=1)
            start_time = time.time()
            
            # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
            response_times = []
            successful_requests = 0
            failed_requests = 0
            
            async def make_requests(session: aiohttp.ClientSession, user_id: int) -> List[float]:
                user_response_times = []
                for _ in range(requests_per_user):
                    try:
                        request_start = time.time()
                        async with session.get(endpoint, timeout=aiohttp.ClientTimeout(total=10)) as response:
                            await response.text()  # ì‘ë‹µ ë‚´ìš© ì½ê¸°
                            request_time = (time.time() - request_start) * 1000  # ms
                            user_response_times.append(request_time)
                            
                            if response.status == 200:
                                nonlocal successful_requests
                                successful_requests += 1
                            else:
                                nonlocal failed_requests
                                failed_requests += 1
                    except Exception as e:
                        failed_requests += 1
                        logger.debug(f"ìš”ì²­ ì‹¤íŒ¨ (User {user_id}): {e}")
                
                return user_response_times
            
            # ë™ì‹œ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜
            async with aiohttp.ClientSession() as session:
                tasks = []
                for user_id in range(concurrent_users):
                    task = make_requests(session, user_id)
                    tasks.append(task)
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ì‘ë‹µ ì‹œê°„ ì§‘ê³„
                for user_times in results:
                    if isinstance(user_times, list):
                        response_times.extend(user_times)
            
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¸¡ì • ì¢…ë£Œ
            end_time = time.time()
            end_memory = process.memory_info().rss / 1024 / 1024  # MB
            end_cpu = psutil.cpu_percent(interval=1)
            
            # í†µê³„ ê³„ì‚°
            if response_times:
                response_times.sort()
                total_requests = successful_requests + failed_requests
                test_duration_actual = end_time - start_time
                
                # ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
                p95_index = int(len(response_times) * 0.95)
                p99_index = int(len(response_times) * 0.99)
                
                result = BenchmarkResult(
                    test_name=scenario['name'],
                    endpoint=endpoint,
                    timestamp=datetime.now(),
                    total_requests=total_requests,
                    successful_requests=successful_requests,
                    failed_requests=failed_requests,
                    average_response_time=sum(response_times) / len(response_times),
                    p95_response_time=response_times[p95_index] if p95_index < len(response_times) else response_times[-1],
                    p99_response_time=response_times[p99_index] if p99_index < len(response_times) else response_times[-1],
                    requests_per_second=total_requests / test_duration_actual,
                    memory_usage_mb=max(start_memory, end_memory),
                    cpu_usage_percent=max(start_cpu, end_cpu)
                )
                
                return result
            
            return None
            
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return None

    async def setup_load_balancer(self, lb_name: str) -> Dict[str, Any]:
        """ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì •"""
        try:
            if lb_name not in self.load_balancers:
                return {'error': f'ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì • {lb_name}ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ'}
            
            config = self.load_balancers[lb_name]
            
            # Nginx ì„¤ì • íŒŒì¼ ìƒì„±
            nginx_config = self._generate_nginx_config(config)
            
            # ì„¤ì • íŒŒì¼ ì €ì¥
            nginx_conf_path = Path(f"/etc/nginx/sites-available/stockpilot_{lb_name}")
            nginx_conf_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(nginx_conf_path, 'w') as f:
                f.write(nginx_config)
            
            # ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
            nginx_enabled_path = Path(f"/etc/nginx/sites-enabled/stockpilot_{lb_name}")
            if not nginx_enabled_path.exists():
                nginx_enabled_path.symlink_to(nginx_conf_path)
            
            # Nginx ì„¤ì • í…ŒìŠ¤íŠ¸
            test_result = subprocess.run(['nginx', '-t'], capture_output=True, text=True)
            
            if test_result.returncode == 0:
                # Nginx ì¬ë¡œë“œ
                reload_result = subprocess.run(['systemctl', 'reload', 'nginx'], capture_output=True, text=True)
                
                if reload_result.returncode == 0:
                    return {
                        'status': 'success',
                        'message': f'ë¡œë“œ ë°¸ëŸ°ì„œ {lb_name} ì„¤ì • ì™„ë£Œ',
                        'config_file': str(nginx_conf_path),
                        'frontend_port': config.frontend_port,
                        'backend_servers': len(config.backend_servers)
                    }
                else:
                    return {
                        'status': 'failed',
                        'error': f'Nginx ì¬ë¡œë“œ ì‹¤íŒ¨: {reload_result.stderr}'
                    }
            else:
                return {
                    'status': 'failed',
                    'error': f'Nginx ì„¤ì • í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_result.stderr}'
                }
                
        except Exception as e:
            logger.error(f"ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì • ì˜¤ë¥˜ ({lb_name}): {e}")
            return {'status': 'error', 'error': str(e)}

    def _generate_nginx_config(self, config: LoadBalancerConfig) -> str:
        """Nginx ì„¤ì • íŒŒì¼ ìƒì„±"""
        upstream_servers = []
        for server in config.backend_servers:
            weight = server.get('weight', 1)
            upstream_servers.append(f"    server {server['host']}:{server['port']} weight={weight};")
        
        upstream_block = '\n'.join(upstream_servers)
        
        nginx_config = f"""
upstream stockpilot_backend_{config.name.lower().replace(' ', '_')} {{
    {config.algorithm};
{upstream_block}
    
    # í—¬ìŠ¤ ì²´í¬ ì„¤ì •
    keepalive 32;
}}

server {{
    listen {config.frontend_port};
    server_name stockpilot.ai *.stockpilot.ai;
    
    # ë³´ì•ˆ í—¤ë”
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains";
    
    # ë¡œê¹…
    access_log /var/log/nginx/stockpilot_access.log;
    error_log /var/log/nginx/stockpilot_error.log;
    
    # í”„ë¡ì‹œ ì„¤ì •
    location / {{
        proxy_pass http://stockpilot_backend_{config.name.lower().replace(' ', '_')};
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
        
        # ë²„í¼ë§ ì„¤ì •
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
        
        # HTTP/1.1 ì§€ì›
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }}
    
    # í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
    location /nginx-health {{
        access_log off;
        return 200 "healthy\\n";
        add_header Content-Type text/plain;
    }}
    
    # ì •ì  íŒŒì¼ ìºì‹±
    location ~* \\.(js|css|png|jpg|jpeg|gif|ico|svg)$ {{
        expires 1y;
        add_header Cache-Control "public, immutable";
    }}
}}
"""
        
        return nginx_config

    async def get_production_status(self) -> Dict[str, Any]:
        """í”„ë¡œë•ì…˜ í™˜ê²½ ì „ì²´ ìƒíƒœ ì¡°íšŒ"""
        try:
            status = {
                'timestamp': datetime.now().isoformat(),
                'ssl_certificates': await self.check_ssl_certificates(),
                'service_health': await self.check_service_health(),
                'system_resources': self._get_system_resources(),
                'disk_usage': self._get_disk_usage(),
                'recent_backups': await self._get_recent_backup_info(),
                'benchmark_summary': self._get_benchmark_summary(),
                'uptime': self._get_system_uptime()
            }
            
            return status
            
        except Exception as e:
            logger.error(f"í”„ë¡œë•ì…˜ ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}

    def _get_system_resources(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´"""
        try:
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
            
            return {
                'cpu_percent': cpu_percent,
                'memory_total_gb': round(memory.total / (1024**3), 2),
                'memory_used_gb': round(memory.used / (1024**3), 2),
                'memory_percent': memory.percent,
                'load_average': os.getloadavg() if hasattr(os, 'getloadavg') else None
            }
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}

    def _get_disk_usage(self) -> Dict[str, Any]:
        """ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì •ë³´"""
        try:
            disk_info = {}
            
            # ì£¼ìš” ê²½ë¡œë“¤ì˜ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            important_paths = [
                ('/', 'Root'),
                ('/opt/stockpilot', 'Application'),
                ('/var/log', 'Logs'),
                ('/tmp', 'Temporary')
            ]
            
            for path, name in important_paths:
                if os.path.exists(path):
                    usage = shutil.disk_usage(path)
                    disk_info[name.lower()] = {
                        'total_gb': round(usage.total / (1024**3), 2),
                        'used_gb': round(usage.used / (1024**3), 2),
                        'free_gb': round(usage.free / (1024**3), 2),
                        'percent_used': round((usage.used / usage.total) * 100, 1)
                    }
            
            return disk_info
            
        except Exception as e:
            logger.error(f"ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}

    async def _get_recent_backup_info(self) -> Dict[str, Any]:
        """ìµœê·¼ ë°±ì—… ì •ë³´"""
        try:
            backup_info = {}
            
            for backup_name, config in self.backup_configs.items():
                destination_dir = Path(config.destination_path)
                if destination_dir.exists():
                    backup_files = list(destination_dir.glob(f"{backup_name.lower().replace(' ', '_')}_*"))
                    backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                    
                    if backup_files:
                        latest_backup = backup_files[0]
                        backup_info[backup_name] = {
                            'latest_backup': latest_backup.name,
                            'backup_date': datetime.fromtimestamp(latest_backup.stat().st_mtime).isoformat(),
                            'file_size_mb': round(latest_backup.stat().st_size / (1024*1024), 2),
                            'total_backups': len(backup_files)
                        }
                    else:
                        backup_info[backup_name] = {
                            'status': 'no_backups_found'
                        }
                else:
                    backup_info[backup_name] = {
                        'status': 'backup_directory_not_found'
                    }
            
            return backup_info
            
        except Exception as e:
            logger.error(f"ë°±ì—… ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}

    def _get_benchmark_summary(self) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ì •ë³´"""
        try:
            if not self.benchmark_history:
                return {'status': 'no_benchmarks_run'}
            
            recent_benchmarks = sorted(self.benchmark_history, key=lambda x: x.timestamp, reverse=True)[:10]
            
            summary = {
                'total_benchmarks_run': len(self.benchmark_history),
                'latest_benchmark_date': recent_benchmarks[0].timestamp.isoformat(),
                'average_rps': round(sum(b.requests_per_second for b in recent_benchmarks) / len(recent_benchmarks), 2),
                'average_response_time_ms': round(sum(b.average_response_time for b in recent_benchmarks) / len(recent_benchmarks), 2),
                'success_rate_percent': round(sum(b.successful_requests / max(b.total_requests, 1) for b in recent_benchmarks) / len(recent_benchmarks) * 100, 1)
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}

    def _get_system_uptime(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê°€ë™ì‹œê°„"""
        try:
            boot_time = psutil.boot_time()
            uptime_seconds = time.time() - boot_time
            uptime_days = int(uptime_seconds // 86400)
            uptime_hours = int((uptime_seconds % 86400) // 3600)
            uptime_minutes = int((uptime_seconds % 3600) // 60)
            
            return {
                'uptime_seconds': int(uptime_seconds),
                'uptime_formatted': f"{uptime_days}ì¼ {uptime_hours}ì‹œê°„ {uptime_minutes}ë¶„",
                'boot_time': datetime.fromtimestamp(boot_time).isoformat()
            }
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ê°€ë™ì‹œê°„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}

# ì „ì—­ í”„ë¡œë•ì…˜ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
production_manager = None

def get_production_manager() -> ProductionManager:
    """í”„ë¡œë•ì…˜ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global production_manager
    if production_manager is None:
        production_manager = ProductionManager()
    return production_manager

async def run_production_health_check():
    """í”„ë¡œë•ì…˜ í™˜ê²½ ìƒíƒœ ì ê²€ ì‹¤í–‰"""
    try:
        manager = get_production_manager()
        
        logger.info("ğŸ” í”„ë¡œë•ì…˜ í™˜ê²½ ìƒíƒœ ì ê²€ ì‹œì‘")
        
        # SSL ì¸ì¦ì„œ í™•ì¸
        ssl_status = await manager.check_ssl_certificates()
        if ssl_status.get('renewal_needed'):
            logger.warning(f"ê°±ì‹  í•„ìš”í•œ SSL ì¸ì¦ì„œ: {ssl_status['renewal_needed']}")
            await manager.renew_ssl_certificates(ssl_status['renewal_needed'])
        
        # ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        service_status = await manager.check_service_health()
        if service_status.get('overall_status') != 'healthy':
            logger.warning(f"ì„œë¹„ìŠ¤ ìƒíƒœ ì´ìƒ: {service_status['overall_status']}")
        
        # ì „ì²´ ìƒíƒœ ë¦¬í¬íŠ¸
        full_status = await manager.get_production_status()
        logger.info(f"âœ… í”„ë¡œë•ì…˜ í™˜ê²½ ì ê²€ ì™„ë£Œ - SSL: {ssl_status.get('valid_certificates', 0)}ê°œ, ì„œë¹„ìŠ¤: {service_status.get('healthy_services', 0)}ê°œ")
        
        return full_status
        
    except Exception as e:
        logger.error(f"í”„ë¡œë•ì…˜ ìƒíƒœ ì ê²€ ì˜¤ë¥˜: {e}")
        return {'error': str(e)}

if __name__ == "__main__":
    asyncio.run(run_production_health_check())