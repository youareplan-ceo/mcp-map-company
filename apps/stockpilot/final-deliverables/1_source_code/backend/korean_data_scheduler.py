#!/usr/bin/env python3
"""
StockPilot KR ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬
ì£¼ê¸°ì  ë°ì´í„° ìˆ˜ì§‘, í’ˆì§ˆ ê²€ì¦, ìºì‹œ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì„œë¹„ìŠ¤
"""

import asyncio
import signal
import sys
import os
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import schedule
import time
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.korean_data_sources import create_korean_data_manager, DataSourceType

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/stockpilot/korean_data_scheduler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class KoreanDataScheduler:
    """KR ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self):
        self.running = False
        self.data_manager = None
        self.collection_stats = {
            'total_collections': 0,
            'successful_collections': 0,
            'failed_collections': 0,
            'last_collection': None,
            'start_time': datetime.now()
        }
        
        # ìˆ˜ì§‘ ì¼ì • ì„¤ì •
        self.collection_schedule = {
            DataSourceType.DART_DISCLOSURE: {
                'frequency': '1h',  # 1ì‹œê°„ë§ˆë‹¤
                'active_hours': (9, 18),  # 9ì‹œ-18ì‹œë§Œ í™œì„±
                'priority': 1
            },
            DataSourceType.SECURITIES_REPORT: {
                'frequency': '6h',  # 6ì‹œê°„ë§ˆë‹¤
                'active_hours': (8, 20),  # 8ì‹œ-20ì‹œ
                'priority': 2
            },
            DataSourceType.CALENDAR_DATA: {
                'frequency': '30m',  # 30ë¶„ë§ˆë‹¤
                'active_hours': (6, 22),  # 6ì‹œ-22ì‹œ
                'priority': 1
            },
            DataSourceType.PRICE_DATA: {
                'frequency': '5m',   # 5ë¶„ë§ˆë‹¤ (ì¥ì¤‘)
                'active_hours': (9, 15),  # ì¥ì‹œê°„ë§Œ
                'priority': 1
            },
            DataSourceType.NEWS_DATA: {
                'frequency': '15m',  # 15ë¶„ë§ˆë‹¤
                'active_hours': (6, 23),  # 6ì‹œ-23ì‹œ
                'priority': 2
            }
        }

    async def setup(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”"""
        try:
            # ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”
            self.data_manager = await create_korean_data_manager()
            
            # ìº˜ë¦°ë” ì´ì¤‘í™” ì„¤ì •
            await self.data_manager.setup_calendar_redundancy()
            
            # ìºì‹œ TTL ì„¤ì •
            await self.data_manager.configure_cache_ttl()
            
            # ìŠ¤ì¼€ì¤„ ì„¤ì •
            self._setup_schedules()
            
            logger.info("KR ë°ì´í„° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _setup_schedules(self):
        """ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        try:
            # DART ì „ìê³µì‹œ (1ì‹œê°„ë§ˆë‹¤)
            schedule.every().hour.do(
                self._schedule_collection, 
                DataSourceType.DART_DISCLOSURE, 
                "DART ì „ìê³µì‹œ ìˆ˜ì§‘"
            )
            
            # ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ (6ì‹œê°„ë§ˆë‹¤)
            schedule.every(6).hours.do(
                self._schedule_collection,
                DataSourceType.SECURITIES_REPORT,
                "ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"
            )
            
            # ìº˜ë¦°ë” ë°ì´í„° (30ë¶„ë§ˆë‹¤)
            schedule.every(30).minutes.do(
                self._schedule_collection,
                DataSourceType.CALENDAR_DATA,
                "ìº˜ë¦°ë” ë°ì´í„° ìˆ˜ì§‘"
            )
            
            # ê°€ê²© ë°ì´í„° (5ë¶„ë§ˆë‹¤, ì¥ì‹œê°„ë§Œ)
            schedule.every(5).minutes.do(
                self._schedule_trading_hours_collection,
                DataSourceType.PRICE_DATA,
                "ê°€ê²© ë°ì´í„° ìˆ˜ì§‘"
            )
            
            # ë‰´ìŠ¤ ë°ì´í„° (15ë¶„ë§ˆë‹¤)
            schedule.every(15).minutes.do(
                self._schedule_collection,
                DataSourceType.NEWS_DATA,
                "ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"
            )
            
            # í’ˆì§ˆ ê²€ì¦ (ë§¤ì¼ ìì •)
            schedule.every().day.at("00:00").do(
                self._schedule_quality_validation,
                "ì¼ì¼ í’ˆì§ˆ ê²€ì¦"
            )
            
            # ìºì‹œ ì •ë¦¬ (ë§¤ì¼ ìƒˆë²½ 3ì‹œ)
            schedule.every().day.at("03:00").do(
                self._schedule_cache_cleanup,
                "ìºì‹œ ì •ë¦¬"
            )
            
            # í†µê³„ ë¦¬í¬íŠ¸ (ë§¤ì¼ ì˜¤ì „ 9ì‹œ)
            schedule.every().day.at("09:00").do(
                self._schedule_daily_report,
                "ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±"
            )
            
            logger.info("ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¤„ ì„¤ì • ì˜¤ë¥˜: {e}")

    def _schedule_collection(self, source_type: DataSourceType, task_name: str):
        """ì¼ë°˜ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„"""
        try:
            # í™œì„± ì‹œê°„ëŒ€ ì²´í¬
            current_hour = datetime.now().hour
            schedule_config = self.collection_schedule.get(source_type, {})
            active_hours = schedule_config.get('active_hours', (0, 24))
            
            if not (active_hours[0] <= current_hour <= active_hours[1]):
                logger.debug(f"{task_name} - ë¹„í™œì„± ì‹œê°„ëŒ€ë¡œ ê±´ë„ˆëœ€")
                return
            
            # ë¹„ë™ê¸° ìˆ˜ì§‘ ì‘ì—… ì‹¤í–‰
            asyncio.create_task(self._execute_collection(source_type, task_name))
            
        except Exception as e:
            logger.error(f"{task_name} ìŠ¤ì¼€ì¤„ë§ ì˜¤ë¥˜: {e}")

    def _schedule_trading_hours_collection(self, source_type: DataSourceType, task_name: str):
        """ì¥ì‹œê°„ í•œì • ë°ì´í„° ìˆ˜ì§‘"""
        try:
            current_time = datetime.now()
            current_hour = current_time.hour
            
            # í‰ì¼ì¸ì§€ í™•ì¸ (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
            if current_time.weekday() >= 5:  # ì£¼ë§
                logger.debug(f"{task_name} - ì£¼ë§ë¡œ ê±´ë„ˆëœ€")
                return
            
            # ì¥ì‹œê°„ ì²´í¬ (9:00-15:30)
            if not (9 <= current_hour < 15 or (current_hour == 15 and current_time.minute <= 30)):
                logger.debug(f"{task_name} - ì¥ì‹œê°„ ì™¸ë¡œ ê±´ë„ˆëœ€")
                return
            
            # ë¹„ë™ê¸° ìˆ˜ì§‘ ì‘ì—… ì‹¤í–‰
            asyncio.create_task(self._execute_collection(source_type, task_name))
            
        except Exception as e:
            logger.error(f"{task_name} ì¥ì‹œê°„ ìŠ¤ì¼€ì¤„ë§ ì˜¤ë¥˜: {e}")

    def _schedule_quality_validation(self, task_name: str):
        """í’ˆì§ˆ ê²€ì¦ ìŠ¤ì¼€ì¤„"""
        try:
            asyncio.create_task(self._execute_quality_validation(task_name))
        except Exception as e:
            logger.error(f"{task_name} ìŠ¤ì¼€ì¤„ë§ ì˜¤ë¥˜: {e}")

    def _schedule_cache_cleanup(self, task_name: str):
        """ìºì‹œ ì •ë¦¬ ìŠ¤ì¼€ì¤„"""
        try:
            asyncio.create_task(self._execute_cache_cleanup(task_name))
        except Exception as e:
            logger.error(f"{task_name} ìŠ¤ì¼€ì¤„ë§ ì˜¤ë¥˜: {e}")

    def _schedule_daily_report(self, task_name: str):
        """ì¼ì¼ ë¦¬í¬íŠ¸ ìŠ¤ì¼€ì¤„"""
        try:
            asyncio.create_task(self._execute_daily_report(task_name))
        except Exception as e:
            logger.error(f"{task_name} ìŠ¤ì¼€ì¤„ë§ ì˜¤ë¥˜: {e}")

    async def _execute_collection(self, source_type: DataSourceType, task_name: str):
        """ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ”„ {task_name} ì‹œì‘")
            start_time = datetime.now()
            
            self.collection_stats['total_collections'] += 1
            
            if not self.data_manager:
                logger.error("ë°ì´í„° ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                self.collection_stats['failed_collections'] += 1
                return
            
            success_count = 0
            error_count = 0
            
            # í•´ë‹¹ íƒ€ì…ì˜ ëª¨ë“  ì†ŒìŠ¤ì— ëŒ€í•´ ìˆ˜ì§‘ ì‹¤í–‰
            for source_id, source in self.data_manager.data_sources.items():
                if source.source_type != source_type or not source.enabled:
                    continue
                
                try:
                    if source_type == DataSourceType.SECURITIES_REPORT:
                        items = await self.data_manager.collect_securities_reports(source_id)
                        success_count += len(items) if items else 0
                        
                    elif source_type == DataSourceType.DART_DISCLOSURE:
                        # DART ê³µì‹œ ìˆ˜ì§‘ (ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´)
                        disclosure_data = await self.data_manager.fetch_dart_disclosure_details(
                            "00126380", "ì‚¬ì—…ë³´ê³ ì„œ"  # ì‚¼ì„±ì „ì ì˜ˆì‹œ
                        )
                        if disclosure_data.get('items'):
                            success_count += len(disclosure_data['items'])
                        
                    elif source_type == DataSourceType.CALENDAR_DATA:
                        # ìº˜ë¦°ë” ë°ì´í„° ìˆ˜ì§‘ (êµ¬í˜„ í•„ìš”)
                        logger.info(f"ìº˜ë¦°ë” ë°ì´í„° ìˆ˜ì§‘: {source_id}")
                        success_count += 1
                        
                    elif source_type == DataSourceType.PRICE_DATA:
                        # ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ (êµ¬í˜„ í•„ìš”)
                        logger.info(f"ê°€ê²© ë°ì´í„° ìˆ˜ì§‘: {source_id}")
                        success_count += 1
                        
                    elif source_type == DataSourceType.NEWS_DATA:
                        # ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (êµ¬í˜„ í•„ìš”)
                        logger.info(f"ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘: {source_id}")
                        success_count += 1
                    
                    # ì†ŒìŠ¤ ì—…ë°ì´íŠ¸ ì‹œê°„ ê°±ì‹ 
                    source.last_updated = datetime.now()
                    
                except Exception as e:
                    logger.error(f"ì†ŒìŠ¤ {source_id} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                    source.error_count += 1
                    error_count += 1
            
            # ìˆ˜ì§‘ ì™„ë£Œ ì²˜ë¦¬
            duration = (datetime.now() - start_time).total_seconds()
            
            if error_count == 0:
                self.collection_stats['successful_collections'] += 1
                logger.info(f"âœ… {task_name} ì™„ë£Œ - ì„±ê³µ: {success_count}ê°œ, ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ")
            else:
                self.collection_stats['failed_collections'] += 1
                logger.warning(f"âš ï¸ {task_name} ë¶€ë¶„ ì™„ë£Œ - ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {error_count}ê°œ, ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ")
            
            self.collection_stats['last_collection'] = datetime.now()
            
        except Exception as e:
            logger.error(f"{task_name} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            self.collection_stats['failed_collections'] += 1

    async def _execute_quality_validation(self, task_name: str):
        """í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ” {task_name} ì‹œì‘")
            start_time = datetime.now()
            
            if not self.data_manager:
                logger.error("ë°ì´í„° ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return
            
            # ëª¨ë“  ì†ŒìŠ¤ì— ëŒ€í•´ í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰
            reports = await self.data_manager.run_quality_validation()
            
            # í’ˆì§ˆ ë¦¬í¬íŠ¸ ìš”ì•½
            total_sources = len(reports)
            high_quality = sum(1 for r in reports if r.quality_score >= 80)
            low_quality = sum(1 for r in reports if r.quality_score < 50)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"âœ… {task_name} ì™„ë£Œ - ì´ {total_sources}ê°œ ì†ŒìŠ¤, ê³ í’ˆì§ˆ: {high_quality}ê°œ, ì €í’ˆì§ˆ: {low_quality}ê°œ, ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ")
            
            # ì €í’ˆì§ˆ ì†ŒìŠ¤ì— ëŒ€í•œ ê²½ê³ 
            if low_quality > 0:
                low_quality_sources = [r.source_id for r in reports if r.quality_score < 50]
                logger.warning(f"âš ï¸ ì €í’ˆì§ˆ ì†ŒìŠ¤ ë°œê²¬: {', '.join(low_quality_sources)}")
            
        except Exception as e:
            logger.error(f"{task_name} ì‹¤í–‰ ì˜¤ë¥˜: {e}")

    async def _execute_cache_cleanup(self, task_name: str):
        """ìºì‹œ ì •ë¦¬ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ§¹ {task_name} ì‹œì‘")
            start_time = datetime.now()
            
            if not self.data_manager or not self.data_manager.redis_client:
                logger.error("Redis í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return
            
            # ë§Œë£Œëœ ìºì‹œ í‚¤ íŒ¨í„´ë“¤
            cache_patterns = [
                "data_cache:*",
                "throttle:*",
                "emergency_stop:*",
                "cost_metric:*"
            ]
            
            cleaned_keys = 0
            
            for pattern in cache_patterns:
                keys = await self.data_manager.redis_client.keys(pattern)
                if keys:
                    # TTLì´ 0 ì´í•˜ì¸ í‚¤ë“¤ ì •ë¦¬
                    for key in keys:
                        ttl = await self.data_manager.redis_client.ttl(key)
                        if ttl == -1:  # TTLì´ ì„¤ì •ë˜ì§€ ì•Šì€ í‚¤
                            await self.data_manager.redis_client.delete(key)
                            cleaned_keys += 1
            
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… {task_name} ì™„ë£Œ - {cleaned_keys}ê°œ í‚¤ ì •ë¦¬, ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ")
            
        except Exception as e:
            logger.error(f"{task_name} ì‹¤í–‰ ì˜¤ë¥˜: {e}")

    async def _execute_daily_report(self, task_name: str):
        """ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            logger.info(f"ğŸ“Š {task_name} ì‹œì‘")
            start_time = datetime.now()
            
            if not self.data_manager:
                logger.error("ë°ì´í„° ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return
            
            # ë°ì´í„° ì†ŒìŠ¤ ìƒíƒœ ì¡°íšŒ
            status = await self.data_manager.get_data_source_status()
            
            # ìˆ˜ì§‘ í†µê³„
            uptime_hours = (datetime.now() - self.collection_stats['start_time']).total_seconds() / 3600
            success_rate = (self.collection_stats['successful_collections'] / 
                          max(self.collection_stats['total_collections'], 1)) * 100
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = f"""
ğŸ“Š StockPilot KR ë°ì´í„° ìˆ˜ì§‘ ì¼ì¼ ë¦¬í¬íŠ¸ ({datetime.now().strftime('%Y-%m-%d')})

ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:
- ì´ ë°ì´í„° ì†ŒìŠ¤: {status.get('total_sources', 0)}ê°œ
- í™œì„± ì†ŒìŠ¤: {status.get('active_sources', 0)}ê°œ
- í’ˆì§ˆ ìš°ìˆ˜: {status.get('quality_summary', {}).get('excellent', 0)}ê°œ
- í’ˆì§ˆ ì–‘í˜¸: {status.get('quality_summary', {}).get('good', 0)}ê°œ
- í’ˆì§ˆ ë¶ˆëŸ‰: {status.get('quality_summary', {}).get('poor', 0)}ê°œ

ğŸ“ˆ ìˆ˜ì§‘ í†µê³„:
- ì´ ìˆ˜ì§‘ ì‘ì—…: {self.collection_stats['total_collections']}íšŒ
- ì„±ê³µë¥ : {success_rate:.1f}%
- ë§ˆì§€ë§‰ ìˆ˜ì§‘: {self.collection_stats['last_collection'].strftime('%H:%M:%S') if self.collection_stats['last_collection'] else 'N/A'}
- ì„œë¹„ìŠ¤ ê°€ë™ì‹œê°„: {uptime_hours:.1f}ì‹œê°„

ğŸ¯ í’ˆì§ˆ í˜„í™©:
"""
            
            # ì†ŒìŠ¤ë³„ ìƒì„¸ ì •ë³´
            for source_id, source_info in status.get('sources', {}).items():
                quality_icon = "ğŸŸ¢" if source_info['quality_score'] >= 80 else "ğŸŸ¡" if source_info['quality_score'] >= 50 else "ğŸ”´"
                report += f"- {source_info['name']}: {quality_icon} {source_info['quality_score']:.1f}ì  (24ì‹œê°„ ìˆ˜ì§‘: {source_info['recent_items_24h']}ê°œ)\n"
            
            logger.info(report)
            
            # ë¦¬í¬íŠ¸ íŒŒì¼ë¡œ ì €ì¥
            report_file = Path(f"/var/log/stockpilot/daily_report_{datetime.now().strftime('%Y%m%d')}.txt")
            report_file.parent.mkdir(exist_ok=True)
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… {task_name} ì™„ë£Œ - ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ")
            
        except Exception as e:
            logger.error(f"{task_name} ì‹¤í–‰ ì˜¤ë¥˜: {e}")

    async def start(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        if not await self.setup():
            logger.error("ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False
        
        self.running = True
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì •
        def signal_handler(signum, frame):
            logger.info(f"ì‹œê·¸ë„ {signum} ìˆ˜ì‹  - ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì¤‘...")
            self.running = False
        
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        
        logger.info("ğŸš€ KR ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
        
        try:
            # ì´ˆê¸° í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰
            await self._execute_quality_validation("ì´ˆê¸° í’ˆì§ˆ ê²€ì¦")
            
            # ë©”ì¸ ìŠ¤ì¼€ì¤„ë§ ë£¨í”„
            while self.running:
                try:
                    # ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ì‹¤í–‰
                    schedule.run_pending()
                    
                    # 1ì´ˆ ëŒ€ê¸°
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"ìŠ¤ì¼€ì¤„ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                    await asyncio.sleep(5)
                    
        except Exception as e:
            logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        finally:
            await self.cleanup()
        
        return True

    async def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        logger.info("KR ë°ì´í„° ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì¤‘...")
        
        if self.data_manager:
            try:
                await self.data_manager.__aexit__(None, None, None)
            except Exception as e:
                logger.error(f"ë°ì´í„° ë§¤ë‹ˆì € ì •ë¦¬ ì˜¤ë¥˜: {e}")
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        uptime = datetime.now() - self.collection_stats['start_time']
        logger.info(f"ğŸ“Š ìµœì¢… í†µê³„: ì´ {self.collection_stats['total_collections']}íšŒ ìˆ˜ì§‘, "
                   f"ì„±ê³µ {self.collection_stats['successful_collections']}íšŒ, "
                   f"ì‹¤íŒ¨ {self.collection_stats['failed_collections']}íšŒ, "
                   f"ê°€ë™ì‹œê°„ {uptime}")
        
        logger.info("KR ë°ì´í„° ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì™„ë£Œ")

    def get_status(self) -> Dict:
        """í˜„ì¬ ìƒíƒœ ì¡°íšŒ"""
        uptime = datetime.now() - self.collection_stats['start_time']
        
        return {
            'running': self.running,
            'uptime_seconds': uptime.total_seconds(),
            'collection_stats': self.collection_stats.copy(),
            'scheduled_jobs': len(schedule.jobs),
            'data_manager_initialized': self.data_manager is not None
        }

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("/var/log/stockpilot")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("ğŸ‡°ğŸ‡· StockPilot KR ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
    
    scheduler = KoreanDataScheduler()
    
    try:
        success = await scheduler.start()
        if not success:
            sys.exit(1)
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•œ ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())