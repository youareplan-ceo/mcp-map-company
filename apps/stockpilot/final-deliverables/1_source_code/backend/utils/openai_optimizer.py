#!/usr/bin/env python3
"""
OpenAI API í˜¸ì¶œ ìµœì í™”ê¸°
- LRU ìºì‹œë¥¼ í†µí•œ ì¤‘ë³µ ìš”ì²­ ë°©ì§€
- ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ í†µí•œ Rate Limit ì²˜ë¦¬
- ì‹¤ì‹œê°„ ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë° ì„ê³„ì¹˜ ì œì–´
- í”„ë¡œë•ì…˜ìš© ê³ ì„±ëŠ¥ ìµœì í™”
"""

import asyncio
import hashlib
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, asdict
from functools import lru_cache
import json

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

@dataclass
class OpenAICost:
    """OpenAI API ë¹„ìš© ì •ë³´"""
    model: str
    input_tokens: int
    output_tokens: int
    input_cost: float
    output_cost: float
    total_cost: float
    timestamp: str

class OpenAIOptimizer:
    """OpenAI API í˜¸ì¶œ ìµœì í™”ê¸°"""
    
    # ëª¨ë¸ë³„ ë¹„ìš© (USD per token)
    MODEL_COSTS = {
        "gpt-4o-mini": {
            "input": 0.00000015,   # $0.15 / 1M tokens
            "output": 0.00000060   # $0.60 / 1M tokens
        },
        "gpt-4o": {
            "input": 0.0000025,    # $2.50 / 1M tokens  
            "output": 0.00001      # $10.00 / 1M tokens
        }
    }
    
    def __init__(self, 
                 daily_budget: float = 10.0,  # ì¼ì¼ ì˜ˆì‚° ($10)
                 cache_ttl: int = 300,         # ìºì‹œ ìœ íš¨ì‹œê°„ (5ë¶„)
                 max_retries: int = 5):        # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        """
        ìµœì í™”ê¸° ì´ˆê¸°í™”
        
        Args:
            daily_budget: ì¼ì¼ ì˜ˆì‚° (USD)
            cache_ttl: ìºì‹œ ìœ íš¨ì‹œê°„ (ì´ˆ)  
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        """
        self.daily_budget = daily_budget
        self.cache_ttl = cache_ttl
        self.max_retries = max_retries
        
        # ë¹„ìš© ì¶”ì 
        self.daily_costs: List[OpenAICost] = []
        self.total_daily_cost = 0.0
        self.last_reset_date = datetime.now().date()
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.response_cache: Dict[str, Tuple[Any, float]] = {}  # {request_hash: (response, timestamp)}
        
        # Rate Limit ì¶”ì 
        self.rate_limit_resets: Dict[str, float] = {}  # {model: reset_timestamp}
        self.request_counts: Dict[str, int] = {}       # {model: count}
        
        # ë°±ì˜¤í”„ ìƒíƒœ
        self.backoff_delays: Dict[str, float] = {}     # {model: delay_seconds}
        
        logger.info(f"âœ… OpenAI ìµœì í™”ê¸° ì´ˆê¸°í™” (ì˜ˆì‚°: ${daily_budget}/day, ìºì‹œ: {cache_ttl}ì´ˆ)")
    
    def _reset_daily_costs(self):
        """ì¼ì¼ ë¹„ìš© ë¦¬ì…‹ (ìì •ì—)"""
        current_date = datetime.now().date()
        if current_date != self.last_reset_date:
            logger.info(f"ğŸ”„ ì¼ì¼ ë¹„ìš© ë¦¬ì…‹: ${self.total_daily_cost:.4f} -> $0.00")
            self.daily_costs.clear()
            self.total_daily_cost = 0.0
            self.last_reset_date = current_date
    
    def _generate_cache_key(self, model: str, messages: List[Dict], **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ë©”ì‹œì§€ì™€ ì„¤ì •ì„ í•´ì‹œë¡œ ë³€í™˜
        cache_data = {
            "model": model,
            "messages": messages,
            **kwargs
        }
        
        cache_str = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        return hashlib.sha256(cache_str.encode()).hexdigest()
    
    def _get_cached_response(self, cache_key: str) -> Optional[Any]:
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
        if cache_key in self.response_cache:
            response, timestamp = self.response_cache[cache_key]
            
            # TTL í™•ì¸
            if time.time() - timestamp <= self.cache_ttl:
                logger.info(f"ğŸ’¡ ìºì‹œ íˆíŠ¸: {cache_key[:8]}...")
                return response
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self.response_cache[cache_key]
                logger.debug(f"ğŸ—‘ï¸ ë§Œë£Œëœ ìºì‹œ ì‚­ì œ: {cache_key[:8]}...")
        
        return None
    
    def _cache_response(self, cache_key: str, response: Any):
        """ì‘ë‹µ ìºì‹œ ì €ì¥"""
        self.response_cache[cache_key] = (response, time.time())
        logger.debug(f"ğŸ’¾ ìºì‹œ ì €ì¥: {cache_key[:8]}...")
        
        # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 1000ê°œ)
        if len(self.response_cache) > 1000:
            # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ 10ê°œ ì‚­ì œ
            sorted_cache = sorted(
                self.response_cache.items(),
                key=lambda x: x[1][1]  # timestampë¡œ ì •ë ¬
            )
            
            for i in range(10):
                if i < len(sorted_cache):
                    del self.response_cache[sorted_cache[i][0]]
            
            logger.info(f"ğŸ§¹ ìºì‹œ ì •ë¦¬: {len(self.response_cache)}ê°œ ìœ ì§€")
    
    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> OpenAICost:
        """API í˜¸ì¶œ ë¹„ìš© ê³„ì‚°"""
        if model not in self.MODEL_COSTS:
            logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ ë¹„ìš©: {model}")
            model = "gpt-4o-mini"  # ê¸°ë³¸ê°’
        
        costs = self.MODEL_COSTS[model]
        input_cost = input_tokens * costs["input"]
        output_cost = output_tokens * costs["output"]
        total_cost = input_cost + output_cost
        
        cost_info = OpenAICost(
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            input_cost=input_cost,
            output_cost=output_cost,
            total_cost=total_cost,
            timestamp=datetime.now().isoformat()
        )
        
        return cost_info
    
    def _track_cost(self, cost_info: OpenAICost):
        """ë¹„ìš© ì¶”ì  ë° ì˜ˆì‚° ê´€ë¦¬"""
        self._reset_daily_costs()
        
        self.daily_costs.append(cost_info)
        self.total_daily_cost += cost_info.total_cost
        
        logger.info(
            f"ğŸ’° API ë¹„ìš©: ${cost_info.total_cost:.6f} "
            f"({cost_info.input_tokens}â†’{cost_info.output_tokens} tokens) "
            f"ì¼ì¼ ëˆ„ì : ${self.total_daily_cost:.4f}"
        )
        
        # ì˜ˆì‚° ê²½ê³ 
        if self.total_daily_cost >= self.daily_budget * 0.8:
            logger.warning(f"âš ï¸ ì¼ì¼ ì˜ˆì‚° 80% ë„ë‹¬: ${self.total_daily_cost:.4f}/${self.daily_budget}")
        
        if self.total_daily_cost >= self.daily_budget:
            logger.error(f"ğŸš¨ ì¼ì¼ ì˜ˆì‚° ì´ˆê³¼: ${self.total_daily_cost:.4f}/${self.daily_budget}")
            raise Exception("ì¼ì¼ OpenAI ì˜ˆì‚° ì´ˆê³¼")
    
    def _should_throttle(self, model: str) -> bool:
        """Rate Limit í™•ì¸"""
        # ì˜ˆì‚° ì²´í¬
        if self.total_daily_cost >= self.daily_budget:
            return True
        
        # ë°±ì˜¤í”„ ì²´í¬
        if model in self.backoff_delays:
            if time.time() < self.backoff_delays[model]:
                return True
            else:
                del self.backoff_delays[model]
        
        return False
    
    def _handle_rate_limit(self, model: str, retry_count: int):
        """Rate Limit ì²˜ë¦¬ (ì§€ìˆ˜ ë°±ì˜¤í”„)"""
        base_delay = 2 ** retry_count  # 2, 4, 8, 16, 32ì´ˆ
        jitter = base_delay * 0.1 * (0.5 - asyncio.get_event_loop().time() % 1)  # ëœë¤ ì§€í„°
        delay = base_delay + jitter
        
        self.backoff_delays[model] = time.time() + delay
        
        logger.warning(f"â³ Rate Limit - ë°±ì˜¤í”„ ëŒ€ê¸°: {delay:.1f}ì´ˆ (ì¬ì‹œë„ {retry_count}/{self.max_retries})")
        return delay
    
    async def optimize_chat_completion(self, 
                                     openai_client,
                                     model: str,
                                     messages: List[Dict],
                                     **kwargs) -> Any:
        """
        ìµœì í™”ëœ OpenAI Chat Completion í˜¸ì¶œ
        
        Args:
            openai_client: OpenAI í´ë¼ì´ì–¸íŠ¸
            model: ëª¨ë¸ëª…
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            OpenAI API ì‘ë‹µ
        """
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_cache_key(model, messages, **kwargs)
        
        # ìºì‹œëœ ì‘ë‹µ í™•ì¸
        cached_response = self._get_cached_response(cache_key)
        if cached_response:
            return cached_response
        
        # Rate Limit í™•ì¸
        if self._should_throttle(model):
            raise Exception("API í˜¸ì¶œ ì œí•œ (ì˜ˆì‚° ì´ˆê³¼ ë˜ëŠ” Rate Limit)")
        
        # ì¬ì‹œë„ ë¡œì§
        last_exception = None
        
        for retry_count in range(self.max_retries + 1):
            try:
                logger.info(f"ğŸ¤– OpenAI API í˜¸ì¶œ: {model} (ì¬ì‹œë„: {retry_count})")
                
                # API í˜¸ì¶œ
                response = await openai_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    **kwargs
                )
                
                # ë¹„ìš© ê³„ì‚° ë° ì¶”ì 
                usage = response.usage
                if usage:
                    cost_info = self._calculate_cost(
                        model, 
                        usage.prompt_tokens, 
                        usage.completion_tokens
                    )
                    self._track_cost(cost_info)
                
                # ì‘ë‹µ ìºì‹œ ì €ì¥
                self._cache_response(cache_key, response)
                
                logger.info(f"âœ… OpenAI API ì„±ê³µ: {model}")
                return response
                
            except Exception as e:
                last_exception = e
                error_str = str(e).lower()
                
                if "rate limit" in error_str or "429" in error_str:
                    if retry_count < self.max_retries:
                        delay = self._handle_rate_limit(model, retry_count + 1)
                        await asyncio.sleep(delay)
                        continue
                else:
                    # Rate Limitì´ ì•„ë‹Œ ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¦‰ì‹œ ì‹¤íŒ¨
                    logger.error(f"âŒ OpenAI API ì˜¤ë¥˜: {e}")
                    break
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        logger.error(f"âŒ OpenAI API ìµœì¢… ì‹¤íŒ¨: {last_exception}")
        raise last_exception
    
    def get_cost_summary(self) -> Dict[str, Any]:
        """ë¹„ìš© ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        self._reset_daily_costs()
        
        summary = {
            "daily_budget": self.daily_budget,
            "total_daily_cost": self.total_daily_cost,
            "remaining_budget": max(0, self.daily_budget - self.total_daily_cost),
            "budget_utilization": (self.total_daily_cost / self.daily_budget) * 100,
            "total_requests": len(self.daily_costs),
            "cache_size": len(self.response_cache),
            "cache_hit_rate": 0.0  # TODO: ê³„ì‚° êµ¬í˜„
        }
        
        if self.daily_costs:
            model_costs = {}
            for cost in self.daily_costs:
                model = cost.model
                if model not in model_costs:
                    model_costs[model] = {
                        "requests": 0,
                        "total_cost": 0.0,
                        "total_tokens": 0
                    }
                
                model_costs[model]["requests"] += 1
                model_costs[model]["total_cost"] += cost.total_cost
                model_costs[model]["total_tokens"] += cost.input_tokens + cost.output_tokens
            
            summary["model_breakdown"] = model_costs
        
        return summary
    
    def clear_cache(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        cache_size = len(self.response_cache)
        self.response_cache.clear()
        logger.info(f"ğŸ§¹ ìºì‹œ ì „ì²´ ì‚­ì œ: {cache_size}ê°œ í•­ëª©")

# ì „ì—­ ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤
_global_optimizer = None

def get_openai_optimizer(daily_budget: float = 10.0) -> OpenAIOptimizer:
    """ì „ì—­ OpenAI ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_optimizer
    if _global_optimizer is None:
        _global_optimizer = OpenAIOptimizer(daily_budget=daily_budget)
    return _global_optimizer

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_openai_optimizer():
    """OpenAI ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸"""
    print("=== OpenAI API í˜¸ì¶œ ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸ ===")
    
    optimizer = OpenAIOptimizer(daily_budget=5.0)
    
    # Mock OpenAI í´ë¼ì´ì–¸íŠ¸ ì‘ë‹µ
    class MockResponse:
        def __init__(self):
            self.usage = MockUsage()
    
    class MockUsage:
        def __init__(self):
            self.prompt_tokens = 50
            self.completion_tokens = 30
    
    class MockClient:
        async def create(self, **kwargs):
            await asyncio.sleep(0.1)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
            return MockResponse()
    
    class MockCompletions:
        def __init__(self):
            self.create = MockClient().create
    
    class MockChat:
        def __init__(self):
            self.completions = MockCompletions()
    
    class MockOpenAIClient:
        def __init__(self):
            self.chat = MockChat()
    
    mock_client = MockOpenAIClient()
    
    # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
    test_messages = [
        {"role": "user", "content": "Hello, world!"}
    ]
    
    try:
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ 1: ì¼ë°˜ API í˜¸ì¶œ")
        response1 = await optimizer.optimize_chat_completion(
            mock_client, 
            "gpt-4o-mini", 
            test_messages
        )
        print("âœ… ì„±ê³µ")
        
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ 2: ìºì‹œëœ ìš”ì²­ (ê°™ì€ ìš”ì²­)")
        response2 = await optimizer.optimize_chat_completion(
            mock_client, 
            "gpt-4o-mini", 
            test_messages
        )
        print("âœ… ìºì‹œ íˆíŠ¸ ì„±ê³µ")
        
        print("\nğŸ“Š ë¹„ìš© ìš”ì•½:")
        summary = optimizer.get_cost_summary()
        print(json.dumps(summary, indent=2, ensure_ascii=False))
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    asyncio.run(test_openai_optimizer())