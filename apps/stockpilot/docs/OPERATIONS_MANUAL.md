# ğŸ”§ StockPilot ìš´ì˜ì ë§¤ë‰´ì–¼

> ì‹œìŠ¤í…œ ê´€ë¦¬ìë¥¼ ìœ„í•œ ì¢…í•© ìš´ì˜ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
- [ì„¤ì¹˜ ë° ë°°í¬](#ì„¤ì¹˜-ë°-ë°°í¬)
- [ì¼ìƒ ìš´ì˜](#ì¼ìƒ-ìš´ì˜)
- [ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼](#ëª¨ë‹ˆí„°ë§-ë°-ì•Œë¦¼)
- [ë°±ì—… ë° ë³µêµ¬](#ë°±ì—…-ë°-ë³µêµ¬)
- [ì„±ëŠ¥ íŠœë‹](#ì„±ëŠ¥-íŠœë‹)
- [ë³´ì•ˆ ê´€ë¦¬](#ë³´ì•ˆ-ê´€ë¦¬)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
- [ì¥ì•  ëŒ€ì‘](#ì¥ì• -ëŒ€ì‘)

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        StockPilot ì‹œìŠ¤í…œ                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frontend (React)                                               â”‚
â”‚  â”œâ”€â”€ í¬íŠ¸: 3000                                                 â”‚
â”‚  â”œâ”€â”€ ì •ì  íŒŒì¼ ì„œë¹™                                              â”‚
â”‚  â””â”€â”€ WebSocket í´ë¼ì´ì–¸íŠ¸                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  API Gateway / Load Balancer                                   â”‚
â”‚  â”œâ”€â”€ Nginx (í¬íŠ¸: 80, 443)                                      â”‚
â”‚  â”œâ”€â”€ SSL í„°ë¯¸ë„¤ì´ì…˜                                              â”‚
â”‚  â””â”€â”€ ìš”ì²­ ë¼ìš°íŒ…                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Backend Services                                              â”‚
â”‚  â”œâ”€â”€ FastAPI ì„œë²„ (í¬íŠ¸: 8000)                                  â”‚
â”‚  â”œâ”€â”€ WebSocket ì„œë²„ (í¬íŠ¸: 8765)                                â”‚
â”‚  â”œâ”€â”€ ë¹„ìš© ëŒ€ì‹œë³´ë“œ (í¬íŠ¸: 8004)                                  â”‚
â”‚  â””â”€â”€ í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§ (systemd)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data Layer                                                    â”‚
â”‚  â”œâ”€â”€ PostgreSQL (í¬íŠ¸: 5432)                                    â”‚
â”‚  â”œâ”€â”€ Redis (í¬íŠ¸: 6379)                                         â”‚
â”‚  â””â”€â”€ íŒŒì¼ ìŠ¤í† ë¦¬ì§€ (/opt/stockpilot/data)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  External APIs                                                 â”‚
â”‚  â”œâ”€â”€ OpenAI GPT                                                â”‚
â”‚  â”œâ”€â”€ Yahoo Finance                                             â”‚
â”‚  â”œâ”€â”€ Alpha Vantage                                             â”‚
â”‚  â”œâ”€â”€ Korean Data Sources (DART, KRX)                          â”‚
â”‚  â””â”€â”€ News Providers (Reuters, NewsAPI)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

#### 1. ì›¹ ì„œë²„ (Nginx)
```nginx
# /etc/nginx/sites-available/stockpilot
server {
    listen 80;
    listen [::]:80;
    server_name stockpilot.ai www.stockpilot.ai;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    listen [::]:443 ssl http2;
    server_name stockpilot.ai www.stockpilot.ai;
    
    ssl_certificate /etc/letsencrypt/live/stockpilot.ai/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/stockpilot.ai/privkey.pem;
    
    # Frontend
    location / {
        root /opt/stockpilot/frontend/build;
        index index.html;
        try_files $uri $uri/ /index.html;
    }
    
    # API
    location /api/ {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    # WebSocket
    location /ws {
        proxy_pass http://localhost:8765;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```

#### 2. ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ êµ¬ì¡°
```
systemd ì„œë¹„ìŠ¤:
â”œâ”€â”€ stockpilot-api.service          # FastAPI ë©”ì¸ ì„œë²„
â”œâ”€â”€ stockpilot-websocket.service    # WebSocket ì„œë²„
â”œâ”€â”€ stockpilot-health.service       # í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§
â”œâ”€â”€ stockpilot-cost-dashboard.service # ë¹„ìš© ëŒ€ì‹œë³´ë“œ
â”œâ”€â”€ stockpilot-korean-data.service  # KR ë°ì´í„° ìˆ˜ì§‘
â””â”€â”€ stockpilot-failover.service     # ë‹¤ì¤‘ í”„ë¡œë°”ì´ë” í˜ì¼ì˜¤ë²„
```

#### 3. ë°ì´í„° í”Œë¡œìš°
```
ë°ì´í„° ìˆ˜ì§‘ â†’ ì²˜ë¦¬ â†’ ì €ì¥ â†’ API â†’ í”„ë¡ íŠ¸ì—”ë“œ
     â†“
ì™¸ë¶€ API â†’ í˜ì¼ì˜¤ë²„ â†’ ìºì‹± â†’ ë¶„ì„ â†’ WebSocket â†’ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
     â†“
ëª¨ë‹ˆí„°ë§ â†’ ì•Œë¦¼ â†’ ë¡œê¹… â†’ ë°±ì—…
```

---

## ğŸš€ ì„¤ì¹˜ ë° ë°°í¬

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

#### ìµœì†Œ ì‚¬ì–‘ (ê°œë°œ/í…ŒìŠ¤íŠ¸)
```
CPU: 2ì½”ì–´ 2GHz ì´ìƒ
RAM: 4GB ì´ìƒ
ë””ìŠ¤í¬: 20GB SSD
ë„¤íŠ¸ì›Œí¬: 10Mbps ì´ìƒ
OS: Ubuntu 20.04+ ë˜ëŠ” CentOS 8+
```

#### ê¶Œì¥ ì‚¬ì–‘ (í”„ë¡œë•ì…˜)
```
CPU: 4ì½”ì–´ 3GHz ì´ìƒ (ARM64 ë˜ëŠ” x86_64)
RAM: 16GB ì´ìƒ
ë””ìŠ¤í¬: 100GB SSD (NVMe ê¶Œì¥)
ë„¤íŠ¸ì›Œí¬: 100Mbps ì´ìƒ, ë‚®ì€ ì§€ì—°ì‹œê°„
OS: Ubuntu 22.04 LTS
ë°±ì—…: ë³„ë„ ìŠ¤í† ë¦¬ì§€ 200GB+
```

### ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

#### 1. ê¸°ë³¸ ì„¤ì¹˜
```bash
#!/bin/bash
# install_stockpilot.sh

set -euo pipefail

echo "ğŸš€ StockPilot ìë™ ì„¤ì¹˜ ì‹œì‘"

# ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
sudo apt update && sudo apt upgrade -y

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
sudo apt install -y \
    git curl wget \
    python3 python3-pip python3-venv \
    nodejs npm \
    nginx redis-server postgresql postgresql-contrib \
    certbot python3-certbot-nginx \
    htop iotop netstat-nat \
    docker.io docker-compose

# Python ê°€ìƒí™˜ê²½ ì„¤ì •
cd /opt
sudo git clone https://github.com/youareplan/stockpilot-ai.git
sudo chown -R $USER:$USER stockpilot-ai
cd stockpilot-ai/backend

python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

# Node.js ì˜ì¡´ì„± ì„¤ì¹˜
cd ../frontend
npm install
npm run build

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
sudo -u postgres createdb stockpilot
sudo -u postgres createuser stockpilot --pwprompt

# systemd ì„œë¹„ìŠ¤ ë“±ë¡
cd ../systemd
sudo cp *.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable stockpilot-*.service

# Nginx ì„¤ì •
sudo cp ../nginx/stockpilot.conf /etc/nginx/sites-available/
sudo ln -sf /etc/nginx/sites-available/stockpilot.conf /etc/nginx/sites-enabled/
sudo nginx -t && sudo systemctl reload nginx

echo "âœ… StockPilot ì„¤ì¹˜ ì™„ë£Œ"
echo "ğŸ“ ë‹¤ìŒ ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ë° ì„œë¹„ìŠ¤ ì‹œì‘"
```

#### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
# /opt/stockpilot/.env.production
# ë°ì´í„°ë² ì´ìŠ¤
DATABASE_URL=postgresql://stockpilot:password@localhost:5432/stockpilot
REDIS_URL=redis://localhost:6379/0

# API í‚¤ë“¤ (ì‹¤ì œ ê°’ìœ¼ë¡œ êµì²´)
OPENAI_API_KEY=sk-...
ALPHA_VANTAGE_API_KEY=...
NEWSAPI_KEY=...
DART_API_KEY=...

# ë³´ì•ˆ
JWT_SECRET_KEY=$(openssl rand -hex 32)
ENCRYPT_KEY=$(openssl rand -hex 32)

# ë¡œê¹…
LOG_LEVEL=INFO
LOG_DIR=/var/log/stockpilot

# ì•Œë¦¼
SLACK_WEBHOOK_URL=https://hooks.slack.com/...
EMAIL_SMTP_SERVER=smtp.gmail.com
EMAIL_USERNAME=...
EMAIL_PASSWORD=...

# ëª¨ë‹ˆí„°ë§
ENABLE_METRICS=true
PROMETHEUS_PORT=9090
GRAFANA_PORT=3001
```

#### 3. ì„œë¹„ìŠ¤ ì‹œì‘
```bash
#!/bin/bash
# start_services.sh

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
sudo mkdir -p /var/log/stockpilot
sudo chown stockpilot:stockpilot /var/log/stockpilot

# ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
sudo mkdir -p /opt/stockpilot/data
sudo chown stockpilot:stockpilot /opt/stockpilot/data

# ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start stockpilot-api
sudo systemctl start stockpilot-websocket
sudo systemctl start stockpilot-health
sudo systemctl start stockpilot-cost-dashboard

# ìƒíƒœ í™•ì¸
systemctl status stockpilot-*

# ì›¹ ì„œë²„ ì‹œì‘
sudo systemctl start nginx

echo "ğŸ‰ ëª¨ë“  ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤"
echo "ğŸŒ http://localhost ì—ì„œ ì ‘ì† ê°€ëŠ¥"
```

### Docker ë°°í¬

#### Docker Compose ì„¤ì •
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/ssl/certs
    depends_on:
      - backend
      - frontend

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    environment:
      - DATABASE_URL=postgresql://stockpilot:${DB_PASSWORD}@db:5432/stockpilot
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - db
      - redis
    volumes:
      - ./data:/opt/stockpilot/data
      - ./logs:/var/log/stockpilot

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    volumes:
      - frontend_build:/app/build

  websocket:
    build:
      context: ./backend
      dockerfile: Dockerfile.websocket
    ports:
      - "8765:8765"
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=stockpilot
      - POSTGRES_USER=stockpilot
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data

  monitoring:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

volumes:
  postgres_data:
  redis_data:
  frontend_build:
```

---

## ğŸ“Š ì¼ìƒ ìš´ì˜

### ì¼ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì•„ì¹¨ ì ê²€ (09:00)
```bash
#!/bin/bash
# daily_morning_check.sh

echo "ğŸ“… $(date '+%Y-%m-%d %H:%M:%S') - ì¼ì¼ ì•„ì¹¨ ì ê²€ ì‹œì‘"

# 1. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
echo "ğŸ–¥ï¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸"
echo "CPU ì‚¬ìš©ë¥ : $(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | awk -F'%' '{print $1}')"
echo "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : $(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')"
echo "ë””ìŠ¤í¬ ì‚¬ìš©ë¥ : $(df -h | grep '/opt' | awk '{print $5}')"

# 2. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
echo "ğŸ”§ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"
for service in stockpilot-api stockpilot-websocket stockpilot-health; do
    if systemctl is-active --quiet $service; then
        echo "âœ… $service: ì‹¤í–‰ ì¤‘"
    else
        echo "âŒ $service: ì¤‘ë‹¨ë¨"
        sudo systemctl start $service
    fi
done

# 3. ë¡œê·¸ í™•ì¸ (ìµœê·¼ 1ì‹œê°„ ì—ëŸ¬)
echo "ğŸ“ ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ í™•ì¸"
error_count=$(grep -i error /var/log/stockpilot/*.log | grep "$(date '+%Y-%m-%d %H')" | wc -l)
echo "ìµœê·¼ 1ì‹œê°„ ì—ëŸ¬ ìˆ˜: $error_count"

if [ $error_count -gt 10 ]; then
    echo "âš ï¸ ì—ëŸ¬ê°€ ë§ìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
fi

# 4. API í—¬ìŠ¤ ì²´í¬
echo "ğŸŒ API í—¬ìŠ¤ ì²´í¬"
health_response=$(curl -s -w "%{http_code}" http://localhost:8000/api/v1/system/health)
health_code="${health_response: -3}"

if [ "$health_code" = "200" ]; then
    echo "âœ… API ì„œë²„: ì •ìƒ"
else
    echo "âŒ API ì„œë²„: ì´ìƒ (HTTP $health_code)"
fi

# 5. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
echo "ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸"
if pg_isready -h localhost -p 5432 -U stockpilot > /dev/null 2>&1; then
    echo "âœ… PostgreSQL: ì—°ê²° ì •ìƒ"
else
    echo "âŒ PostgreSQL: ì—°ê²° ì‹¤íŒ¨"
fi

# 6. Redis ì—°ê²° í™•ì¸
echo "ğŸ”´ Redis ì—°ê²° í™•ì¸"
if redis-cli ping > /dev/null 2>&1; then
    echo "âœ… Redis: ì—°ê²° ì •ìƒ"
else
    echo "âŒ Redis: ì—°ê²° ì‹¤íŒ¨"
fi

echo "âœ… ì¼ì¼ ì•„ì¹¨ ì ê²€ ì™„ë£Œ"
```

#### ì €ë… ì ê²€ (18:00)
```bash
#!/bin/bash
# daily_evening_check.sh

echo "ğŸŒ… $(date '+%Y-%m-%d %H:%M:%S') - ì¼ì¼ ì €ë… ì ê²€ ì‹œì‘"

# 1. ë°±ì—… ìƒíƒœ í™•ì¸
echo "ğŸ’¾ ë°±ì—… ìƒíƒœ í™•ì¸"
latest_backup=$(ls -t /opt/stockpilot/backups/db_backup_*.sql.gz 2>/dev/null | head -n1)
if [ -n "$latest_backup" ]; then
    backup_date=$(basename "$latest_backup" | grep -o '[0-9]\{8\}')
    today=$(date +%Y%m%d)
    if [ "$backup_date" = "$today" ]; then
        echo "âœ… ì˜¤ëŠ˜ ë°±ì—…: ì™„ë£Œ"
    else
        echo "âš ï¸ ì˜¤ëŠ˜ ë°±ì—…: ë¯¸ì™„ë£Œ - ë°±ì—… ì‹¤í–‰"
        /opt/stockpilot/scripts/backup_database.sh
    fi
else
    echo "âŒ ë°±ì—… íŒŒì¼ ì—†ìŒ - ë°±ì—… ì‹¤í–‰"
    /opt/stockpilot/scripts/backup_database.sh
fi

# 2. ë¡œê·¸ ë¡œí…Œì´ì…˜
echo "ğŸ“œ ë¡œê·¸ ë¡œí…Œì´ì…˜"
find /var/log/stockpilot -name "*.log" -size +100M -exec logrotate -f /etc/logrotate.d/stockpilot {} \;

# 3. ìºì‹œ ì •ë¦¬
echo "ğŸ§¹ ìºì‹œ ì •ë¦¬"
redis-cli FLUSHDB

# 4. ì„±ëŠ¥ í†µê³„
echo "ğŸ“Š ì¼ì¼ ì„±ëŠ¥ í†µê³„"
echo "API í˜¸ì¶œ ìˆ˜: $(grep "API_CALL" /var/log/stockpilot/api.log | grep "$(date +%Y-%m-%d)" | wc -l)"
echo "WebSocket ì—°ê²° ìˆ˜: $(grep "WS_CONNECT" /var/log/stockpilot/websocket.log | grep "$(date +%Y-%m-%d)" | wc -l)"
echo "ì˜¤ë¥˜ ë°œìƒ ìˆ˜: $(grep -i error /var/log/stockpilot/*.log | grep "$(date +%Y-%m-%d)" | wc -l)"

echo "âœ… ì¼ì¼ ì €ë… ì ê²€ ì™„ë£Œ"
```

### ì£¼ê°„ ì ê²€

#### ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±
```python
#!/usr/bin/env python3
# weekly_report.py

import psycopg2
import json
from datetime import datetime, timedelta
from collections import defaultdict

def generate_weekly_report():
    """ì£¼ê°„ ìš´ì˜ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = psycopg2.connect(
        host="localhost",
        database="stockpilot",
        user="stockpilot",
        password="password"
    )
    
    cur = conn.cursor()
    
    # ì§€ë‚œ ì£¼ ë‚ ì§œ ë²”ìœ„
    end_date = datetime.now()
    start_date = end_date - timedelta(days=7)
    
    report = {
        "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
        "generated_at": datetime.now().isoformat()
    }
    
    # ì‚¬ìš©ì í™œë™ í†µê³„
    cur.execute("""
        SELECT DATE(created_at) as date, COUNT(*) as api_calls
        FROM api_usage_log
        WHERE created_at >= %s AND created_at <= %s
        GROUP BY DATE(created_at)
        ORDER BY date
    """, (start_date, end_date))
    
    daily_stats = cur.fetchall()
    report["daily_api_calls"] = [{"date": str(row[0]), "calls": row[1]} for row in daily_stats]
    
    # ì—ëŸ¬ í†µê³„
    cur.execute("""
        SELECT error_type, COUNT(*) as count
        FROM error_log
        WHERE created_at >= %s AND created_at <= %s
        GROUP BY error_type
        ORDER BY count DESC
    """, (start_date, end_date))
    
    error_stats = cur.fetchall()
    report["error_statistics"] = [{"type": row[0], "count": row[1]} for row in error_stats]
    
    # ì„±ëŠ¥ ì§€í‘œ
    cur.execute("""
        SELECT AVG(response_time) as avg_response_time,
               MAX(response_time) as max_response_time,
               MIN(response_time) as min_response_time
        FROM performance_log
        WHERE created_at >= %s AND created_at <= %s
    """, (start_date, end_date))
    
    perf_stats = cur.fetchone()
    report["performance"] = {
        "avg_response_time": float(perf_stats[0]) if perf_stats[0] else 0,
        "max_response_time": float(perf_stats[1]) if perf_stats[1] else 0,
        "min_response_time": float(perf_stats[2]) if perf_stats[2] else 0
    }
    
    # ì¸ê¸° ì¢…ëª© TOP 10
    cur.execute("""
        SELECT symbol, COUNT(*) as requests
        FROM stock_request_log
        WHERE created_at >= %s AND created_at <= %s
        GROUP BY symbol
        ORDER BY requests DESC
        LIMIT 10
    """, (start_date, end_date))
    
    popular_stocks = cur.fetchall()
    report["popular_stocks"] = [{"symbol": row[0], "requests": row[1]} for row in popular_stocks]
    
    cur.close()
    conn.close()
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_file = f"/var/log/stockpilot/weekly_report_{datetime.now().strftime('%Y%m%d')}.json"
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2, default=str)
    
    print(f"ğŸ“Š ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_file}")
    return report

if __name__ == "__main__":
    generate_weekly_report()
```

### ì›”ê°„ ìœ ì§€ë³´ìˆ˜

#### ì‹œìŠ¤í…œ ìµœì í™”
```bash
#!/bin/bash
# monthly_maintenance.sh

echo "ğŸ”§ ì›”ê°„ ìœ ì§€ë³´ìˆ˜ ì‘ì—… ì‹œì‘"

# 1. íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
echo "ğŸ“¦ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸"
sudo apt update && sudo apt upgrade -y

# 2. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
echo "ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”"
sudo -u postgres psql stockpilot -c "VACUUM FULL;"
sudo -u postgres psql stockpilot -c "REINDEX DATABASE stockpilot;"

# 3. ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ (30ì¼ ì´ìƒ)
echo "ğŸ—‘ï¸ ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ"
find /var/log/stockpilot -name "*.log*" -mtime +30 -delete

# 4. ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ (90ì¼ ì´ìƒ)
echo "ğŸ—‚ï¸ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ"
find /opt/stockpilot/backups -name "*.sql.gz" -mtime +90 -delete

# 5. SSL ì¸ì¦ì„œ ê°±ì‹ 
echo "ğŸ”’ SSL ì¸ì¦ì„œ ê°±ì‹ "
sudo certbot renew --nginx

# 6. ë³´ì•ˆ ìŠ¤ìº”
echo "ğŸ›¡ï¸ ë³´ì•ˆ ìŠ¤ìº” ì‹¤í–‰"
sudo chkrootkit
sudo rkhunter --update
sudo rkhunter --checkall --skip-keypress

# 7. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
echo "âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"
cd /opt/stockpilot/backend
source venv/bin/activate
python services/performance_benchmark.py

echo "âœ… ì›”ê°„ ìœ ì§€ë³´ìˆ˜ ì‘ì—… ì™„ë£Œ"
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

#### Prometheus ì„¤ì •
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "stockpilot_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'stockpilot-api'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics'

  - job_name: 'stockpilot-system'
    static_configs:
      - targets: ['localhost:9100']
    
  - job_name: 'postgres'
    static_configs:
      - targets: ['localhost:9187']
    
  - job_name: 'redis'
    static_configs:
      - targets: ['localhost:9121']
```

#### ì•Œë¦¼ ê·œì¹™
```yaml
# stockpilot_rules.yml
groups:
  - name: stockpilot_alerts
    rules:
      # API ì„œë²„ ë‹¤ìš´
      - alert: APIServerDown
        expr: up{job="stockpilot-api"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "StockPilot API ì„œë²„ê°€ ë‹¤ìš´ë˜ì—ˆìŠµë‹ˆë‹¤"
          
      # ë†’ì€ ì‘ë‹µ ì‹œê°„
      - alert: HighResponseTime
        expr: http_request_duration_seconds{quantile="0.95"} > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "API ì‘ë‹µ ì‹œê°„ì´ 2ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤"
          
      # ë†’ì€ ì—ëŸ¬ìœ¨
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "5xx ì—ëŸ¬ìœ¨ì´ 10%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤"
          
      # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ
      - alert: HighMemoryUsage
        expr: (node_memory_Active_bytes / node_memory_MemTotal_bytes) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ 80%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤"
          
      # ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±
      - alert: DiskSpaceLow
        expr: (node_filesystem_free_bytes / node_filesystem_size_bytes) * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„ì´ 10% ë¯¸ë§Œì…ë‹ˆë‹¤"
```

### ì»¤ìŠ¤í…€ ëª¨ë‹ˆí„°ë§

#### ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
#!/usr/bin/env python3
# business_metrics_collector.py

import psycopg2
import redis
import time
from prometheus_client import start_http_server, Gauge, Counter, Histogram
import logging

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
active_users = Gauge('stockpilot_active_users', 'í˜„ì¬ í™œì„± ì‚¬ìš©ì ìˆ˜')
api_requests_total = Counter('stockpilot_api_requests_total', 'API ìš”ì²­ ì´ ìˆ˜', ['endpoint', 'method', 'status'])
response_time = Histogram('stockpilot_response_time_seconds', 'API ì‘ë‹µ ì‹œê°„', ['endpoint'])
stock_analysis_requests = Counter('stockpilot_stock_analysis_requests', 'ì£¼ì‹ ë¶„ì„ ìš”ì²­ ìˆ˜', ['symbol'])
websocket_connections = Gauge('stockpilot_websocket_connections', 'WebSocket ì—°ê²° ìˆ˜')
portfolio_count = Gauge('stockpilot_portfolios_total', 'ì´ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜')
ai_model_usage = Counter('stockpilot_ai_model_usage', 'AI ëª¨ë¸ ì‚¬ìš©ëŸ‰', ['model', 'provider'])
cost_tracking = Gauge('stockpilot_daily_cost_usd', 'ì¼ì¼ ë¹„ìš© (USD)', ['category'])

class BusinessMetricsCollector:
    def __init__(self):
        self.db_conn = psycopg2.connect(
            host="localhost",
            database="stockpilot",
            user="stockpilot",
            password="password"
        )
        self.redis_conn = redis.Redis(host='localhost', port=6379, db=0)
        
    def collect_user_metrics(self):
        """ì‚¬ìš©ì ê´€ë ¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            cur = self.db_conn.cursor()
            
            # í™œì„± ì‚¬ìš©ì ìˆ˜ (ìµœê·¼ 1ì‹œê°„ ì´ë‚´ í™œë™)
            cur.execute("""
                SELECT COUNT(DISTINCT user_id)
                FROM user_activity_log
                WHERE last_activity > NOW() - INTERVAL '1 hour'
            """)
            active_count = cur.fetchone()[0]
            active_users.set(active_count)
            
            # ì´ í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜
            cur.execute("SELECT COUNT(*) FROM portfolios WHERE active = true")
            portfolio_total = cur.fetchone()[0]
            portfolio_count.set(portfolio_total)
            
            cur.close()
            
        except Exception as e:
            logging.error(f"ì‚¬ìš©ì ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def collect_api_metrics(self):
        """API ê´€ë ¨ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            cur = self.db_conn.cursor()
            
            # ìµœê·¼ 5ë¶„ê°„ API ìš”ì²­ í†µê³„
            cur.execute("""
                SELECT endpoint, method, status_code, COUNT(*), AVG(response_time)
                FROM api_request_log
                WHERE timestamp > NOW() - INTERVAL '5 minutes'
                GROUP BY endpoint, method, status_code
            """)
            
            for row in cur.fetchall():
                endpoint, method, status, count, avg_time = row
                api_requests_total.labels(
                    endpoint=endpoint,
                    method=method,
                    status=str(status)
                ).inc(count)
                
                if avg_time:
                    response_time.labels(endpoint=endpoint).observe(avg_time / 1000)
            
            cur.close()
            
        except Exception as e:
            logging.error(f"API ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def collect_business_metrics(self):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            cur = self.db_conn.cursor()
            
            # ì¸ê¸° ì¢…ëª© ë¶„ì„ ìš”ì²­
            cur.execute("""
                SELECT symbol, COUNT(*)
                FROM stock_analysis_log
                WHERE timestamp > NOW() - INTERVAL '1 hour'
                GROUP BY symbol
                ORDER BY COUNT(*) DESC
                LIMIT 10
            """)
            
            for symbol, count in cur.fetchall():
                stock_analysis_requests.labels(symbol=symbol).inc(count)
            
            # AI ëª¨ë¸ ì‚¬ìš©ëŸ‰
            cur.execute("""
                SELECT model_name, provider, COUNT(*), SUM(cost)
                FROM ai_model_usage
                WHERE timestamp > NOW() - INTERVAL '1 hour'
                GROUP BY model_name, provider
            """)
            
            total_cost = 0
            for model, provider, count, cost in cur.fetchall():
                ai_model_usage.labels(model=model, provider=provider).inc(count)
                total_cost += cost or 0
            
            cost_tracking.labels(category='ai_models').set(total_cost)
            
            cur.close()
            
        except Exception as e:
            logging.error(f"ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def collect_websocket_metrics(self):
        """WebSocket ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # Redisì—ì„œ í™œì„± WebSocket ì—°ê²° ìˆ˜ ì¡°íšŒ
            connection_count = self.redis_conn.scard("websocket:connections")
            websocket_connections.set(connection_count)
            
        except Exception as e:
            logging.error(f"WebSocket ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def run(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤í–‰"""
        while True:
            self.collect_user_metrics()
            self.collect_api_metrics()
            self.collect_business_metrics()
            self.collect_websocket_metrics()
            
            time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ìˆ˜ì§‘

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # Prometheus HTTP ì„œë²„ ì‹œì‘
    start_http_server(8001)
    logging.info("Prometheus ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘: http://localhost:8001/metrics")
    
    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì‹œì‘
    collector = BusinessMetricsCollector()
    collector.run()
```

### Grafana ëŒ€ì‹œë³´ë“œ

#### ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ
```json
{
  "dashboard": {
    "title": "StockPilot ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§",
    "panels": [
      {
        "title": "ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤",
        "type": "graph",
        "targets": [
          {
            "expr": "100 - (avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU ì‚¬ìš©ë¥ "
          },
          {
            "expr": "(node_memory_Active_bytes / node_memory_MemTotal_bytes) * 100",
            "legendFormat": "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ "
          }
        ]
      },
      {
        "title": "API ì„±ëŠ¥",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "ì´ˆë‹¹ ìš”ì²­ ìˆ˜"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile ì‘ë‹µì‹œê°„"
          }
        ]
      },
      {
        "title": "ì—ëŸ¬ìœ¨",
        "type": "singlestat",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m]) * 100"
          }
        ]
      },
      {
        "title": "í™œì„± ì‚¬ìš©ì",
        "type": "singlestat",
        "targets": [
          {
            "expr": "stockpilot_active_users"
          }
        ]
      }
    ]
  }
}
```

---

## ğŸ’¾ ë°±ì—… ë° ë³µêµ¬

### ìë™ ë°±ì—… ì‹œìŠ¤í…œ

#### ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
```bash
#!/bin/bash
# backup_database.sh

set -euo pipefail

BACKUP_DIR="/opt/stockpilot/backups"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
DB_NAME="stockpilot"
DB_USER="stockpilot"

# ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$BACKUP_DIR"

echo "ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹œì‘: $TIMESTAMP"

# PostgreSQL ë¤í”„
pg_dump -h localhost -U "$DB_USER" -W "$DB_NAME" | gzip > "$BACKUP_DIR/db_backup_$TIMESTAMP.sql.gz"

if [ $? -eq 0 ]; then
    echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ: db_backup_$TIMESTAMP.sql.gz"
    
    # ë°±ì—… íŒŒì¼ í¬ê¸° í™•ì¸
    backup_size=$(du -h "$BACKUP_DIR/db_backup_$TIMESTAMP.sql.gz" | cut -f1)
    echo "ğŸ“ ë°±ì—… íŒŒì¼ í¬ê¸°: $backup_size"
    
    # ë°±ì—… ë¬´ê²°ì„± ê²€ì¦
    echo "ğŸ” ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ ì¤‘..."
    if gunzip -t "$BACKUP_DIR/db_backup_$TIMESTAMP.sql.gz"; then
        echo "âœ… ë°±ì—… íŒŒì¼ ë¬´ê²°ì„± í™•ì¸ë¨"
    else
        echo "âŒ ë°±ì—… íŒŒì¼ ì†ìƒë¨"
        exit 1
    fi
    
    # ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ (30ì¼ ì´ìƒ)
    find "$BACKUP_DIR" -name "db_backup_*.sql.gz" -mtime +30 -delete
    echo "ğŸ—‘ï¸ 30ì¼ ì´ìƒëœ ë°±ì—… íŒŒì¼ ì‚­ì œ ì™„ë£Œ"
    
else
    echo "âŒ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹¤íŒ¨"
    exit 1
fi

# ì„¤ì • íŒŒì¼ ë°±ì—…
echo "âš™ï¸ ì„¤ì • íŒŒì¼ ë°±ì—… ì¤‘..."
tar -czf "$BACKUP_DIR/config_backup_$TIMESTAMP.tar.gz" \
    /opt/stockpilot/.env* \
    /etc/nginx/sites-available/stockpilot* \
    /etc/systemd/system/stockpilot*.service

echo "âœ… ì„¤ì • íŒŒì¼ ë°±ì—… ì™„ë£Œ: config_backup_$TIMESTAMP.tar.gz"

# ì›ê²© ë°±ì—… (S3, rsync ë“±)
if [ -n "${AWS_S3_BUCKET:-}" ]; then
    echo "â˜ï¸ S3 ì›ê²© ë°±ì—… ì¤‘..."
    aws s3 cp "$BACKUP_DIR/db_backup_$TIMESTAMP.sql.gz" "s3://$AWS_S3_BUCKET/stockpilot/backups/"
    aws s3 cp "$BACKUP_DIR/config_backup_$TIMESTAMP.tar.gz" "s3://$AWS_S3_BUCKET/stockpilot/backups/"
    echo "âœ… S3 ì›ê²© ë°±ì—… ì™„ë£Œ"
fi

echo "ğŸ‰ ì „ì²´ ë°±ì—… í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ"
```

#### ì‹¤ì‹œê°„ ë°±ì—… (WAL ì•„ì¹´ì´ë¹™)
```bash
#!/bin/bash
# setup_wal_archiving.sh

# PostgreSQL ì„¤ì • ìˆ˜ì •
sudo -u postgres psql -c "ALTER SYSTEM SET wal_level = replica;"
sudo -u postgres psql -c "ALTER SYSTEM SET archive_mode = on;"
sudo -u postgres psql -c "ALTER SYSTEM SET archive_command = 'cp %p /opt/stockpilot/backups/wal_archive/%f';"

# WAL ì•„ì¹´ì´ë¸Œ ë””ë ‰í† ë¦¬ ìƒì„±
sudo mkdir -p /opt/stockpilot/backups/wal_archive
sudo chown postgres:postgres /opt/stockpilot/backups/wal_archive

# PostgreSQL ì¬ì‹œì‘
sudo systemctl restart postgresql

echo "âœ… WAL ì•„ì¹´ì´ë¹™ ì„¤ì • ì™„ë£Œ"
```

### ë°ì´í„° ë³µêµ¬

#### ì „ì²´ ë³µêµ¬ ì ˆì°¨
```bash
#!/bin/bash
# restore_database.sh

set -euo pipefail

BACKUP_FILE="$1"
DB_NAME="stockpilot"
DB_USER="stockpilot"

if [ -z "$BACKUP_FILE" ]; then
    echo "ì‚¬ìš©ë²•: $0 <ë°±ì—…íŒŒì¼ê²½ë¡œ>"
    echo "ì˜ˆì‹œ: $0 /opt/stockpilot/backups/db_backup_20240101_120000.sql.gz"
    exit 1
fi

if [ ! -f "$BACKUP_FILE" ]; then
    echo "âŒ ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $BACKUP_FILE"
    exit 1
fi

echo "âš ï¸ ìœ„í—˜: í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ê°€ ë°±ì—… ë°ì´í„°ë¡œ ì™„ì „íˆ êµì²´ë©ë‹ˆë‹¤!"
echo "ë°±ì—… íŒŒì¼: $BACKUP_FILE"
read -p "ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): " confirm

if [ "$confirm" != "yes" ]; then
    echo "ë³µêµ¬ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
    exit 0
fi

echo "ğŸ›‘ ëª¨ë“  StockPilot ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì¤‘..."
sudo systemctl stop stockpilot-*

echo "ğŸ’¾ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ìƒì„± ì¤‘..."
current_backup="/tmp/pre_restore_backup_$(date +%Y%m%d_%H%M%S).sql.gz"
pg_dump -h localhost -U "$DB_USER" -W "$DB_NAME" | gzip > "$current_backup"
echo "âœ… í˜„ì¬ ìƒíƒœ ë°±ì—… ì™„ë£Œ: $current_backup"

echo "ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬ ì‹œì‘..."

# ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ ë° ì¬ìƒì„±
sudo -u postgres psql -c "DROP DATABASE IF EXISTS $DB_NAME;"
sudo -u postgres psql -c "CREATE DATABASE $DB_NAME OWNER $DB_USER;"

# ë°±ì—…ì—ì„œ ë³µêµ¬
if [[ "$BACKUP_FILE" == *.gz ]]; then
    gunzip -c "$BACKUP_FILE" | sudo -u postgres psql "$DB_NAME"
else
    sudo -u postgres psql "$DB_NAME" < "$BACKUP_FILE"
fi

if [ $? -eq 0 ]; then
    echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬ ì™„ë£Œ"
    
    # ì„œë¹„ìŠ¤ ì¬ì‹œì‘
    echo "ğŸš€ ì„œë¹„ìŠ¤ ì¬ì‹œì‘ ì¤‘..."
    sudo systemctl start stockpilot-*
    
    # í—¬ìŠ¤ ì²´í¬
    sleep 10
    health_response=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/api/v1/system/health)
    if [ "$health_response" = "200" ]; then
        echo "âœ… ì‹œìŠ¤í…œ ì •ìƒ ë³µêµ¬ ì™„ë£Œ"
    else
        echo "âš ï¸ ì‹œìŠ¤í…œ ë³µêµ¬ëŠ” ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨ (HTTP $health_response)"
        echo "ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."
    fi
    
else
    echo "âŒ ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬ ì‹¤íŒ¨"
    echo "ì´ì „ ìƒíƒœ ë°±ì—…ì´ ìˆìŠµë‹ˆë‹¤: $current_backup"
    exit 1
fi
```

#### Point-in-Time ë³µêµ¬
```bash
#!/bin/bash
# point_in_time_recovery.sh

TARGET_TIME="$1"  # ì˜ˆ: "2024-01-01 12:00:00"
BASE_BACKUP="$2"

if [ -z "$TARGET_TIME" ] || [ -z "$BASE_BACKUP" ]; then
    echo "ì‚¬ìš©ë²•: $0 '<ë³µêµ¬ì‹œì >' '<ë² ì´ìŠ¤ë°±ì—…íŒŒì¼>'"
    echo "ì˜ˆì‹œ: $0 '2024-01-01 12:00:00' /opt/stockpilot/backups/db_backup_20240101_000000.sql.gz"
    exit 1
fi

echo "â° Point-in-Time ë³µêµ¬ ì‹œì‘"
echo "ëª©í‘œ ì‹œì : $TARGET_TIME"
echo "ë² ì´ìŠ¤ ë°±ì—…: $BASE_BACKUP"

# PostgreSQL ì¤‘ë‹¨
sudo systemctl stop postgresql

# ë°ì´í„° ë””ë ‰í† ë¦¬ ë°±ì—…
sudo cp -r /var/lib/postgresql/15/main /var/lib/postgresql/15/main.backup

# ë² ì´ìŠ¤ ë°±ì—… ë³µì›
sudo -u postgres rm -rf /var/lib/postgresql/15/main/*
sudo -u postgres tar -xzf "$BASE_BACKUP" -C /var/lib/postgresql/15/main

# recovery.conf ì„¤ì •
cat << EOF | sudo -u postgres tee /var/lib/postgresql/15/main/recovery.conf
restore_command = 'cp /opt/stockpilot/backups/wal_archive/%f %p'
recovery_target_time = '$TARGET_TIME'
recovery_target_action = 'promote'
EOF

# PostgreSQL ì‹œì‘
sudo systemctl start postgresql

echo "âœ… Point-in-Time ë³µêµ¬ ì™„ë£Œ"
```

---

## âš¡ ì„±ëŠ¥ íŠœë‹

### ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

#### PostgreSQL íŠœë‹
```sql
-- postgresql.conf ìµœì í™” ì„¤ì •

-- ë©”ëª¨ë¦¬ ì„¤ì • (16GB RAM ê¸°ì¤€)
shared_buffers = '4GB'                    -- ì´ RAMì˜ 25%
effective_cache_size = '12GB'             -- ì´ RAMì˜ 75%
work_mem = '256MB'                        -- ì •ë ¬/ì¡°ì¸ìš© ë©”ëª¨ë¦¬
maintenance_work_mem = '1GB'              -- ìœ ì§€ë³´ìˆ˜ ì‘ì—…ìš© ë©”ëª¨ë¦¬

-- ì»¤ë„¥ì…˜ ì„¤ì •
max_connections = 200                     -- ìµœëŒ€ ë™ì‹œ ì—°ê²°
max_prepared_statements = 100

-- WAL ì„¤ì •
wal_buffers = '64MB'
checkpoint_completion_target = 0.9
wal_writer_delay = '200ms'

-- ì¿¼ë¦¬ í”Œë˜ë„ˆ
random_page_cost = 1.1                    -- SSD ê¸°ì¤€
effective_io_concurrency = 200

-- ë¡œê¹…
log_min_duration_statement = 1000         -- 1ì´ˆ ì´ìƒ ì¿¼ë¦¬ ë¡œê¹…
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
```

#### ì¸ë±ìŠ¤ ìµœì í™”
```sql
-- ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ìš© ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_stocks_symbol ON stocks(symbol);
CREATE INDEX CONCURRENTLY idx_stock_prices_symbol_date ON stock_prices(symbol, date DESC);
CREATE INDEX CONCURRENTLY idx_news_published_sentiment ON news(published_at DESC, sentiment);
CREATE INDEX CONCURRENTLY idx_portfolios_user_active ON portfolios(user_id) WHERE active = true;
CREATE INDEX CONCURRENTLY idx_api_logs_timestamp ON api_request_log(timestamp DESC);

-- ë³µí•© ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_user_activity_user_time ON user_activity_log(user_id, last_activity DESC);
CREATE INDEX CONCURRENTLY idx_analysis_symbol_date ON stock_analysis(symbol, created_at DESC);

-- ë¶€ë¶„ ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_active_subscriptions ON subscriptions(user_id) WHERE active = true;
CREATE INDEX CONCURRENTLY idx_recent_errors ON error_log(timestamp) WHERE timestamp > NOW() - INTERVAL '1 day';
```

#### íŒŒí‹°ì…”ë‹ ì„¤ì •
```sql
-- ì‹œê³„ì—´ ë°ì´í„° íŒŒí‹°ì…”ë‹ (stock_prices í…Œì´ë¸”)
CREATE TABLE stock_prices (
    id BIGSERIAL,
    symbol VARCHAR(10) NOT NULL,
    price DECIMAL(10,2) NOT NULL,
    volume BIGINT,
    date DATE NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
) PARTITION BY RANGE (date);

-- ì›”ë³„ íŒŒí‹°ì…˜ ìƒì„±
CREATE TABLE stock_prices_2024_01 PARTITION OF stock_prices
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
    
CREATE TABLE stock_prices_2024_02 PARTITION OF stock_prices
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- ìë™ íŒŒí‹°ì…˜ ìƒì„± í•¨ìˆ˜
CREATE OR REPLACE FUNCTION create_monthly_partition(table_name text, start_date date)
RETURNS void AS $$
DECLARE
    partition_name text;
    end_date date;
BEGIN
    partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM');
    end_date := start_date + interval '1 month';
    
    EXECUTE format('CREATE TABLE %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',
                   partition_name, table_name, start_date, end_date);
END;
$$ LANGUAGE plpgsql;
```

### ì• í”Œë¦¬ì¼€ì´ì…˜ ìµœì í™”

#### ìºì‹± ì „ëµ
```python
# redis_cache.py
import redis
import json
from functools import wraps
from typing import Any, Callable

class CacheManager:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    def cache_result(self, ttl: int = 300, key_prefix: str = ""):
        """ê²°ê³¼ ìºì‹± ë°ì½”ë ˆì´í„°"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
                
                # ìºì‹œì—ì„œ ì¡°íšŒ
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    return json.loads(cached_result)
                
                # í•¨ìˆ˜ ì‹¤í–‰ ë° ê²°ê³¼ ìºì‹±
                result = func(*args, **kwargs)
                self.redis_client.setex(
                    cache_key, 
                    ttl, 
                    json.dumps(result, default=str)
                )
                
                return result
            return wrapper
        return decorator
    
    def invalidate_pattern(self, pattern: str):
        """íŒ¨í„´ì— ë§ëŠ” ìºì‹œ ë¬´íš¨í™”"""
        keys = self.redis_client.keys(pattern)
        if keys:
            self.redis_client.delete(*keys)

# ì‚¬ìš© ì˜ˆì‹œ
cache = CacheManager()

@cache.cache_result(ttl=300, key_prefix="stock_data")
async def get_stock_data(symbol: str, interval: str = "1d"):
    # ì‹¤ì œ ë°ì´í„° ì¡°íšŒ ë¡œì§
    pass

@cache.cache_result(ttl=600, key_prefix="stock_analysis")
async def get_stock_analysis(symbol: str):
    # AI ë¶„ì„ ë¡œì§ (ë¹„ìš©ì´ ë†’ìœ¼ë¯€ë¡œ ê¸´ ìºì‹œ)
    pass
```

#### ë¹„ë™ê¸° ì²˜ë¦¬
```python
# async_processor.py
import asyncio
from concurrent.futures import ThreadPoolExecutor
import aioredis
from typing import List

class AsyncTaskProcessor:
    def __init__(self):
        self.redis = None
        self.executor = ThreadPoolExecutor(max_workers=20)
        self.queue_name = "stockpilot:tasks"
    
    async def init_redis(self):
        self.redis = await aioredis.from_url("redis://localhost:6379")
    
    async def add_task(self, task_type: str, data: dict):
        """ë¹„ë™ê¸° ì‘ì—… íì— ì¶”ê°€"""
        task = {
            "type": task_type,
            "data": data,
            "timestamp": datetime.now().isoformat()
        }
        await self.redis.lpush(self.queue_name, json.dumps(task))
    
    async def process_tasks(self):
        """ì‘ì—… í ì²˜ë¦¬"""
        while True:
            try:
                # ë¸”ë¡œí‚¹ ë°©ì‹ìœ¼ë¡œ ì‘ì—… ëŒ€ê¸°
                _, task_data = await self.redis.brpop([self.queue_name], timeout=1)
                if task_data:
                    task = json.loads(task_data)
                    await self.execute_task(task)
                    
            except Exception as e:
                logger.error(f"ì‘ì—… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def execute_task(self, task: dict):
        """ê°œë³„ ì‘ì—… ì‹¤í–‰"""
        task_type = task.get("type")
        data = task.get("data", {})
        
        if task_type == "stock_analysis":
            await self.process_stock_analysis(data)
        elif task_type == "news_sentiment":
            await self.process_news_sentiment(data)
        elif task_type == "portfolio_update":
            await self.process_portfolio_update(data)
    
    async def process_stock_analysis(self, data: dict):
        """ì£¼ì‹ ë¶„ì„ ì‘ì—… ì²˜ë¦¬"""
        symbol = data.get("symbol")
        # CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor, 
            self.analyze_stock_sync, 
            symbol
        )
        
        # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        cache_key = f"analysis:{symbol}"
        await self.redis.setex(cache_key, 3600, json.dumps(result))
    
    def analyze_stock_sync(self, symbol: str):
        """ë™ê¸°ì  ì£¼ì‹ ë¶„ì„ (CPU ì§‘ì•½ì )"""
        # ì‹¤ì œ ë¶„ì„ ë¡œì§
        pass
```

#### ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ë§
```python
# db_pool.py
import asyncpg
import asyncio
from typing import Optional

class DatabasePool:
    def __init__(self):
        self.pool: Optional[asyncpg.Pool] = None
    
    async def init_pool(self):
        """ì—°ê²° í’€ ì´ˆê¸°í™”"""
        self.pool = await asyncpg.create_pool(
            host="localhost",
            port=5432,
            user="stockpilot",
            password="password",
            database="stockpilot",
            min_size=10,
            max_size=50,
            max_queries=50000,
            max_inactive_connection_lifetime=300,
            command_timeout=60
        )
    
    async def execute_query(self, query: str, *args):
        """ì¿¼ë¦¬ ì‹¤í–‰"""
        async with self.pool.acquire() as connection:
            return await connection.fetch(query, *args)
    
    async def execute_transaction(self, queries: list):
        """íŠ¸ëœì­ì…˜ ì‹¤í–‰"""
        async with self.pool.acquire() as connection:
            async with connection.transaction():
                results = []
                for query, args in queries:
                    result = await connection.fetch(query, *args)
                    results.append(result)
                return results
    
    async def close_pool(self):
        """ì—°ê²° í’€ ì¢…ë£Œ"""
        if self.pool:
            await self.pool.close()

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
db_pool = DatabasePool()
```

### í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™”

#### React ì„±ëŠ¥ ìµœì í™”
```javascript
// performance_optimizations.js
import React, { memo, useMemo, useCallback, lazy, Suspense } from 'react';
import { debounce } from 'lodash';

// 1. ì»´í¬ë„ŒíŠ¸ ë©”ëª¨ì´ì œì´ì…˜
const StockCard = memo(({ stock, onSelect }) => {
  return (
    <div onClick={() => onSelect(stock.symbol)}>
      <h3>{stock.symbol}</h3>
      <p>${stock.price}</p>
    </div>
  );
});

// 2. ì½”ë“œ ìŠ¤í”Œë¦¬íŒ…
const StockAnalysis = lazy(() => import('./StockAnalysis'));
const Portfolio = lazy(() => import('./Portfolio'));

// 3. ê°€ìƒí™”ëœ ë¦¬ìŠ¤íŠ¸
import { VariableSizeList as List } from 'react-window';

const VirtualizedStockList = ({ stocks }) => {
  const getItemSize = useCallback((index) => {
    return stocks[index].expanded ? 200 : 80;
  }, [stocks]);

  const renderItem = useCallback(({ index, style }) => (
    <div style={style}>
      <StockCard stock={stocks[index]} />
    </div>
  ), [stocks]);

  return (
    <List
      height={600}
      itemCount={stocks.length}
      itemSize={getItemSize}
    >
      {renderItem}
    </List>
  );
};

// 4. API í˜¸ì¶œ ìµœì í™”
const useStockData = (symbol) => {
  const [data, setData] = useState(null);
  const [loading, setLoading] = useState(false);

  const fetchData = useCallback(
    debounce(async (sym) => {
      setLoading(true);
      try {
        const response = await fetch(`/api/v1/stocks/${sym}/data`);
        const result = await response.json();
        setData(result);
      } finally {
        setLoading(false);
      }
    }, 300),
    []
  );

  useEffect(() => {
    if (symbol) {
      fetchData(symbol);
    }
  }, [symbol, fetchData]);

  return { data, loading };
};

// 5. WebSocket ì—°ê²° ìµœì í™”
const useWebSocket = (url, options = {}) => {
  const [socket, setSocket] = useState(null);
  const [data, setData] = useState(null);

  useEffect(() => {
    const ws = new WebSocket(url);
    
    ws.onopen = () => {
      setSocket(ws);
    };

    ws.onmessage = (event) => {
      const newData = JSON.parse(event.data);
      setData(prevData => ({
        ...prevData,
        ...newData
      }));
    };

    ws.onerror = (error) => {
      console.error('WebSocket error:', error);
      // ìë™ ì¬ì—°ê²° ë¡œì§
      setTimeout(() => {
        setSocket(null);
      }, 5000);
    };

    return () => {
      ws.close();
    };
  }, [url]);

  return { socket, data };
};
```

---

## ğŸ›¡ï¸ ë³´ì•ˆ ê´€ë¦¬

### ì‹œìŠ¤í…œ ë³´ì•ˆ

#### ë°©í™”ë²½ ì„¤ì •
```bash
#!/bin/bash
# setup_firewall.sh

# UFW ì´ˆê¸° ì„¤ì •
sudo ufw --force reset
sudo ufw default deny incoming
sudo ufw default allow outgoing

# SSH ì ‘ê·¼ í—ˆìš© (íŠ¹ì • IPë§Œ)
sudo ufw allow from 192.168.1.0/24 to any port 22
sudo ufw allow from 10.0.0.0/8 to any port 22

# HTTP/HTTPS í—ˆìš©
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp

# ë‚´ë¶€ ì„œë¹„ìŠ¤ í¬íŠ¸ (ë¡œì»¬í˜¸ìŠ¤íŠ¸ë§Œ)
sudo ufw allow from 127.0.0.1 to any port 5432  # PostgreSQL
sudo ufw allow from 127.0.0.1 to any port 6379  # Redis
sudo ufw allow from 127.0.0.1 to any port 8000  # API
sudo ufw allow from 127.0.0.1 to any port 8765  # WebSocket

# ëª¨ë‹ˆí„°ë§ í¬íŠ¸ (ê´€ë¦¬ì ë„¤íŠ¸ì›Œí¬ë§Œ)
sudo ufw allow from 192.168.1.0/24 to any port 9090  # Prometheus
sudo ufw allow from 192.168.1.0/24 to any port 3001  # Grafana

# ë°©í™”ë²½ í™œì„±í™”
sudo ufw --force enable

echo "âœ… ë°©í™”ë²½ ì„¤ì • ì™„ë£Œ"
```

#### SSL/TLS ì„¤ì •
```bash
#!/bin/bash
# setup_ssl.sh

# Let's Encrypt ì¸ì¦ì„œ ë°œê¸‰
sudo certbot --nginx -d stockpilot.ai -d www.stockpilot.ai --non-interactive --agree-tos -m admin@stockpilot.ai

# SSL ì„¤ì • ê°•í™”
cat << 'EOF' > /etc/nginx/snippets/ssl-params.conf
ssl_protocols TLSv1.2 TLSv1.3;
ssl_prefer_server_ciphers on;
ssl_dhparam /etc/nginx/dhparam.pem;
ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384;
ssl_ecdh_curve secp384r1;
ssl_session_timeout  10m;
ssl_session_cache shared:SSL:10m;
ssl_session_tickets off;
ssl_stapling on;
ssl_stapling_verify on;
resolver 8.8.8.8 8.8.4.4 valid=300s;
resolver_timeout 5s;

# ë³´ì•ˆ í—¤ë”
add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";
add_header X-Frame-Options DENY;
add_header X-Content-Type-Options nosniff;
add_header X-XSS-Protection "1; mode=block";
add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://cdn.jsdelivr.net; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; font-src 'self' https://fonts.gstatic.com; img-src 'self' data: https:; connect-src 'self' wss: https:";
EOF

# DH íŒŒë¼ë¯¸í„° ìƒì„±
sudo openssl dhparam -out /etc/nginx/dhparam.pem 2048

# ìë™ ê°±ì‹  ì„¤ì •
echo "0 12 * * * /usr/bin/certbot renew --quiet" | sudo crontab -

echo "âœ… SSL/TLS ì„¤ì • ì™„ë£Œ"
```

#### ì¹¨ì… íƒì§€ ì‹œìŠ¤í…œ
```bash
#!/bin/bash
# setup_intrusion_detection.sh

# Fail2ban ì„¤ì¹˜ ë° ì„¤ì •
sudo apt install -y fail2ban

# SSH ë³´í˜¸ ì„¤ì •
cat << 'EOF' > /etc/fail2ban/jail.local
[DEFAULT]
bantime = 3600
findtime = 600
maxretry = 3
ignoreip = 127.0.0.1/8 192.168.1.0/24

[sshd]
enabled = true
port = 22
filter = sshd
logpath = /var/log/auth.log

[nginx-http-auth]
enabled = true
filter = nginx-http-auth
port = http,https
logpath = /var/log/nginx/error.log

[nginx-req-limit]
enabled = true
filter = nginx-req-limit
action = iptables-multiport[name=ReqLimit, port="http,https", protocol=tcp]
logpath = /var/log/nginx/access.log
findtime = 600
bantime = 7200
maxretry = 10
EOF

# ì»¤ìŠ¤í…€ í•„í„° ìƒì„±
cat << 'EOF' > /etc/fail2ban/filter.d/stockpilot-api.conf
[Definition]
failregex = ^<HOST> .* "POST /api/v1/auth/login" 401
            ^<HOST> .* "POST /api/v1/auth/login" 429
ignoreregex =
EOF

# StockPilot API ë³´í˜¸ ê·œì¹™
cat << 'EOF' >> /etc/fail2ban/jail.local

[stockpilot-api]
enabled = true
port = http,https
filter = stockpilot-api
logpath = /var/log/nginx/access.log
maxretry = 5
bantime = 1800
EOF

sudo systemctl enable fail2ban
sudo systemctl restart fail2ban

echo "âœ… ì¹¨ì… íƒì§€ ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ"
```

### ì• í”Œë¦¬ì¼€ì´ì…˜ ë³´ì•ˆ

#### ì¸ì¦ ë° ê¶Œí•œ ê´€ë¦¬
```python
# security.py
import jwt
import bcrypt
from datetime import datetime, timedelta
from functools import wraps
from flask import request, jsonify, current_app
import redis

class SecurityManager:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=1)
        self.jwt_algorithm = 'HS256'
        
    def hash_password(self, password: str) -> str:
        """ë¹„ë°€ë²ˆí˜¸ í•´ì‹œí™”"""
        salt = bcrypt.gensalt(rounds=12)
        return bcrypt.hashpw(password.encode('utf-8'), salt).decode('utf-8')
    
    def verify_password(self, password: str, hashed: str) -> bool:
        """ë¹„ë°€ë²ˆí˜¸ ê²€ì¦"""
        return bcrypt.checkpw(password.encode('utf-8'), hashed.encode('utf-8'))
    
    def generate_jwt_token(self, user_id: str, expires_in: int = 3600) -> str:
        """JWT í† í° ìƒì„±"""
        payload = {
            'user_id': user_id,
            'exp': datetime.utcnow() + timedelta(seconds=expires_in),
            'iat': datetime.utcnow(),
            'iss': 'stockpilot.ai'
        }
        return jwt.encode(payload, current_app.config['JWT_SECRET_KEY'], algorithm=self.jwt_algorithm)
    
    def verify_jwt_token(self, token: str) -> dict:
        """JWT í† í° ê²€ì¦"""
        try:
            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ í™•ì¸
            if self.redis_client.get(f"blacklist:{token}"):
                raise jwt.InvalidTokenError("Token is blacklisted")
            
            payload = jwt.decode(
                token, 
                current_app.config['JWT_SECRET_KEY'], 
                algorithms=[self.jwt_algorithm]
            )
            return payload
        except jwt.ExpiredSignatureError:
            raise jwt.InvalidTokenError("Token has expired")
        except jwt.InvalidTokenError as e:
            raise e
    
    def blacklist_token(self, token: str, expires_in: int = 3600):
        """í† í° ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë“±ë¡"""
        self.redis_client.setex(f"blacklist:{token}", expires_in, "1")
    
    def require_auth(self, f):
        """ì¸ì¦ í•„ìˆ˜ ë°ì½”ë ˆì´í„°"""
        @wraps(f)
        def decorated_function(*args, **kwargs):
            auth_header = request.headers.get('Authorization')
            if not auth_header or not auth_header.startswith('Bearer '):
                return jsonify({'error': 'Missing or invalid token'}), 401
            
            token = auth_header.split(' ')[1]
            try:
                payload = self.verify_jwt_token(token)
                request.current_user = payload
                return f(*args, **kwargs)
            except jwt.InvalidTokenError as e:
                return jsonify({'error': str(e)}), 401
        
        return decorated_function
    
    def require_permission(self, permission: str):
        """ê¶Œí•œ í™•ì¸ ë°ì½”ë ˆì´í„°"""
        def decorator(f):
            @wraps(f)
            def decorated_function(*args, **kwargs):
                if not hasattr(request, 'current_user'):
                    return jsonify({'error': 'Authentication required'}), 401
                
                user_id = request.current_user['user_id']
                if not self.check_user_permission(user_id, permission):
                    return jsonify({'error': 'Insufficient permissions'}), 403
                
                return f(*args, **kwargs)
            return decorated_function
        return decorator
    
    def check_user_permission(self, user_id: str, permission: str) -> bool:
        """ì‚¬ìš©ì ê¶Œí•œ í™•ì¸"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©ì ê¶Œí•œ ì¡°íšŒ
        user_permissions = self.get_user_permissions(user_id)
        return permission in user_permissions
    
    def get_user_permissions(self, user_id: str) -> list:
        """ì‚¬ìš©ì ê¶Œí•œ ëª©ë¡ ì¡°íšŒ"""
        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        cached_permissions = self.redis_client.get(f"permissions:{user_id}")
        if cached_permissions:
            return json.loads(cached_permissions)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ í›„ ìºì‹±
        # ì‹¤ì œ êµ¬í˜„ í•„ìš”
        permissions = []  # DB ì¿¼ë¦¬ ê²°ê³¼
        self.redis_client.setex(f"permissions:{user_id}", 300, json.dumps(permissions))
        return permissions

# ì‚¬ìš© ì˜ˆì‹œ
security = SecurityManager()

@app.route('/api/v1/admin/users')
@security.require_auth
@security.require_permission('admin.users.read')
def get_users():
    # ê´€ë¦¬ìë§Œ ì ‘ê·¼ ê°€ëŠ¥í•œ ì‚¬ìš©ì ëª©ë¡
    pass
```

#### API ë³´ì•ˆ ê°•í™”
```python
# api_security.py
from flask import request, jsonify
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
import hashlib
import hmac
import time

class APISecurityMiddleware:
    def __init__(self, app):
        self.app = app
        
        # Rate Limiting ì„¤ì •
        self.limiter = Limiter(
            app,
            key_func=get_remote_address,
            default_limits=["100 per hour", "10 per minute"]
        )
        
        self.setup_security_middleware()
    
    def setup_security_middleware(self):
        """ë³´ì•ˆ ë¯¸ë“¤ì›¨ì–´ ì„¤ì •"""
        
        @self.app.before_request
        def security_checks():
            # 1. API í‚¤ ê²€ì¦
            if request.path.startswith('/api/'):
                if not self.verify_api_key():
                    return jsonify({'error': 'Invalid API key'}), 401
            
            # 2. ìš”ì²­ ì„œëª… ê²€ì¦ (ì¤‘ìš”í•œ ì—”ë“œí¬ì¸íŠ¸)
            if request.path.startswith('/api/v1/admin/'):
                if not self.verify_request_signature():
                    return jsonify({'error': 'Invalid request signature'}), 401
            
            # 3. IP í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í™•ì¸ (ê´€ë¦¬ ê¸°ëŠ¥)
            if request.path.startswith('/api/v1/admin/'):
                if not self.check_ip_whitelist():
                    return jsonify({'error': 'Access denied'}), 403
        
        @self.app.after_request
        def security_headers(response):
            """ë³´ì•ˆ í—¤ë” ì¶”ê°€"""
            response.headers['X-Content-Type-Options'] = 'nosniff'
            response.headers['X-Frame-Options'] = 'DENY'
            response.headers['X-XSS-Protection'] = '1; mode=block'
            response.headers['Strict-Transport-Security'] = 'max-age=31536000; includeSubDomains'
            return response
    
    def verify_api_key(self) -> bool:
        """API í‚¤ ê²€ì¦"""
        api_key = request.headers.get('X-API-Key')
        if not api_key:
            return False
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ API í‚¤ ê²€ì¦
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í•´ì‹œëœ API í‚¤ì™€ ë¹„êµ
        valid_keys = self.get_valid_api_keys()
        return api_key in valid_keys
    
    def verify_request_signature(self) -> bool:
        """ìš”ì²­ ì„œëª… ê²€ì¦"""
        signature = request.headers.get('X-Signature')
        timestamp = request.headers.get('X-Timestamp')
        
        if not signature or not timestamp:
            return False
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê²€ì¦ (5ë¶„ ì´ë‚´)
        current_time = int(time.time())
        if abs(current_time - int(timestamp)) > 300:
            return False
        
        # ì„œëª… ìƒì„± ë° ê²€ì¦
        secret_key = self.get_secret_key_for_request()
        payload = request.get_data() + timestamp.encode()
        expected_signature = hmac.new(
            secret_key.encode(),
            payload,
            hashlib.sha256
        ).hexdigest()
        
        return hmac.compare_digest(signature, expected_signature)
    
    def check_ip_whitelist(self) -> bool:
        """IP í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í™•ì¸"""
        client_ip = get_remote_address()
        whitelist = self.get_ip_whitelist()
        return client_ip in whitelist
    
    def get_valid_api_keys(self) -> set:
        """ìœ íš¨í•œ API í‚¤ ëª©ë¡ ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„: ë°ì´í„°ë² ì´ìŠ¤ ë˜ëŠ” ìºì‹œì—ì„œ ì¡°íšŒ
        return {'api_key_1', 'api_key_2'}
    
    def get_secret_key_for_request(self) -> str:
        """ìš”ì²­ë³„ ë¹„ë°€ í‚¤ ì¡°íšŒ"""
        # ì‹¤ì œ êµ¬í˜„: API í‚¤ë³„ ê³ ìœ  ë¹„ë°€ í‚¤ ë°˜í™˜
        return "secret_key"
    
    def get_ip_whitelist(self) -> set:
        """IP í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
        return {'127.0.0.1', '192.168.1.0/24'}

# Rate Limiting ì„¤ì •
@limiter.limit("5 per minute")
def login_endpoint():
    pass

@limiter.limit("100 per hour")
def api_endpoint():
    pass

# ë¸Œë£¨íŠ¸ í¬ìŠ¤ ê³µê²© ë°©ì§€
@limiter.limit("3 per minute", per_method=True)
def password_reset():
    pass
```

### ë°ì´í„° ë³´í˜¸

#### ë°ì´í„° ì•”í˜¸í™”
```python
# encryption.py
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import base64
import os

class DataEncryption:
    def __init__(self, password: str = None):
        if password:
            self.key = self.derive_key_from_password(password)
        else:
            self.key = self.generate_key()
        self.cipher = Fernet(self.key)
    
    def generate_key(self) -> bytes:
        """ìƒˆ ì•”í˜¸í™” í‚¤ ìƒì„±"""
        return Fernet.generate_key()
    
    def derive_key_from_password(self, password: str) -> bytes:
        """íŒ¨ìŠ¤ì›Œë“œì—ì„œ í‚¤ ìœ ë„"""
        salt = os.urandom(16)
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )
        key = base64.urlsafe_b64encode(kdf.derive(password.encode()))
        return key
    
    def encrypt(self, data: str) -> str:
        """ë°ì´í„° ì•”í˜¸í™”"""
        encrypted_data = self.cipher.encrypt(data.encode())
        return base64.urlsafe_b64encode(encrypted_data).decode()
    
    def decrypt(self, encrypted_data: str) -> str:
        """ë°ì´í„° ë³µí˜¸í™”"""
        encrypted_bytes = base64.urlsafe_b64decode(encrypted_data.encode())
        decrypted_data = self.cipher.decrypt(encrypted_bytes)
        return decrypted_data.decode()
    
    def encrypt_file(self, file_path: str, output_path: str):
        """íŒŒì¼ ì•”í˜¸í™”"""
        with open(file_path, 'rb') as file:
            file_data = file.read()
        
        encrypted_data = self.cipher.encrypt(file_data)
        
        with open(output_path, 'wb') as encrypted_file:
            encrypted_file.write(encrypted_data)

# ì‚¬ìš© ì˜ˆì‹œ
encryption = DataEncryption(password="your_secure_password")

# ë¯¼ê°í•œ ë°ì´í„° ì•”í˜¸í™”
api_key = "sk-..."
encrypted_api_key = encryption.encrypt(api_key)

# ë°ì´í„°ë² ì´ìŠ¤ì— ì•”í˜¸í™”ëœ ë°ì´í„° ì €ì¥
def store_encrypted_api_key(user_id: str, api_key: str):
    encrypted_key = encryption.encrypt(api_key)
    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë¡œì§
    pass

def retrieve_api_key(user_id: str) -> str:
    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì•”í˜¸í™”ëœ í‚¤ ì¡°íšŒ
    encrypted_key = get_encrypted_key_from_db(user_id)
    return encryption.decrypt(encrypted_key)
```

---

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨
```bash
# ë¬¸ì œ: systemctl start stockpilot-api ì‹¤íŒ¨

# 1ë‹¨ê³„: ìƒíƒœ í™•ì¸
sudo systemctl status stockpilot-api

# 2ë‹¨ê³„: ìƒì„¸ ë¡œê·¸ í™•ì¸
sudo journalctl -u stockpilot-api -n 50 -f

# 3ë‹¨ê³„: ì„¤ì • íŒŒì¼ ê²€ì¦
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
cat /opt/stockpilot/.env.production

# Python ê°€ìƒí™˜ê²½ í™•ì¸
cd /opt/stockpilot/backend
source venv/bin/activate
python -c "import sys; print(sys.path)"

# 4ë‹¨ê³„: ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°
pip install --upgrade -r requirements.txt

# 5ë‹¨ê³„: ê¶Œí•œ ë¬¸ì œ í™•ì¸
ls -la /opt/stockpilot/
sudo chown -R stockpilot:stockpilot /opt/stockpilot/
```

#### 2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜
```bash
# ë¬¸ì œ: PostgreSQL ì—°ê²° ì‹¤íŒ¨

# 1ë‹¨ê³„: PostgreSQL ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
sudo systemctl status postgresql
sudo systemctl restart postgresql

# 2ë‹¨ê³„: ì—°ê²° í…ŒìŠ¤íŠ¸
psql -h localhost -U stockpilot -d stockpilot

# 3ë‹¨ê³„: ì—°ê²° ì„¤ì • í™•ì¸
sudo -u postgres psql -c "SELECT * FROM pg_stat_activity;"

# 4ë‹¨ê³„: pg_hba.conf í™•ì¸
sudo nano /etc/postgresql/15/main/pg_hba.conf
# ë‹¤ìŒ ë¼ì¸ì´ ìˆëŠ”ì§€ í™•ì¸:
# local   all             stockpilot                              md5
# host    all             stockpilot      127.0.0.1/32            md5

# 5ë‹¨ê³„: PostgreSQL ì„¤ì • íŒŒì¼ í™•ì¸
sudo nano /etc/postgresql/15/main/postgresql.conf
# listen_addresses = 'localhost'
# port = 5432

sudo systemctl restart postgresql
```

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ
```bash
# ë¬¸ì œ: Out of Memory ì—ëŸ¬

# 1ë‹¨ê³„: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
free -h
ps aux --sort=-%mem | head -10

# 2ë‹¨ê³„: ìŠ¤ì™‘ íŒŒì¼ ìƒì„±
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# ì˜êµ¬ì  ìŠ¤ì™‘ ì„¤ì •
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab

# 3ë‹¨ê³„: Python ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
# systemd ì„œë¹„ìŠ¤ íŒŒì¼ì— ì¶”ê°€:
# MemoryMax=2G
# MemoryHigh=1.5G

# 4ë‹¨ê³„: PostgreSQL ë©”ëª¨ë¦¬ íŠœë‹
sudo -u postgres psql -c "
ALTER SYSTEM SET shared_buffers = '512MB';
ALTER SYSTEM SET work_mem = '64MB';
ALTER SYSTEM SET maintenance_work_mem = '256MB';
SELECT pg_reload_conf();
"
```

#### 4. ë†’ì€ CPU ì‚¬ìš©ë¥ 
```bash
# ë¬¸ì œ: CPU ì‚¬ìš©ë¥  100%

# 1ë‹¨ê³„: í”„ë¡œì„¸ìŠ¤ í™•ì¸
top -o %CPU
htop

# 2ë‹¨ê³„: íŠ¹ì • í”„ë¡œì„¸ìŠ¤ ë¶„ì„
# Python í”„ë¡œíŒŒì¼ë§
cd /opt/stockpilot/backend
source venv/bin/activate
python -m cProfile -o profile.out main.py

# 3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”
sudo -u postgres psql stockpilot -c "
SELECT query, calls, total_time, mean_time
FROM pg_stat_statements
ORDER BY total_time DESC LIMIT 10;
"

# 4ë‹¨ê³„: ë¹„ë™ê¸° ì‘ì—… í í™•ì¸
redis-cli LLEN stockpilot:tasks
redis-cli LRANGE stockpilot:tasks 0 -1
```

#### 5. WebSocket ì—°ê²° ë¬¸ì œ
```bash
# ë¬¸ì œ: WebSocket ì—°ê²° ì‹¤íŒ¨

# 1ë‹¨ê³„: WebSocket ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
sudo systemctl status stockpilot-websocket
sudo netstat -tlnp | grep :8765

# 2ë‹¨ê³„: ë°©í™”ë²½ ì„¤ì • í™•ì¸
sudo ufw status
sudo ufw allow 8765/tcp

# 3ë‹¨ê³„: Nginx WebSocket í”„ë¡ì‹œ ì„¤ì • í™•ì¸
sudo nginx -t
sudo systemctl reload nginx

# 4ë‹¨ê³„: WebSocket ì—°ê²° í…ŒìŠ¤íŠ¸
wscat -c ws://localhost:8765

# 5ë‹¨ê³„: í´ë¼ì´ì–¸íŠ¸ ë¡œê·¸ í™•ì¸
# ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ Consoleì—ì„œ:
# WebSocket connection error ë©”ì‹œì§€ í™•ì¸
```

### ì„±ëŠ¥ ë¬¸ì œ ì§„ë‹¨

#### 1. ì‘ë‹µ ì‹œê°„ ì§€ì—°
```python
#!/usr/bin/env python3
# performance_diagnosis.py

import asyncio
import aiohttp
import time
import statistics

async def diagnose_api_performance():
    """API ì„±ëŠ¥ ì§„ë‹¨"""
    
    endpoints = [
        '/api/v1/system/health',
        '/api/v1/stocks/AAPL/data',
        '/api/v1/news',
        '/api/v1/portfolio'
    ]
    
    results = {}
    
    for endpoint in endpoints:
        print(f"\nğŸ” {endpoint} ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        response_times = []
        
        async with aiohttp.ClientSession() as session:
            for i in range(10):
                start_time = time.time()
                
                try:
                    async with session.get(f'http://localhost:8000{endpoint}') as response:
                        await response.text()
                        response_time = (time.time() - start_time) * 1000
                        response_times.append(response_time)
                        
                        print(f"  ìš”ì²­ {i+1}: {response_time:.1f}ms (HTTP {response.status})")
                        
                except Exception as e:
                    print(f"  ìš”ì²­ {i+1}: ì‹¤íŒ¨ - {e}")
        
        if response_times:
            results[endpoint] = {
                'avg': statistics.mean(response_times),
                'min': min(response_times),
                'max': max(response_times),
                'p95': sorted(response_times)[int(len(response_times) * 0.95)]
            }
            
            print(f"  í‰ê· : {results[endpoint]['avg']:.1f}ms")
            print(f"  ìµœì†Œ: {results[endpoint]['min']:.1f}ms")
            print(f"  ìµœëŒ€: {results[endpoint]['max']:.1f}ms")
            print(f"  95th: {results[endpoint]['p95']:.1f}ms")
    
    return results

if __name__ == "__main__":
    results = asyncio.run(diagnose_api_performance())
```

#### 2. ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë¶„ì„
```sql
-- slow_query_analysis.sql

-- 1. ê°€ì¥ ëŠë¦° ì¿¼ë¦¬ TOP 10
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements
ORDER BY total_time DESC
LIMIT 10;

-- 2. ìì£¼ ì‹¤í–‰ë˜ëŠ” ì¿¼ë¦¬ TOP 10
SELECT 
    query,
    calls,
    total_time,
    mean_time
FROM pg_stat_statements
ORDER BY calls DESC
LIMIT 10;

-- 3. ì¸ë±ìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„
SELECT 
    schemaname,
    tablename,
    attname,
    n_distinct,
    correlation,
    most_common_vals,
    most_common_freqs
FROM pg_stats
WHERE schemaname = 'public'
ORDER BY tablename, attname;

-- 4. ì¸ë±ìŠ¤ íš¨ìœ¨ì„± í™•ì¸
SELECT 
    t.tablename,
    indexname,
    c.reltuples AS num_rows,
    pg_size_pretty(pg_relation_size(quote_ident(t.tablename)::text)) AS table_size,
    pg_size_pretty(pg_relation_size(quote_ident(indexrelname)::text)) AS index_size,
    CASE WHEN indisunique THEN 'Y' ELSE 'N' END AS UNIQUE,
    idx_scan AS number_of_scans,
    idx_tup_read AS tuples_read,
    idx_tup_fetch AS tuples_fetched
FROM pg_tables t
LEFT OUTER JOIN pg_class c ON c.relname = t.tablename
LEFT OUTER JOIN (
    SELECT 
        c.relname AS ctablename, 
        ipg.relname AS indexname, 
        x.indnatts AS number_of_columns, 
        idx_scan, 
        idx_tup_read, 
        idx_tup_fetch, 
        indexrelname, 
        indisunique 
    FROM pg_index x
    JOIN pg_class c ON c.oid = x.indrelid
    JOIN pg_class ipg ON ipg.oid = x.indexrelid
    JOIN pg_stat_user_indexes psui ON x.indexrelid = psui.indexrelid
) AS foo ON t.tablename = foo.ctablename
WHERE t.schemaname = 'public'
ORDER BY 1, 2;
```

### ì¥ì•  ëŒ€ì‘ í”Œë ˆì´ë¶

#### 1. ì „ì²´ ì„œë¹„ìŠ¤ ë‹¤ìš´
```bash
#!/bin/bash
# emergency_recovery.sh

echo "ğŸš¨ ê¸´ê¸‰ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘"

# 1. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
echo "1. ì„œë¹„ìŠ¤ ìƒíƒœ ì ê²€"
systemctl status stockpilot-* | grep -E "(Active|Main PID)"

# 2. ë¡œê·¸ í™•ì¸
echo "2. ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ í™•ì¸"
tail -100 /var/log/stockpilot/*.log | grep -i error

# 3. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
echo "3. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸"
echo "CPU: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}')"
echo "Memory: $(free -m | awk 'NR==2{printf "%.1f%%", $3*100/$2}')"
echo "Disk: $(df -h | grep '/opt' | awk '{print $5}')"

# 4. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
echo "4. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸"
netstat -tlnp | grep -E "(5432|6379|8000|8765)"

# 5. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
echo "5. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"
if pg_isready -h localhost -p 5432 -U stockpilot; then
    echo "âœ… PostgreSQL ì—°ê²° ì •ìƒ"
else
    echo "âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨ - ì¬ì‹œì‘ ì‹œë„"
    sudo systemctl restart postgresql
    sleep 5
fi

# 6. Redis ì—°ê²° í…ŒìŠ¤íŠ¸
echo "6. Redis ì—°ê²° í…ŒìŠ¤íŠ¸"
if redis-cli ping > /dev/null 2>&1; then
    echo "âœ… Redis ì—°ê²° ì •ìƒ"
else
    echo "âŒ Redis ì—°ê²° ì‹¤íŒ¨ - ì¬ì‹œì‘ ì‹œë„"
    sudo systemctl restart redis-server
    sleep 5
fi

# 7. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
echo "7. StockPilot ì„œë¹„ìŠ¤ ì¬ì‹œì‘"
sudo systemctl restart stockpilot-*

# 8. í—¬ìŠ¤ ì²´í¬
echo "8. í—¬ìŠ¤ ì²´í¬ ì‹¤í–‰"
sleep 10
for i in {1..5}; do
    response=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/api/v1/system/health)
    if [ "$response" = "200" ]; then
        echo "âœ… ì„œë¹„ìŠ¤ ì •ìƒ ë³µêµ¬ ì™„ë£Œ"
        exit 0
    else
        echo "â³ í—¬ìŠ¤ ì²´í¬ ì¬ì‹œë„ $i/5 (HTTP $response)"
        sleep 10
    fi
done

echo "âŒ ì„œë¹„ìŠ¤ ë³µêµ¬ ì‹¤íŒ¨ - ìˆ˜ë™ ì ê²€ í•„ìš”"
exit 1
```

#### 2. ë°ì´í„°ë² ì´ìŠ¤ ì¥ì• 
```bash
#!/bin/bash
# database_recovery.sh

echo "ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì¥ì•  ë³µêµ¬ ì‹œì‘"

# 1. PostgreSQL ìƒíƒœ í™•ì¸
if ! systemctl is-active --quiet postgresql; then
    echo "PostgreSQL ì„œë¹„ìŠ¤ ì¤‘ë‹¨ë¨ - ì¬ì‹œì‘ ì‹œë„"
    sudo systemctl start postgresql
    sleep 5
fi

# 2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
if ! pg_isready -h localhost -p 5432 -U stockpilot; then
    echo "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¶ˆê°€ - ìƒì„¸ ì§„ë‹¨ ì‹œì‘"
    
    # ë¡œê·¸ í™•ì¸
    sudo tail -50 /var/log/postgresql/postgresql-*.log
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    df -h /var/lib/postgresql
    
    # í”„ë¡œì„¸ìŠ¤ í™•ì¸
    ps aux | grep postgres
fi

# 3. ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
echo "ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ì‹œì‘"
sudo -u postgres psql stockpilot -c "
SELECT schemaname, tablename, n_tup_ins, n_tup_upd, n_tup_del, n_tup_hot_upd
FROM pg_stat_user_tables
ORDER BY schemaname, tablename;
"

# 4. ë°±ì—…ì—ì„œ ë³µêµ¬ (ìµœí›„ ìˆ˜ë‹¨)
if [ "$1" = "--restore-from-backup" ]; then
    echo "âš ï¸ ë°±ì—…ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬ ì‹œì‘"
    latest_backup=$(ls -t /opt/stockpilot/backups/db_backup_*.sql.gz | head -n1)
    if [ -n "$latest_backup" ]; then
        echo "ìµœì‹  ë°±ì—… íŒŒì¼: $latest_backup"
        /opt/stockpilot/scripts/restore_database.sh "$latest_backup"
    else
        echo "âŒ ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
        exit 1
    fi
fi
```

#### 3. ë†’ì€ ë¶€í•˜ ìƒí™©
```bash
#!/bin/bash
# high_load_mitigation.sh

echo "âš¡ ë†’ì€ ë¶€í•˜ ìƒí™© ëŒ€ì‘ ì‹œì‘"

# 1. í˜„ì¬ ë¶€í•˜ ìƒíƒœ í™•ì¸
load_avg=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//')
cpu_count=$(nproc)
load_threshold=$(echo "$cpu_count * 2" | bc)

echo "í˜„ì¬ Load Average: $load_avg"
echo "CPU ì½”ì–´ ìˆ˜: $cpu_count"
echo "ë¶€í•˜ ì„ê³„ê°’: $load_threshold"

if (( $(echo "$load_avg > $load_threshold" | bc -l) )); then
    echo "ğŸš¨ ë†’ì€ ë¶€í•˜ ê°ì§€ë¨"
    
    # 2. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì´ ë†’ì€ í”„ë¡œì„¸ìŠ¤ ì‹ë³„
    echo "ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë†’ì€ í”„ë¡œì„¸ìŠ¤:"
    ps aux --sort=-%cpu | head -10
    
    # 3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìˆ˜ í™•ì¸
    active_connections=$(sudo -u postgres psql -t -c "SELECT count(*) FROM pg_stat_activity;")
    echo "í™œì„± DB ì—°ê²° ìˆ˜: $active_connections"
    
    if [ "$active_connections" -gt 100 ]; then
        echo "âš ï¸ DB ì—°ê²° ìˆ˜ê°€ ê³¼ë„í•¨ - ì—°ê²° ì¢…ë£Œ"
        sudo -u postgres psql -c "
        SELECT pg_terminate_backend(pid)
        FROM pg_stat_activity
        WHERE state = 'idle'
        AND query_start < NOW() - INTERVAL '10 minutes';
        "
    fi
    
    # 4. ìºì‹œ ì •ë¦¬
    echo "ìºì‹œ ì •ë¦¬ ì‹¤í–‰"
    redis-cli FLUSHALL
    
    # 5. ë¶ˆí•„ìš”í•œ ì„œë¹„ìŠ¤ ì„ì‹œ ì¤‘ë‹¨
    echo "ë¹„í•„ìˆ˜ ì„œë¹„ìŠ¤ ì„ì‹œ ì¤‘ë‹¨"
    sudo systemctl stop stockpilot-cost-dashboard
    
    # 6. Rate Limiting ê°•í™”
    echo "Rate Limiting ê°•í™”"
    # Nginx ì„¤ì •ì—ì„œ rate limit ê°•í™”
    sudo nginx -s reload
    
else
    echo "âœ… ë¶€í•˜ ìˆ˜ì¤€ ì •ìƒ"
fi
```

---

## ğŸ“Š ì¥ì•  ëŒ€ì‘

### ì¥ì•  ì•Œë¦¼ ì‹œìŠ¤í…œ

#### ìë™ ì•Œë¦¼ ì„¤ì •
```python
#!/usr/bin/env python3
# alert_manager.py

import smtplib
import requests
import json
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime
import logging

class AlertManager:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def send_email_alert(self, subject: str, message: str, priority: str = "high"):
        """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
        try:
            msg = MIMEMultipart()
            msg['From'] = self.config['email']['sender']
            msg['To'] = ', '.join(self.config['email']['recipients'])
            msg['Subject'] = f"[{priority.upper()}] {subject}"
            
            body = f"""
StockPilot ì‹œìŠ¤í…œ ì•Œë¦¼

ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ìš°ì„ ìˆœìœ„: {priority.upper()}

ë©”ì‹œì§€:
{message}

---
StockPilot ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
            """
            
            msg.attach(MIMEText(body, 'plain'))
            
            with smtplib.SMTP(self.config['email']['smtp_server'], 587) as server:
                server.starttls()
                server.login(self.config['email']['username'], self.config['email']['password'])
                server.send_message(msg)
            
            self.logger.info(f"ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ: {subject}")
            
        except Exception as e:
            self.logger.error(f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
    
    def send_slack_alert(self, message: str, channel: str = "#alerts", priority: str = "high"):
        """Slack ì•Œë¦¼ ë°œì†¡"""
        try:
            color = {
                "critical": "#FF0000",
                "high": "#FF8C00", 
                "medium": "#FFD700",
                "low": "#00FF00"
            }.get(priority, "#FFD700")
            
            payload = {
                "channel": channel,
                "username": "StockPilot Monitor",
                "icon_emoji": ":warning:",
                "attachments": [{
                    "color": color,
                    "title": f"{priority.upper()} Priority Alert",
                    "text": message,
                    "timestamp": datetime.now().timestamp(),
                    "fields": [
                        {"title": "System", "value": "StockPilot", "short": True},
                        {"title": "Environment", "value": "Production", "short": True}
                    ]
                }]
            }
            
            response = requests.post(
                self.config['slack']['webhook_url'],
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                self.logger.info("Slack ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ")
            else:
                self.logger.error(f"Slack ë°œì†¡ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            self.logger.error(f"Slack ì•Œë¦¼ ì‹¤íŒ¨: {e}")
    
    def send_sms_alert(self, message: str, priority: str = "critical"):
        """SMS ì•Œë¦¼ ë°œì†¡ (ì¤‘ìš”í•œ ì•Œë¦¼ë§Œ)"""
        if priority not in ["critical", "high"]:
            return
        
        try:
            # Twilio, AWS SNS ë“±ì„ ì‚¬ìš©í•œ SMS ë°œì†¡
            # ì‹¤ì œ êµ¬í˜„ í•„ìš”
            pass
        except Exception as e:
            self.logger.error(f"SMS ë°œì†¡ ì‹¤íŒ¨: {e}")
    
    def trigger_alert(self, alert_type: str, message: str, priority: str = "high"):
        """í†µí•© ì•Œë¦¼ ë°œì†¡"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        full_message = f"[{timestamp}] {alert_type}: {message}"
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì•Œë¦¼ ë°©ì‹ ê²°ì •
        if priority == "critical":
            self.send_email_alert(alert_type, full_message, priority)
            self.send_slack_alert(full_message, priority=priority)
            self.send_sms_alert(full_message, priority)
        elif priority == "high":
            self.send_email_alert(alert_type, full_message, priority)
            self.send_slack_alert(full_message, priority=priority)
        else:
            self.send_slack_alert(full_message, priority=priority)

# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ê³¼ ì—°ë™
class SystemMonitor:
    def __init__(self):
        self.alert_manager = AlertManager(config)
        
    def check_system_health(self):
        """ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬"""
        issues = []
        
        # API ì„œë²„ í™•ì¸
        try:
            response = requests.get('http://localhost:8000/api/v1/system/health', timeout=5)
            if response.status_code != 200:
                issues.append(f"API ì„œë²„ ì´ìƒ (HTTP {response.status_code})")
        except:
            issues.append("API ì„œë²„ ì—°ê²° ë¶ˆê°€")
        
        # ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
        try:
            import psycopg2
            conn = psycopg2.connect(
                host="localhost", database="stockpilot", 
                user="stockpilot", password="password"
            )
            conn.close()
        except:
            issues.append("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        import psutil
        memory_percent = psutil.virtual_memory().percent
        if memory_percent > 90:
            issues.append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ìœ„í—˜ ({memory_percent}%)")
        elif memory_percent > 80:
            issues.append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì£¼ì˜ ({memory_percent}%)")
        
        # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        disk_percent = psutil.disk_usage('/').percent
        if disk_percent > 90:
            issues.append(f"ë””ìŠ¤í¬ ì‚¬ìš©ë¥  ìœ„í—˜ ({disk_percent}%)")
        
        # ì•Œë¦¼ ë°œì†¡
        if issues:
            priority = "critical" if any("ìœ„í—˜" in issue for issue in issues) else "high"
            message = "\n".join(issues)
            self.alert_manager.trigger_alert("ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬", message, priority)
        
        return len(issues) == 0

if __name__ == "__main__":
    monitor = SystemMonitor()
    monitor.check_system_health()
```

### ì¥ì•  ëŒ€ì‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ğŸ“‹ Level 1: ê²½ê³  (Warning)
```
â–¡ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ 70% ì´ìƒ
â–¡ API ì‘ë‹µì‹œê°„ 2ì´ˆ ì´ìƒ
â–¡ ì—ëŸ¬ìœ¨ 5% ì´ìƒ
â–¡ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìˆ˜ 80% ì´ìƒ

ëŒ€ì‘ ì ˆì°¨:
1. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ í™•ì¸
2. ë¡œê·¸ íŒŒì¼ ì ê²€
3. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìµœì í™”
4. 15ë¶„ í›„ ì¬ì ê²€
```

#### ğŸš¨ Level 2: ì‹¬ê° (Critical)
```
â–¡ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ 90% ì´ìƒ
â–¡ API ì„œë¹„ìŠ¤ ì¤‘ë‹¨
â–¡ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨
â–¡ ëŒ€ëŸ‰ì˜ 5xx ì—ëŸ¬ ë°œìƒ

ëŒ€ì‘ ì ˆì°¨:
1. ì¦‰ì‹œ ë‹´ë‹¹ì í˜¸ì¶œ
2. ì„œë¹„ìŠ¤ ìƒíƒœ ì ê²€
3. ê¸´ê¸‰ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
4. ë°±ì—… ì‹œìŠ¤í…œìœ¼ë¡œ ì „í™˜ ê³ ë ¤
5. ì‚¬ìš©ì ê³µì§€ ì¤€ë¹„
```

#### ğŸ”¥ Level 3: ì¬í•´ (Disaster)
```
â–¡ ì „ì²´ ì‹œìŠ¤í…œ ë‹¤ìš´
â–¡ ë°ì´í„° ì†ìƒ ì˜ì‹¬
â–¡ ë³´ì•ˆ ì¹¨í•´ ì˜ì‹¬
â–¡ ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì¥ì• 

ëŒ€ì‘ ì ˆì°¨:
1. ì „ì²´ íŒ€ ì†Œì§‘
2. ì‚¬ìš©ì ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ê³µì§€
3. ë°±ì—…ì—ì„œ ì™„ì „ ë³µêµ¬
4. ë³´ì•ˆ ì ê²€ ì‹¤ì‹œ
5. ê·¼ë³¸ ì›ì¸ ë¶„ì„
6. ì¬ë°œ ë°©ì§€ì±… ìˆ˜ë¦½
```

---

*ì´ ìš´ì˜ ë§¤ë‰´ì–¼ì€ StockPilot ì‹œìŠ¤í…œì˜ ì•ˆì •ì  ìš´ì˜ì„ ìœ„í•œ ì¢…í•© ê°€ì´ë“œì…ë‹ˆë‹¤. ì •ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê³  ì‹¤ì œ ìš´ì˜ í™˜ê²½ì— ë§ê²Œ ì¡°ì •í•˜ì—¬ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.*

**ğŸ“ ê¸´ê¸‰ ì—°ë½ì²˜:**
- ì‹œìŠ¤í…œ ê´€ë¦¬ì: admin@stockpilot.ai
- ê¸°ìˆ  ì§€ì›: support@stockpilot.ai  
- 24ì‹œê°„ í•«ë¼ì¸: 02-1234-5678