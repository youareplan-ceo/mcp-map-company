"""
í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
í•œêµ­ ì£¼ì‹/ê¸ˆìœµ í…ìŠ¤íŠ¸ì˜ ì „ì²˜ë¦¬, ì •ê·œí™”, ì¶”ì¶œ ê¸°ëŠ¥
"""

import re
import logging
from typing import List, Dict, Tuple, Optional, Any, Set
from dataclasses import dataclass
import unicodedata
from datetime import datetime
import json

logger = logging.getLogger(__name__)

@dataclass
class ProcessedText:
    """ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ê²°ê³¼"""
    original: str
    processed: str
    removed_chars: List[str]
    normalized_numbers: Dict[str, str]
    extracted_entities: Dict[str, List[str]]
    confidence_score: float

class KoreanTextProcessor:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        # í•œêµ­ ê¸ˆìœµ ê´€ë ¨ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ë“¤
        self.patterns = {
            # ìˆ«ì ë° ê¸ˆì•¡ íŒ¨í„´
            "amount": r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*([ì¡°ì–µë§Œì²œì›ë‹¬ëŸ¬])',
            "percentage": r'(\d+(?:\.\d+)?)\s*%',
            "stock_code": r'\b(\d{6})\b',
            "price": r'(\d+(?:,\d{3})*)\s*ì›',
            
            # ë‚ ì§œ íŒ¨í„´
            "date_kr": r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            "date_slash": r'(\d{4})[/\-.](\d{1,2})[/\-.](\d{1,2})',
            
            # íšŒì‚¬ëª… íŒ¨í„´
            "company_suffix": r'(ì£¼ì‹íšŒì‚¬|ìœ í•œíšŒì‚¬|\(ì£¼\)|\ãˆœ)',
            "company_name": r'([ê°€-í£A-Za-z0-9]+)(?:ì£¼ì‹íšŒì‚¬|ìœ í•œíšŒì‚¬|\(ì£¼\)|\ãˆœ)',
            
            # ê¸ˆìœµ ìš©ì–´ íŒ¨í„´
            "financial_metric": r'(ë§¤ì¶œì•¡?|ì˜ì—…ì´ìµ|ë‹¹ê¸°ìˆœì´ìµ|ë¶€ì±„ë¹„ìœ¨|ìœ ë™ë¹„ìœ¨|ROE|ROA|PER|PBR|EPS)',
            "trading_term": r'(ë§¤ìˆ˜|ë§¤ë„|ë³´ìœ |ìƒìŠ¹|í•˜ë½|ê¸‰ë“±|ê¸‰ë½|ëŒíŒŒ|ì§€ì§€|ì €í•­)',
            
            # íŠ¹ìˆ˜ ë¬¸ì ë° ì´ëª¨ì§€
            "emoji": r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U000026FF\U00002700-\U000027BF]',
            "special_chars": r'[^\w\sê°€-í£.,!?;:()\-\[\]{}"\'/]',
            
            # ë‰´ìŠ¤ ê´€ë ¨ íŒ¨í„´
            "news_source": r'\[(.*?)\]|\((.*?)\)$',
            "reporter": r'([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›|ì—ë””í„°)',
            "timestamp": r'(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})\s*(\d{1,2}):(\d{2})'
        }
        
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ (ê¸ˆìœµ ë¶„ì„ì—ì„œ ì œì™¸í•  ë‹¨ì–´ë“¤)
        self.stopwords = {
            "ì¡°ì‚¬": ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œ", "ì˜", "ì™€", "ê³¼", "ë„", "ë§Œ"],
            "ì–´ë¯¸": ["ë‹¤", "ë¼", "ì´ë‹¤", "í–ˆë‹¤", "í•œë‹¤", "ëœë‹¤", "ìˆë‹¤", "ì—†ë‹¤"],
            "ê¸°íƒ€": ["ê·¸", "ì´", "ì €", "ê²ƒ", "ìˆ˜", "ë•Œ", "ê³³", "ë“±", "ë°", "ë˜", "ë˜í•œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë”°ë¼ì„œ"]
        }
        
        # í•œêµ­ ê¸ˆìœµìš©ì–´ ì‚¬ì „
        self.financial_dictionary = self._load_financial_dictionary()
        
        # ìˆ«ì ë‹¨ìœ„ ë³€í™˜
        self.number_units = {
            "ì¡°": 1_000_000_000_000,
            "ì–µ": 100_000_000,
            "ë§Œ": 10_000,
            "ì²œ": 1_000
        }
    
    def _load_financial_dictionary(self) -> Dict[str, Dict]:
        """í•œêµ­ ê¸ˆìœµìš©ì–´ ì‚¬ì „ ë¡œë“œ"""
        return {
            # ì¬ë¬´ì œí‘œ ê´€ë ¨
            "revenue_terms": ["ë§¤ì¶œ", "ë§¤ì¶œì•¡", "ì´ë§¤ì¶œ", "ìˆœë§¤ì¶œ", "ì˜ì—…ìˆ˜ìµ"],
            "profit_terms": ["ì˜ì—…ì´ìµ", "ë‹¹ê¸°ìˆœì´ìµ", "ì„¸ì „ì´ìµ", "EBITDA", "ìˆœì´ìµ"],
            "debt_terms": ["ë¶€ì±„", "ì°¨ì…ê¸ˆ", "ì‚¬ì±„", "ë‹¨ê¸°ë¶€ì±„", "ì¥ê¸°ë¶€ì±„", "ë¶€ì±„ë¹„ìœ¨"],
            "asset_terms": ["ìì‚°", "ì´ìì‚°", "ìœ ë™ìì‚°", "ê³ ì •ìì‚°", "í˜„ê¸ˆì„±ìì‚°"],
            
            # ì£¼ì‹ ê´€ë ¨
            "stock_terms": ["ì£¼ê°€", "ì‹œê°€", "ê³ ê°€", "ì €ê°€", "ì¢…ê°€", "ê±°ë˜ëŸ‰", "ì‹œê°€ì´ì•¡"],
            "valuation_terms": ["PER", "PBR", "PSR", "EV/EBITDA", "ROE", "ROA", "ROIC"],
            "trading_terms": ["ë§¤ìˆ˜", "ë§¤ë„", "ë³´ìœ ", "ì†ì ˆ", "ìµì ˆ", "ë¬¼íƒ€ê¸°", "ë¶ˆíƒ€ê¸°"],
            
            # ì‹œì¥ ê´€ë ¨
            "market_terms": ["ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì½”ë„¥ìŠ¤", "ì¥ì¤‘", "ê°œì¥", "ë§ˆê°", "ë™ì‹œí˜¸ê°€"],
            "investor_terms": ["ê°œì¸", "ì™¸êµ­ì¸", "ê¸°ê´€", "ì—°ê¸°ê¸ˆ", "ë³´í—˜ì‚¬", "ì€í–‰", "ì¦ê¶Œì‚¬"],
            
            # ê²½ì œ ì§€í‘œ
            "economic_terms": ["GDP", "ë¬¼ê°€", "ê¸ˆë¦¬", "í™˜ìœ¨", "ë¬´ì—­ìˆ˜ì§€", "ê²½ìƒìˆ˜ì§€", "ì‹¤ì—…ë¥ "],
            "policy_terms": ["ê¸ˆí†µìœ„", "ê¸°ì¤€ê¸ˆë¦¬", "ì–‘ì ì™„í™”", "ê¸´ì¶•", "ë¶€ì–‘ì±…", "ê·œì œ", "ì™„í™”"]
        }
    
    def normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™” (ê¸°ë³¸ ì „ì²˜ë¦¬)"""
        if not text:
            return ""
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFC', text)
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µì¼ (ì¤„ë°”ê¿ˆ ì œì™¸)
        text = re.sub(r'[^\S\n]+', ' ', text)
        
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ í†µì¼
        text = re.sub(r'\n\s*\n', '\n', text)
        
        # ì¤„ ì‹œì‘ ë¶€ë¶„ì˜ ê³µë°± ì œê±°
        text = re.sub(r'\n\s+', '\n', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def clean_financial_text(self, text: str) -> ProcessedText:
        """ê¸ˆìœµ í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì²˜ë¦¬"""
        original = text
        processed = text
        removed_chars = []
        normalized_numbers = {}
        extracted_entities = {}
        
        try:
            # 1. ê¸°ë³¸ ì •ê·œí™”
            processed = self.normalize_text(processed)
            
            # 2. íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•„ìš”í•œ ê²ƒë§Œ ë³´ì¡´)
            special_chars = re.findall(self.patterns["special_chars"], processed)
            removed_chars.extend(special_chars)
            processed = re.sub(self.patterns["special_chars"], ' ', processed)
            
            # 3. ì´ëª¨ì§€ ì œê±°
            emojis = re.findall(self.patterns["emoji"], processed)
            removed_chars.extend(emojis)
            processed = re.sub(self.patterns["emoji"], '', processed)
            
            # 4. ìˆ«ì ë° ê¸ˆì•¡ ì •ê·œí™”
            amounts = re.findall(self.patterns["amount"], processed)
            for amount, unit in amounts:
                original_str = f"{amount}{unit}"
                normalized_value = self._normalize_korean_number(amount, unit)
                normalized_numbers[original_str] = str(normalized_value)
            
            # 5. ì—”í‹°í‹° ì¶”ì¶œ
            extracted_entities = self.extract_entities(processed)
            
            # 6. ì—°ì† ê³µë°± ì¬ì •ë¦¬
            processed = re.sub(r'\s+', ' ', processed).strip()
            
            # 7. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence_score = self._calculate_confidence(original, processed)
            
            return ProcessedText(
                original=original,
                processed=processed,
                removed_chars=removed_chars,
                normalized_numbers=normalized_numbers,
                extracted_entities=extracted_entities,
                confidence_score=confidence_score
            )
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ProcessedText(
                original=original,
                processed=original,
                removed_chars=[],
                normalized_numbers={},
                extracted_entities={},
                confidence_score=0.0
            )
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """ê¸ˆìœµ ê´€ë ¨ ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = {
            "stock_codes": [],
            "company_names": [],
            "amounts": [],
            "percentages": [],
            "dates": [],
            "financial_metrics": [],
            "trading_terms": [],
            "reporters": []
        }
        
        try:
            # ì¢…ëª©ì½”ë“œ ì¶”ì¶œ
            stock_codes = re.findall(self.patterns["stock_code"], text)
            entities["stock_codes"] = list(set(stock_codes))
            
            # íšŒì‚¬ëª… ì¶”ì¶œ
            company_matches = re.findall(self.patterns["company_name"], text)
            entities["company_names"] = list(set(company_matches))
            
            # ê¸ˆì•¡ ì¶”ì¶œ
            amounts = re.findall(self.patterns["amount"], text)
            entities["amounts"] = [f"{amount}{unit}" for amount, unit in amounts]
            
            # í¼ì„¼íŠ¸ ì¶”ì¶œ
            percentages = re.findall(self.patterns["percentage"], text)
            entities["percentages"] = [f"{p}%" for p in percentages]
            
            # ë‚ ì§œ ì¶”ì¶œ
            dates_kr = re.findall(self.patterns["date_kr"], text)
            dates_slash = re.findall(self.patterns["date_slash"], text)
            entities["dates"] = [f"{y}ë…„{m}ì›”{d}ì¼" for y, m, d in dates_kr] + \
                              [f"{y}-{m}-{d}" for y, m, d in dates_slash]
            
            # ê¸ˆìœµ ë©”íŠ¸ë¦­ ì¶”ì¶œ
            financial_metrics = re.findall(self.patterns["financial_metric"], text)
            entities["financial_metrics"] = list(set(financial_metrics))
            
            # ë§¤ë§¤ ìš©ì–´ ì¶”ì¶œ
            trading_terms = re.findall(self.patterns["trading_term"], text)
            entities["trading_terms"] = list(set(trading_terms))
            
            # ê¸°ìëª… ì¶”ì¶œ
            reporters = re.findall(self.patterns["reporter"], text)
            entities["reporters"] = [f"{name} {title}" for name, title in reporters]
            
        except Exception as e:
            logger.error(f"ì—”í‹°í‹° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return entities
    
    def _normalize_korean_number(self, number_str: str, unit: str) -> float:
        """í•œêµ­ì–´ ìˆ«ì ë‹¨ìœ„ ì •ê·œí™”"""
        try:
            # ì‰¼í‘œ ì œê±° í›„ ìˆ«ì ë³€í™˜
            number = float(number_str.replace(',', ''))
            
            # ë‹¨ìœ„ ì ìš©
            if unit in self.number_units:
                return number * self.number_units[unit]
            elif unit in ['ì›', 'ë‹¬ëŸ¬']:
                return number
            elif unit == '%':
                return number / 100
            else:
                return number
                
        except ValueError:
            return 0.0
    
    def _calculate_confidence(self, original: str, processed: str) -> float:
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not original:
            return 0.0
        
        # ê¸¸ì´ ë¹„ìœ¨
        length_ratio = len(processed) / len(original)
        
        # í•œê¸€ ë¬¸ì ë³´ì¡´ìœ¨
        korean_chars_orig = len(re.findall(r'[ê°€-í£]', original))
        korean_chars_proc = len(re.findall(r'[ê°€-í£]', processed))
        korean_preservation = korean_chars_proc / korean_chars_orig if korean_chars_orig > 0 else 1.0
        
        # ì „ì²´ ì‹ ë¢°ë„ (ê°€ì¤‘ í‰ê· )
        confidence = (length_ratio * 0.3 + korean_preservation * 0.7)
        
        return min(1.0, max(0.0, confidence))
    
    def extract_sentiment_keywords(self, text: str) -> Dict[str, List[str]]:
        """ê°ì • ë¶„ì„ìš© í‚¤ì›Œë“œ ì¶”ì¶œ"""
        from ..config.korean_market_config import korean_market_config
        
        sentiment_keywords = {
            "positive": [],
            "negative": [],
            "neutral": []
        }
        
        text_lower = text.lower()
        terms = korean_market_config.korean_financial_terms
        
        # ê¸ì • í‚¤ì›Œë“œ
        for keyword in terms["positive"]:
            if keyword in text_lower:
                sentiment_keywords["positive"].append(keyword)
        
        # ë¶€ì • í‚¤ì›Œë“œ
        for keyword in terms["negative"]:
            if keyword in text_lower:
                sentiment_keywords["negative"].append(keyword)
        
        # ì¤‘ë¦½ í‚¤ì›Œë“œ
        for keyword in terms["neutral"]:
            if keyword in text_lower:
                sentiment_keywords["neutral"].append(keyword)
        
        return sentiment_keywords
    
    def tokenize_korean_text(self, text: str, remove_stopwords: bool = True) -> List[str]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ í† í°í™” (ê°„ë‹¨í•œ êµ¬í˜„)"""
        if not text:
            return []
        
        # ê³µë°± ê¸°ì¤€ ë¶„ë¦¬
        tokens = text.split()
        
        # ë¶ˆìš©ì–´ ì œê±°
        if remove_stopwords:
            all_stopwords = []
            for category in self.stopwords.values():
                all_stopwords.extend(category)
            
            tokens = [token for token in tokens if token not in all_stopwords]
        
        # ë¹ˆ í† í° ì œê±°
        tokens = [token.strip() for token in tokens if token.strip()]
        
        return tokens
    
    def analyze_text_complexity(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ë³µì¡ë„ ë¶„ì„"""
        if not text:
            return {"complexity": "low", "score": 0.0}
        
        # ê¸°ë³¸ í†µê³„
        char_count = len(text)
        word_count = len(text.split())
        sentence_count = len(re.split(r'[.!?]', text))
        
        # í•œê¸€ ë¹„ìœ¨
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        korean_ratio = korean_chars / char_count if char_count > 0 else 0
        
        # ì „ë¬¸ìš©ì–´ ë¹„ìœ¨
        financial_terms_found = 0
        for category in self.financial_dictionary.values():
            for term in category:
                if term in text:
                    financial_terms_found += 1
        
        term_density = financial_terms_found / word_count if word_count > 0 else 0
        
        # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
        factors = {
            "length": min(1.0, char_count / 1000),  # 1000ì ê¸°ì¤€ ì •ê·œí™”
            "avg_word_length": (char_count / word_count) / 10 if word_count > 0 else 0,
            "sentence_length": (word_count / sentence_count) / 20 if sentence_count > 0 else 0,
            "term_density": min(1.0, term_density * 10),
            "korean_ratio": korean_ratio
        }
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë³µì¡ë„ ê³„ì‚°
        weights = {"length": 0.2, "avg_word_length": 0.2, "sentence_length": 0.2, 
                  "term_density": 0.3, "korean_ratio": 0.1}
        
        complexity_score = sum(factors[k] * weights[k] for k in factors.keys())
        
        # ë³µì¡ë„ ë¶„ë¥˜
        if complexity_score < 0.3:
            complexity = "low"
        elif complexity_score < 0.6:
            complexity = "medium"
        else:
            complexity = "high"
        
        return {
            "complexity": complexity,
            "score": complexity_score,
            "factors": factors,
            "stats": {
                "char_count": char_count,
                "word_count": word_count,
                "sentence_count": sentence_count,
                "korean_ratio": korean_ratio,
                "term_density": term_density
            }
        }
    
    def preprocess_for_embedding(self, text: str) -> str:
        """ì„ë² ë”©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ê¸°ë³¸ ì •ë¦¬
        processed_result = self.clean_financial_text(text)
        text = processed_result.processed
        
        # ì¶”ê°€ ì „ì²˜ë¦¬
        # 1. ì¤‘ë³µ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # 2. íŠ¹ìˆ˜ êµ¬ë‘ì  ì •ë¦¬
        text = re.sub(r'[.]{2,}', '.', text)  # ì—°ì† ë§ˆì¹¨í‘œ
        text = re.sub(r'[!]{2,}', '!', text)  # ì—°ì† ëŠë‚Œí‘œ
        text = re.sub(r'[?]{2,}', '?', text)  # ì—°ì† ë¬¼ìŒí‘œ
        
        # 3. ê´„í˜¸ ë‚´ìš© ì •ë¦¬ (í•„ìš”ì— ë”°ë¼)
        # text = re.sub(r'\([^)]*\)', '', text)  # ê´„í˜¸ ì œê±° (ì„ íƒì )
        
        # 4. ìµœì¢… ì •ë¦¬
        text = text.strip()
        
        return text
    
    def extract_key_phrases(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """í•µì‹¬ êµ¬ë¬¸ ì¶”ì¶œ (ê°„ë‹¨í•œ TF ê¸°ë°˜)"""
        if not text:
            return []
        
        # í† í°í™”
        tokens = self.tokenize_korean_text(text, remove_stopwords=True)
        
        # 2-gram ë° 3-gram ìƒì„±
        phrases = []
        phrases.extend(tokens)  # 1-gram
        
        # 2-gram
        for i in range(len(tokens) - 1):
            phrases.append(f"{tokens[i]} {tokens[i+1]}")
        
        # 3-gram
        for i in range(len(tokens) - 2):
            phrases.append(f"{tokens[i]} {tokens[i+1]} {tokens[i+2]}")
        
        # ë¹ˆë„ ê³„ì‚°
        phrase_counts = {}
        for phrase in phrases:
            phrase_counts[phrase] = phrase_counts.get(phrase, 0) + 1
        
        # ì ìˆ˜ ê³„ì‚° (ë¹ˆë„ + ê¸¸ì´ ë³´ì •)
        phrase_scores = []
        for phrase, count in phrase_counts.items():
            # ê¸¸ì´ ë³´ì • (ê¸´ êµ¬ë¬¸ì— ê°€ì¤‘ì¹˜)
            length_bonus = len(phrase.split()) * 0.1
            score = count + length_bonus
            phrase_scores.append((phrase, score))
        
        # ì ìˆ˜ ìˆœ ì •ë ¬
        phrase_scores.sort(key=lambda x: x[1], reverse=True)
        
        return phrase_scores[:top_k]

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
korean_text_processor = KoreanTextProcessor()

# í¸ì˜ í•¨ìˆ˜ë“¤
def clean_korean_text(text: str) -> ProcessedText:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ë¦¬"""
    return korean_text_processor.clean_financial_text(text)

def extract_financial_entities(text: str) -> Dict[str, List[str]]:
    """ê¸ˆìœµ ì—”í‹°í‹° ì¶”ì¶œ"""
    return korean_text_processor.extract_entities(text)

def preprocess_korean_for_ai(text: str) -> str:
    """AI ë¶„ì„ì„ ìœ„í•œ í•œêµ­ì–´ ì „ì²˜ë¦¬"""
    return korean_text_processor.preprocess_for_embedding(text)

def analyze_korean_complexity(text: str) -> Dict[str, Any]:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë³µì¡ë„ ë¶„ì„"""
    return korean_text_processor.analyze_text_complexity(text)

def extract_korean_keywords(text: str, top_k: int = 10) -> List[Tuple[str, float]]:
    """í•œêµ­ì–´ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    return korean_text_processor.extract_key_phrases(text, top_k)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    sample_text = """
    ì‚¼ì„±ì „ì(005930)ê°€ 2024ë…„ 1ë¶„ê¸° ì‹¤ì ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. 
    ë§¤ì¶œì•¡ì€ 60ì¡°ì›ìœ¼ë¡œ ì „ë…„ëŒ€ë¹„ 15% ì¦ê°€í–ˆìœ¼ë©°, ì˜ì—…ì´ìµì€ 8ì¡°5ì²œì–µì›ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.
    ë°˜ë„ì²´ ì‚¬ì—…ë¶€ë¬¸ì˜ í˜¸ì¡°ê°€ ì „ì²´ ì‹¤ì ì„ ê²¬ì¸í–ˆë‹¤ê³  ë¶„ì„ë©ë‹ˆë‹¤. ğŸ“ˆ
    ê¹€ì² ìˆ˜ ê¸°ì (ì„œìš¸=ì—°í•©ë‰´ìŠ¤) 2024ë…„ 4ì›” 30ì¼ ë°œí‘œ
    """
    
    print("=== í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜ˆì‹œ ===")
    
    # 1. í…ìŠ¤íŠ¸ ì •ë¦¬
    cleaned = clean_korean_text(sample_text)
    print(f"ì›ë³¸: {cleaned.original[:100]}...")
    print(f"ì •ë¦¬ë¨: {cleaned.processed[:100]}...")
    print(f"ì‹ ë¢°ë„: {cleaned.confidence_score:.2f}")
    
    # 2. ì—”í‹°í‹° ì¶”ì¶œ
    entities = extract_financial_entities(sample_text)
    print(f"\nì¶”ì¶œëœ ì—”í‹°í‹°:")
    for entity_type, values in entities.items():
        if values:
            print(f"  {entity_type}: {values}")
    
    # 3. ë³µì¡ë„ ë¶„ì„
    complexity = analyze_korean_complexity(sample_text)
    print(f"\në³µì¡ë„: {complexity['complexity']} (ì ìˆ˜: {complexity['score']:.2f})")
    
    # 4. í•µì‹¬ í‚¤ì›Œë“œ
    keywords = extract_korean_keywords(sample_text, top_k=5)
    print(f"\ní•µì‹¬ í‚¤ì›Œë“œ:")
    for keyword, score in keywords:
        print(f"  {keyword}: {score:.2f}")
    
    # 5. AI ì „ì²˜ë¦¬
    ai_ready = preprocess_korean_for_ai(sample_text)
    print(f"\nAI ì²˜ë¦¬ìš©: {ai_ready[:100]}...")