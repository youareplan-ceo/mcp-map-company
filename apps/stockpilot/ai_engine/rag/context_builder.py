"""
RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ëª¨ë“ˆ
ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ AI ëª¨ë¸ì´ ì‚¬ìš©í•  ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±
í† í° ê´€ë¦¬, ì»¨í…ìŠ¤íŠ¸ ì••ì¶•, ì‹œê°„ìˆœ ì •ë ¬, ë©”íƒ€ë°ì´í„° ì²¨ë¶€ ë“±ì˜ ê¸°ëŠ¥ í¬í•¨
"""

import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
import tiktoken
from collections import defaultdict
import json

from .retriever import SearchResult
from .indexer import IndexedDocument
from ..config.model_policy import ModelTier, model_policy

logger = logging.getLogger(__name__)


@dataclass
class ContextChunk:
    """ì»¨í…ìŠ¤íŠ¸ ì²­í¬ ì •ë³´"""
    content: str
    source_document: IndexedDocument
    relevance_score: float
    token_count: int
    chunk_type: str  # 'summary', 'detail', 'metadata'
    priority: int    # 1(ë†’ìŒ) ~ 5(ë‚®ìŒ)


@dataclass
class BuiltContext:
    """êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸"""
    formatted_context: str
    total_tokens: int
    source_documents: List[IndexedDocument]
    chunk_summary: Dict[str, Any]
    metadata: Dict[str, Any]
    truncated: bool = False


class ContextBuilder:
    """
    RAGìš© ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° ìµœì í™” í´ë˜ìŠ¤
    í† í° ì œí•œ, ê´€ë ¨ì„± ê¸°ë°˜ ìš°ì„ ìˆœìœ„, ì••ì¶• ë“±ì„ ì§€ì›
    """
    
    def __init__(self, model_name: str = "gpt-4"):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: í† í° ê³„ì‚°ì— ì‚¬ìš©í•  ëª¨ë¸ëª…
        """
        self.model_name = model_name
        
        try:
            self.tokenizer = tiktoken.encoding_for_model(model_name)
        except KeyError:
            # ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì˜ ê²½ìš° ê¸°ë³¸ ì¸ì½”ë”© ì‚¬ìš©
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
            logger.warning(f"ëª¨ë¸ '{model_name}' í† í¬ë‚˜ì´ì € ë¯¸ì§€ì›, ê¸°ë³¸ ì¸ì½”ë”© ì‚¬ìš©")
        
        # ë¬¸ì„œ íƒ€ì…ë³„ í…œí”Œë¦¿
        self.document_templates = {
            "news": """ğŸ“° **ë‰´ìŠ¤**: {title}
ğŸ“… **ë‚ ì§œ**: {date}
ğŸ“ˆ **ê°ì„±**: {sentiment}
ğŸ“ **ë‚´ìš©**: {content}
ğŸ”— **ì¶œì²˜**: {source}
---""",
            
            "price_data": """ğŸ“Š **ì£¼ê°€ ë°ì´í„°**: {symbol}
ğŸ“… **ë‚ ì§œ**: {date}  
ğŸ’° **ì¢…ê°€**: {close_price}
ğŸ“ˆ **ë³€ë™ë¥ **: {change_percent}%
ğŸ“Š **ê±°ë˜ëŸ‰**: {volume}
---""",
            
            "technical_analysis": """ğŸ“ˆ **ê¸°ìˆ ì  ë¶„ì„**: {symbol}
ğŸ“… **ë¶„ì„ì¼**: {date}
ğŸ¯ **ì‹ í˜¸**: {signal}
ğŸ“Š **ì§€í‘œ**: {indicators}
ğŸ“ **ë¶„ì„**: {content}
---""",
            
            "strategy": """ğŸ¯ **íˆ¬ì ì „ëµ**: {title}
ğŸ“… **ìƒì„±ì¼**: {date}
â­ **ë“±ê¸‰**: {rating}
ğŸ“ **ì „ëµ**: {content}
ğŸ¯ **ëª©í‘œ**: {target}
---""",
            
            "general": """ğŸ“„ **ë¬¸ì„œ**: {title}
ğŸ“… **ë‚ ì§œ**: {date}
ğŸ“ **ë‚´ìš©**: {content}
---"""
        }
        
        # ì••ì¶• ì„¤ì •
        self.compression_ratios = {
            "summary": 0.3,    # ìš”ì•½ìœ¼ë¡œ 30% ì••ì¶•
            "bullet_points": 0.5,  # ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ 50% ì••ì¶•
            "key_facts": 0.2   # í•µì‹¬ ì‚¬ì‹¤ë§Œ 20% ì••ì¶•
        }
    
    def count_tokens(self, text: str) -> int:
        """
        í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            int: í† í° ìˆ˜
        """
        try:
            return len(self.tokenizer.encode(text))
        except Exception as e:
            logger.warning(f"í† í° ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            # ëŒ€ëµì  ì¶”ì • (ì˜ì–´ ê¸°ì¤€)
            return len(text.split()) * 1.3
    
    def build_context(
        self,
        search_results: List[SearchResult],
        query: str,
        model_tier: ModelTier,
        max_tokens: Optional[int] = None,
        include_metadata: bool = True,
        compression_level: str = "none"  # none, summary, bullet_points, key_facts
    ) -> BuiltContext:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            query: ì›ë³¸ ì¿¼ë¦¬
            model_tier: ì‚¬ìš©í•  ëª¨ë¸ í‹°ì–´
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ ëª¨ë¸ ê¸°ë³¸ê°’ ì‚¬ìš©)
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€
            compression_level: ì••ì¶• ìˆ˜ì¤€
            
        Returns:
            BuiltContext: êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸
        """
        try:
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹œì‘: {len(search_results)}ê°œ ë¬¸ì„œ")
            
            # í† í° ì œí•œ ì„¤ì •
            if max_tokens is None:
                policy = model_policy.get_policy(model_tier)
                max_tokens = int(policy.max_tokens * 0.7)  # ì¶œë ¥ í† í° ì—¬ìœ ë¶„ í™•ë³´
            
            # ë¬¸ì„œë“¤ì„ ì²­í¬ë¡œ ë³€í™˜
            chunks = self._create_chunks(
                search_results, 
                compression_level,
                include_metadata
            )
            
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²­í¬ ì„ íƒ
            selected_chunks = self._select_chunks(chunks, max_tokens)
            
            # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            formatted_context = self._format_context(
                selected_chunks, 
                query,
                include_metadata
            )
            
            # ê²°ê³¼ ìƒì„±
            total_tokens = self.count_tokens(formatted_context)
            source_documents = [chunk.source_document for chunk in selected_chunks]
            
            # ì²­í¬ ìš”ì•½ ì •ë³´
            chunk_summary = self._create_chunk_summary(selected_chunks)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "query": query,
                "model_tier": model_tier.value,
                "max_tokens": max_tokens,
                "compression_level": compression_level,
                "total_documents": len(search_results),
                "selected_documents": len(selected_chunks),
                "truncated": len(selected_chunks) < len(chunks),
                "created_at": datetime.utcnow().isoformat()
            }
            
            result = BuiltContext(
                formatted_context=formatted_context,
                total_tokens=total_tokens,
                source_documents=source_documents,
                chunk_summary=chunk_summary,
                metadata=metadata,
                truncated=len(selected_chunks) < len(chunks)
            )
            
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {total_tokens} í† í°, "
                       f"{len(selected_chunks)}/{len(chunks)} ì²­í¬ ì„ íƒ")
            
            return result
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _create_chunks(
        self,
        search_results: List[SearchResult],
        compression_level: str,
        include_metadata: bool
    ) -> List[ContextChunk]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë³€í™˜
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼
            compression_level: ì••ì¶• ìˆ˜ì¤€
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€
            
        Returns:
            List[ContextChunk]: ì»¨í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        
        for result in search_results:
            doc = result.document
            
            # ë‚´ìš© ì²˜ë¦¬ (ì••ì¶• ì ìš©)
            if compression_level != "none":
                content = self._compress_content(doc.content, compression_level)
            else:
                content = doc.content
            
            # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ í¬ë§·íŒ…
            formatted_content = self._format_document(doc, content)
            
            # í† í° ìˆ˜ ê³„ì‚°
            token_count = self.count_tokens(formatted_content)
            
            # ìš°ì„ ìˆœìœ„ ê³„ì‚° (ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜)
            priority = self._calculate_priority(result)
            
            chunk = ContextChunk(
                content=formatted_content,
                source_document=doc,
                relevance_score=result.final_score,
                token_count=token_count,
                chunk_type="content",
                priority=priority
            )
            
            chunks.append(chunk)
            
            # ë©”íƒ€ë°ì´í„° ì²­í¬ ì¶”ê°€ (ì„ íƒì )
            if include_metadata and doc.metadata:
                metadata_content = self._format_metadata(doc.metadata)
                if metadata_content:
                    metadata_chunk = ContextChunk(
                        content=metadata_content,
                        source_document=doc,
                        relevance_score=result.final_score * 0.5,  # ë©”íƒ€ë°ì´í„°ëŠ” ë‚®ì€ ê´€ë ¨ì„±
                        token_count=self.count_tokens(metadata_content),
                        chunk_type="metadata",
                        priority=priority + 1  # ë‚®ì€ ìš°ì„ ìˆœìœ„
                    )
                    chunks.append(metadata_chunk)
        
        # ìš°ì„ ìˆœìœ„ì™€ ê´€ë ¨ì„±ìœ¼ë¡œ ì •ë ¬
        chunks.sort(key=lambda x: (x.priority, -x.relevance_score))
        
        return chunks
    
    def _compress_content(self, content: str, compression_level: str) -> str:
        """
        ë‚´ìš© ì••ì¶•
        
        Args:
            content: ì›ë³¸ ë‚´ìš©
            compression_level: ì••ì¶• ìˆ˜ì¤€
            
        Returns:
            str: ì••ì¶•ëœ ë‚´ìš©
        """
        try:
            if compression_level == "summary":
                # ì²« ë¬¸ì¥ + ì¤‘ìš” ë¬¸ì¥ë“¤
                sentences = content.split('.')
                if len(sentences) <= 3:
                    return content
                
                # ì²« ë¬¸ì¥ê³¼ ë§ˆì§€ë§‰ ë¬¸ì¥, ê·¸ë¦¬ê³  ê°€ì¥ ê¸´ ë¬¸ì¥ ëª‡ ê°œ ì„ íƒ
                first_sentence = sentences[0] + '.'
                last_sentence = sentences[-1] if sentences[-1].strip() else sentences[-2] + '.'
                
                # ì¤‘ê°„ ë¬¸ì¥ë“¤ì„ ê¸¸ì´ìˆœ ì •ë ¬í•´ì„œ ìƒìœ„ ëª‡ ê°œ ì„ íƒ
                middle_sentences = sentences[1:-1]
                middle_sentences.sort(key=len, reverse=True)
                selected_middle = middle_sentences[:2]
                
                return f"{first_sentence} {' '.join(selected_middle)} {last_sentence}"
                
            elif compression_level == "bullet_points":
                # ë¶ˆë¦¿ í¬ì¸íŠ¸ í˜•ì‹ìœ¼ë¡œ ì••ì¶•
                sentences = [s.strip() for s in content.split('.') if s.strip()]
                if len(sentences) <= 3:
                    return content
                
                bullet_points = []
                for sentence in sentences[:5]:  # ìµœëŒ€ 5ê°œ í¬ì¸íŠ¸
                    if len(sentence) > 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                        bullet_points.append(f"â€¢ {sentence}")
                
                return '\n'.join(bullet_points)
                
            elif compression_level == "key_facts":
                # í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
                sentences = content.split('.')
                key_info = []
                
                keywords = ['ìƒìŠ¹', 'í•˜ë½', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ëª©í‘œ', 'ì „ë§', 'ë¶„ì„', 'ì˜ˆìƒ']
                
                for sentence in sentences:
                    if any(keyword in sentence for keyword in keywords):
                        key_info.append(sentence.strip())
                
                if not key_info:
                    # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì²« ë¬¸ì¥
                    key_info = [sentences[0]]
                
                return '. '.join(key_info[:3])  # ìµœëŒ€ 3ê°œ ì‚¬ì‹¤
                
            else:
                return content
                
        except Exception as e:
            logger.warning(f"ë‚´ìš© ì••ì¶• ì‹¤íŒ¨: {str(e)}")
            return content
    
    def _format_document(self, doc: IndexedDocument, content: str) -> str:
        """
        ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ í¬ë§·íŒ…
        
        Args:
            doc: ë¬¸ì„œ ê°ì²´
            content: ë¬¸ì„œ ë‚´ìš©
            
        Returns:
            str: í¬ë§·íŒ…ëœ ë‚´ìš©
        """
        try:
            template = self.document_templates.get(
                doc.document_type, 
                self.document_templates["general"]
            )
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
            metadata = doc.metadata or {}
            
            format_params = {
                "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                "date": doc.created_at.strftime("%Y-%m-%d %H:%M"),
                "content": content,
                "source": metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
            }
            
            # ë¬¸ì„œ íƒ€ì…ë³„ íŠ¹ë³„ ì²˜ë¦¬
            if doc.document_type == "news":
                sentiment_score = metadata.get("sentiment_score", 0)
                if sentiment_score > 0.1:
                    sentiment = "ê¸ì •ì "
                elif sentiment_score < -0.1:
                    sentiment = "ë¶€ì •ì "
                else:
                    sentiment = "ì¤‘ë¦½ì "
                
                format_params.update({
                    "sentiment": sentiment,
                    "title": metadata.get("title", content[:50] + "...")
                })
                
            elif doc.document_type == "price_data":
                format_params.update({
                    "symbol": metadata.get("symbol", "N/A"),
                    "close_price": metadata.get("close_price", "N/A"),
                    "change_percent": metadata.get("change_percent", "N/A"),
                    "volume": metadata.get("volume", "N/A")
                })
                
            elif doc.document_type == "technical_analysis":
                format_params.update({
                    "symbol": metadata.get("symbol", "N/A"),
                    "signal": metadata.get("signal", "ì¤‘ë¦½"),
                    "indicators": metadata.get("indicators", "N/A")
                })
                
            elif doc.document_type == "strategy":
                format_params.update({
                    "rating": metadata.get("rating", "ë“±ê¸‰ ì—†ìŒ"),
                    "target": metadata.get("target", "ëª©í‘œ ì—†ìŒ")
                })
            
            return template.format(**format_params)
            
        except Exception as e:
            logger.warning(f"ë¬¸ì„œ í¬ë§·íŒ… ì‹¤íŒ¨: {str(e)}")
            return f"ğŸ“„ **ë¬¸ì„œ**: {content}"
    
    def _format_metadata(self, metadata: Dict[str, Any]) -> str:
        """
        ë©”íƒ€ë°ì´í„° í¬ë§·íŒ…
        
        Args:
            metadata: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            str: í¬ë§·íŒ…ëœ ë©”íƒ€ë°ì´í„°
        """
        try:
            important_fields = [
                "symbol", "sector", "market_cap", "pe_ratio", 
                "dividend_yield", "52_week_high", "52_week_low"
            ]
            
            formatted_items = []
            for field in important_fields:
                if field in metadata:
                    value = metadata[field]
                    formatted_items.append(f"**{field.replace('_', ' ').title()}**: {value}")
            
            if formatted_items:
                return "ğŸ·ï¸ **ë©”íƒ€ë°ì´í„°**:\n" + "\n".join(formatted_items) + "\n---"
            
            return ""
            
        except Exception as e:
            logger.warning(f"ë©”íƒ€ë°ì´í„° í¬ë§·íŒ… ì‹¤íŒ¨: {str(e)}")
            return ""
    
    def _calculate_priority(self, search_result: SearchResult) -> int:
        """
        ê²€ìƒ‰ ê²°ê³¼ì˜ ìš°ì„ ìˆœìœ„ ê³„ì‚°
        
        Args:
            search_result: ê²€ìƒ‰ ê²°ê³¼
            
        Returns:
            int: ìš°ì„ ìˆœìœ„ (1ì´ ê°€ì¥ ë†’ìŒ)
        """
        try:
            score = search_result.final_score
            doc_type = search_result.document.document_type
            
            # ë¬¸ì„œ íƒ€ì…ë³„ ê¸°ë³¸ ìš°ì„ ìˆœìœ„
            type_priorities = {
                "strategy": 1,
                "technical_analysis": 2, 
                "news": 3,
                "price_data": 4,
                "general": 5
            }
            
            base_priority = type_priorities.get(doc_type, 5)
            
            # ì ìˆ˜ì— ë”°ë¥¸ ì¡°ì •
            if score > 0.8:
                base_priority = max(1, base_priority - 1)
            elif score < 0.6:
                base_priority = min(5, base_priority + 1)
            
            return base_priority
            
        except Exception as e:
            logger.warning(f"ìš°ì„ ìˆœìœ„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 5
    
    def _select_chunks(
        self, 
        chunks: List[ContextChunk], 
        max_tokens: int
    ) -> List[ContextChunk]:
        """
        í† í° ì œí•œ ë‚´ì—ì„œ ìµœì ì˜ ì²­í¬ ì„ íƒ
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            List[ContextChunk]: ì„ íƒëœ ì²­í¬ë“¤
        """
        selected_chunks = []
        total_tokens = 0
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í† í° ì˜ˆì•½ (ì•½ 200í† í°)
        reserved_tokens = 200
        available_tokens = max_tokens - reserved_tokens
        
        for chunk in chunks:
            if total_tokens + chunk.token_count <= available_tokens:
                selected_chunks.append(chunk)
                total_tokens += chunk.token_count
            else:
                # í† í° ì´ˆê³¼ì‹œ ì¤‘ìš”ë„ê°€ ë†’ì€ ì²­í¬ë¼ë©´ ì¼ë¶€ë§Œ í¬í•¨
                if chunk.priority <= 2:  # ë†’ì€ ìš°ì„ ìˆœìœ„
                    remaining_tokens = available_tokens - total_tokens
                    if remaining_tokens > 50:  # ìµœì†Œí•œì˜ ì˜ë¯¸ìˆëŠ” ë‚´ìš©
                        # ì²­í¬ ë‚´ìš© ì¼ë¶€ í¬í•¨
                        truncated_content = self._truncate_chunk(
                            chunk, 
                            remaining_tokens
                        )
                        if truncated_content:
                            selected_chunks.append(truncated_content)
                            total_tokens = available_tokens
                break
        
        return selected_chunks
    
    def _truncate_chunk(
        self, 
        chunk: ContextChunk, 
        max_tokens: int
    ) -> Optional[ContextChunk]:
        """
        ì²­í¬ ë‚´ìš©ì„ í† í° ì œí•œì— ë§ê²Œ ì˜ë¼ë‚´ê¸°
        
        Args:
            chunk: ì›ë³¸ ì²­í¬
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            Optional[ContextChunk]: ì˜ë¼ë‚¸ ì²­í¬ (ë˜ëŠ” None)
        """
        try:
            content_lines = chunk.content.split('\n')
            truncated_lines = []
            current_tokens = 0
            
            for line in content_lines:
                line_tokens = self.count_tokens(line + '\n')
                if current_tokens + line_tokens <= max_tokens:
                    truncated_lines.append(line)
                    current_tokens += line_tokens
                else:
                    break
            
            if truncated_lines:
                truncated_content = '\n'.join(truncated_lines)
                if len(truncated_lines) < len(content_lines):
                    truncated_content += "\n... (ë‚´ìš© ìƒëµ)"
                
                return ContextChunk(
                    content=truncated_content,
                    source_document=chunk.source_document,
                    relevance_score=chunk.relevance_score,
                    token_count=current_tokens,
                    chunk_type=chunk.chunk_type + "_truncated",
                    priority=chunk.priority
                )
            
            return None
            
        except Exception as e:
            logger.warning(f"ì²­í¬ ìë¥´ê¸° ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _format_context(
        self,
        chunks: List[ContextChunk],
        query: str,
        include_metadata: bool
    ) -> str:
        """
        ìµœì¢… ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        
        Args:
            chunks: ì„ íƒëœ ì²­í¬ë“¤
            query: ì›ë³¸ ì¿¼ë¦¬
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€
            
        Returns:
            str: í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸
        """
        try:
            context_parts = []
            
            # í—¤ë”
            context_parts.append("# ğŸ“Š StockPilot AI - ì»¨í…ìŠ¤íŠ¸ ì •ë³´")
            context_parts.append(f"**ì§ˆë¬¸**: {query}")
            context_parts.append(f"**ë¶„ì„ ì‹œê°**: {datetime.utcnow().strftime('%Y-%m-%d %H:%M')} (UTC)")
            context_parts.append("")
            
            # ë¬¸ì„œë“¤ì„ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
            chunks_by_type = defaultdict(list)
            for chunk in chunks:
                if chunk.chunk_type != "metadata":  # ë©”íƒ€ë°ì´í„°ëŠ” ë³„ë„ ì²˜ë¦¬
                    chunks_by_type[chunk.source_document.document_type].append(chunk)
            
            # ë¬¸ì„œ íƒ€ì…ë³„ ì„¹ì…˜
            type_headers = {
                "strategy": "## ğŸ¯ íˆ¬ì ì „ëµ",
                "technical_analysis": "## ğŸ“ˆ ê¸°ìˆ ì  ë¶„ì„", 
                "news": "## ğŸ“° ê´€ë ¨ ë‰´ìŠ¤",
                "price_data": "## ğŸ’° ì£¼ê°€ ë°ì´í„°",
                "general": "## ğŸ“„ ì¼ë°˜ ë¬¸ì„œ"
            }
            
            for doc_type in ["strategy", "technical_analysis", "news", "price_data", "general"]:
                type_chunks = chunks_by_type.get(doc_type, [])
                if type_chunks:
                    context_parts.append(type_headers.get(doc_type, f"## {doc_type}"))
                    context_parts.append("")
                    
                    for chunk in type_chunks:
                        context_parts.append(chunk.content)
                        context_parts.append("")
            
            # ë©”íƒ€ë°ì´í„° ì„¹ì…˜ (ì„ íƒì )
            if include_metadata:
                metadata_chunks = [c for c in chunks if c.chunk_type == "metadata"]
                if metadata_chunks:
                    context_parts.append("## ğŸ·ï¸ ì¶”ê°€ ì •ë³´")
                    context_parts.append("")
                    for chunk in metadata_chunks:
                        context_parts.append(chunk.content)
                        context_parts.append("")
            
            # í‘¸í„°
            context_parts.append("---")
            context_parts.append("ğŸ’¡ **ì°¸ê³ **: ìœ„ ì •ë³´ëŠ” ì‹¤ì‹œê°„ ë°ì´í„° ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            context_parts.append("íˆ¬ì ê²°ì •ì‹œ ë°˜ë“œì‹œ ì¶”ê°€ì ì¸ ê²€í† ì™€ ì „ë¬¸ê°€ ìƒë‹´ì„ ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤.")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… ì‹¤íŒ¨: {str(e)}")
            # ê°„ë‹¨í•œ í´ë°± í¬ë§·
            simple_context = f"ì§ˆë¬¸: {query}\n\n"
            for chunk in chunks:
                simple_context += chunk.content + "\n\n"
            return simple_context
    
    def _create_chunk_summary(self, chunks: List[ContextChunk]) -> Dict[str, Any]:
        """
        ì²­í¬ ìš”ì•½ ì •ë³´ ìƒì„±
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ìš”ì•½ ì •ë³´
        """
        try:
            summary = {
                "total_chunks": len(chunks),
                "total_tokens": sum(c.token_count for c in chunks),
                "chunk_types": {},
                "document_types": {},
                "priority_distribution": {},
                "relevance_stats": {
                    "max": max(c.relevance_score for c in chunks) if chunks else 0,
                    "min": min(c.relevance_score for c in chunks) if chunks else 0,
                    "avg": sum(c.relevance_score for c in chunks) / len(chunks) if chunks else 0
                }
            }
            
            # íƒ€ì…ë³„ í†µê³„
            for chunk in chunks:
                chunk_type = chunk.chunk_type
                doc_type = chunk.source_document.document_type
                priority = chunk.priority
                
                summary["chunk_types"][chunk_type] = summary["chunk_types"].get(chunk_type, 0) + 1
                summary["document_types"][doc_type] = summary["document_types"].get(doc_type, 0) + 1
                summary["priority_distribution"][priority] = summary["priority_distribution"].get(priority, 0) + 1
            
            return summary
            
        except Exception as e:
            logger.warning(f"ì²­í¬ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {"total_chunks": len(chunks), "error": str(e)}
    
    def optimize_context_for_model(
        self,
        context: BuiltContext,
        target_model: ModelTier
    ) -> BuiltContext:
        """
        íŠ¹ì • ëª¨ë¸ì— ë§ê²Œ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”
        
        Args:
            context: ì›ë³¸ ì»¨í…ìŠ¤íŠ¸
            target_model: ëŒ€ìƒ ëª¨ë¸
            
        Returns:
            BuiltContext: ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸
        """
        try:
            policy = model_policy.get_policy(target_model)
            max_tokens = int(policy.max_tokens * 0.7)
            
            # ì´ë¯¸ í† í° ì œí•œ ë‚´ë¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if context.total_tokens <= max_tokens:
                return context
            
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì‹œì‘: {context.total_tokens} -> {max_tokens} í† í°")
            
            # ë¬¸ì„œë“¤ì„ ë‹¤ì‹œ ì²­í¬ë¡œ ë³€í™˜
            search_results = []
            for doc in context.source_documents:
                # ê°€ìƒì˜ ê²€ìƒ‰ ê²°ê³¼ ìƒì„± (ì ìˆ˜ëŠ” ë¬¸ì„œ ìˆœì„œ ê¸°ë°˜)
                from ..rag.retriever import SearchResult
                result = SearchResult(
                    document=doc,
                    relevance_score=0.8,  # ê¸°ë³¸ ì ìˆ˜
                    time_score=0.5,
                    final_score=0.7,
                    rank=len(search_results) + 1,
                    retrieval_reason="context_optimization",
                    matched_keywords=[]
                )
                search_results.append(result)
            
            # ë” ê°•í•œ ì••ì¶•ìœ¼ë¡œ ì¬êµ¬ì„±
            compression_level = "summary" if context.metadata.get("compression_level") == "none" else "key_facts"
            
            optimized_context = self.build_context(
                search_results=search_results,
                query=context.metadata.get("query", ""),
                model_tier=target_model,
                max_tokens=max_tokens,
                include_metadata=False,  # ë©”íƒ€ë°ì´í„° ì œì™¸ë¡œ í† í° ì ˆì•½
                compression_level=compression_level
            )
            
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì™„ë£Œ: {optimized_context.total_tokens} í† í°")
            return optimized_context
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì‹¤íŒ¨: {str(e)}")
            return context  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜