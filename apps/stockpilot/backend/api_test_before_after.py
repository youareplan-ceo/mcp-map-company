#!/usr/bin/env python3
"""
API ν…μ¤νΈ μ „ν›„ λΉ„κµ μλ™ν™”
"""

import asyncio
import aiohttp
import json
from datetime import datetime
from pathlib import Path

class APITestBeforeAfter:
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.test_results = []
        
    async def test_api_endpoints(self):
        """API μ—”λ“ν¬μΈνΈ ν…μ¤νΈ μ‹¤ν–‰"""
        print("π” API μ—”λ“ν¬μΈνΈ ν…μ¤νΈ μ‹¤ν–‰ μ¤‘...")
        
        test_endpoints = [
            {"name": "Backend Health Check", "url": "/health", "method": "GET"},
            {"name": "Stock Search API", "url": "/api/v1/stocks/search?q=μ‚Όμ„±", "method": "GET"},
            {"name": "Portfolio API", "url": "/api/v1/portfolio/overview", "method": "GET"},
            {"name": "Alerts API", "url": "/api/v1/alerts/list", "method": "GET"},
            {"name": "Dashboard API", "url": "/api/v1/dashboard/widgets", "method": "GET"}
        ]
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                for endpoint in test_endpoints:
                    try:
                        url = f"{self.base_url}{endpoint['url']}"
                        start_time = datetime.now()
                        
                        async with session.get(url) as response:
                            end_time = datetime.now()
                            response_time = (end_time - start_time).total_seconds() * 1000
                            
                            status_code = response.status
                            success = status_code == 200
                            
                            try:
                                response_data = await response.json()
                            except:
                                response_data = {"error": "Invalid JSON response"}
                            
                            self.test_results.append({
                                "endpoint": endpoint["name"],
                                "url": endpoint["url"],
                                "method": endpoint["method"],
                                "status_code": status_code,
                                "success": success,
                                "response_time_ms": round(response_time, 2),
                                "response_size": len(str(response_data)),
                                "has_data": len(response_data) > 0,
                                "timestamp": datetime.now().isoformat()
                            })
                            
                    except Exception as e:
                        self.test_results.append({
                            "endpoint": endpoint["name"],
                            "url": endpoint["url"],
                            "method": endpoint["method"],
                            "status_code": 0,
                            "success": False,
                            "error": str(e),
                            "response_time_ms": 0,
                            "timestamp": datetime.now().isoformat()
                        })
                        
        except Exception as e:
            print(f"β ν…μ¤νΈ μ‹¤ν–‰ μ¤λ¥: {str(e)}")
            
    def analyze_results(self):
        """ν…μ¤νΈ κ²°κ³Ό λ¶„μ„"""
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results if result.get('success', False))
        success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # μ‘λ‹µ μ‹κ°„ ν†µκ³„
        response_times = [r.get('response_time_ms', 0) for r in self.test_results if r.get('success', False)]
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        
        return {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "failed_tests": total_tests - successful_tests,
            "success_rate": round(success_rate, 1),
            "performance": {
                "avg_response_time_ms": round(avg_response_time, 2),
                "max_response_time_ms": round(max_response_time, 2),
                "min_response_time_ms": round(min_response_time, 2)
            }
        }
        
    def generate_before_after_report(self):
        """μ „ν›„ λΉ„κµ λ¦¬ν¬νΈ μƒμ„±"""
        print("π“ API ν…μ¤νΈ μ „ν›„ λΉ„κµ λ¦¬ν¬νΈ μƒμ„± μ¤‘...")
        
        # λ¶„μ„ κ²°κ³Ό
        analysis = self.analyze_results()
        
        # BEFORE (κΈ°μ΅΄ μƒνƒ) - κ²€μ¦ λ¦¬ν¬νΈ κΈ°μ¤€
        before_data = {
            "total_tests": 5,
            "successful_tests": 3,
            "failed_tests": 2,
            "success_rate": 60.0,
            "failed_endpoints": [
                "Portfolio API (/api/v1/portfolio/overview)",
                "Alerts API (/api/v1/alerts/list)"
            ],
            "issues": [
                "404 Not Found errors on portfolio and alerts endpoints",
                "Missing route definitions",
                "Inconsistent URL patterns"
            ]
        }
        
        # AFTER (ν„μ¬ μƒνƒ)
        after_data = analysis.copy()
        after_data["improvements"] = []
        
        if after_data["success_rate"] > before_data["success_rate"]:
            after_data["improvements"].append("β… API μ„±κ³µλ¥  ν–¥μƒ")
        if after_data["failed_tests"] < before_data["failed_tests"]:
            after_data["improvements"].append("β… μ‹¤ν¨ν• μ—”λ“ν¬μΈνΈ μ κ°μ†")
        if after_data["success_rate"] >= 100:
            after_data["improvements"].append("β… 100% API μ„±κ³µλ¥  λ‹¬μ„±")
            
        # λΉ„κµ λ¦¬ν¬νΈ
        comparison_report = {
            "timestamp": datetime.now().isoformat(),
            "test_summary": {
                "before": before_data,
                "after": after_data
            },
            "improvements": {
                "success_rate_change": after_data["success_rate"] - before_data["success_rate"],
                "failed_tests_reduced": before_data["failed_tests"] - after_data["failed_tests"],
                "fixed_endpoints": []
            },
            "detailed_results": self.test_results,
            "recommendations": []
        }
        
        # μμ •λ μ—”λ“ν¬μΈνΈ μ‹λ³„
        for endpoint in before_data.get("failed_endpoints", []):
            endpoint_name = endpoint.split(" (")[0]
            current_result = next((r for r in self.test_results if endpoint_name in r.get("endpoint", "")), None)
            if current_result and current_result.get("success", False):
                comparison_report["improvements"]["fixed_endpoints"].append(endpoint_name)
        
        # κ¶μ¥μ‚¬ν•­
        if after_data["success_rate"] < 100:
            comparison_report["recommendations"].append("π”§ λ‚¨μ€ μ‹¤ν¨ μ—”λ“ν¬μΈνΈ μμ • ν•„μ”")
        if after_data.get("performance", {}).get("avg_response_time_ms", 0) > 500:
            comparison_report["recommendations"].append("β΅ API μ‘λ‹µ μ‹κ°„ μµμ ν™” κ¶μ¥")
        if after_data["success_rate"] >= 100:
            comparison_report["recommendations"].append("β… λ¨λ“  API μ—”λ“ν¬μΈνΈκ°€ μ •μƒ μ‘λ™ν•©λ‹λ‹¤")
            
        return comparison_report
    
    async def run_test_and_generate_report(self):
        """ν…μ¤νΈ μ‹¤ν–‰ λ° λ¦¬ν¬νΈ μƒμ„±"""
        print("π€ API ν…μ¤νΈ μ „ν›„ λΉ„κµ μ‹μ‘")
        print("="*60)
        
        # 1. API ν…μ¤νΈ μ‹¤ν–‰
        await self.test_api_endpoints()
        
        # 2. λ¦¬ν¬νΈ μƒμ„±
        report = self.generate_before_after_report()
        
        # 3. νμΌ μ €μ¥
        with open("api_test_before_after_results.json", 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # 4. λ§ν¬λ‹¤μ΄ λ¦¬ν¬νΈ μƒμ„±
        await self.generate_markdown_report(report)
        
        print("="*60)
        print("β… API ν…μ¤νΈ μ „ν›„ λΉ„κµ μ™„λ£!")
        return report
        
    async def generate_markdown_report(self, report):
        """λ§ν¬λ‹¤μ΄ λ¦¬ν¬νΈ μƒμ„±"""
        before = report["test_summary"]["before"]
        after = report["test_summary"]["after"]
        
        markdown_content = f"""# API Test Before/After Comparison Report

Generated: {report["timestamp"]}

## π“ Summary

### Before (Initial State)
- **Total Tests**: {before["total_tests"]}
- **Successful**: {before["successful_tests"]}
- **Failed**: {before["failed_tests"]}
- **Success Rate**: {before["success_rate"]}%

### After (Current State)  
- **Total Tests**: {after["total_tests"]}
- **Successful**: {after["successful_tests"]}
- **Failed**: {after["failed_tests"]}
- **Success Rate**: {after["success_rate"]}%

## π”„ Improvements

- **Success Rate Change**: {report["improvements"]["success_rate_change"]:+.1f}%
- **Failed Tests Reduced**: {report["improvements"]["failed_tests_reduced"]}
- **Fixed Endpoints**: {len(report["improvements"]["fixed_endpoints"])}

## β… Fixed Endpoints

"""
        
        for endpoint in report["improvements"]["fixed_endpoints"]:
            markdown_content += f"- {endpoint}\n"
            
        markdown_content += f"""

## π“ Performance Metrics

"""
        
        if "performance" in after:
            perf = after["performance"]
            markdown_content += f"""- **Average Response Time**: {perf["avg_response_time_ms"]:.2f}ms
- **Max Response Time**: {perf["max_response_time_ms"]:.2f}ms  
- **Min Response Time**: {perf["min_response_time_ms"]:.2f}ms
"""
        
        markdown_content += f"""

## π” Detailed Test Results

| Endpoint | Status | Response Time | Success |
|----------|--------|---------------|---------|
"""
        
        for result in report["detailed_results"]:
            status_icon = "β…" if result.get("success", False) else "β"
            response_time = result.get("response_time_ms", 0)
            markdown_content += f"| {result['endpoint']} | {result.get('status_code', 'N/A')} | {response_time:.2f}ms | {status_icon} |\n"
        
        markdown_content += f"""

## π’΅ Recommendations

"""
        
        for rec in report.get("recommendations", []):
            markdown_content += f"- {rec}\n"
        
        # νμΌ μ €μ¥
        with open("API_TEST_BEFORE_AFTER.md", 'w', encoding='utf-8') as f:
            f.write(markdown_content)

async def main():
    tester = APITestBeforeAfter()
    report = await tester.run_test_and_generate_report()
    
    print(f"\nπ‰ API ν…μ¤νΈ μ „ν›„ λΉ„κµ μ™„λ£!")
    print(f"π“ μ„±κ³µλ¥ : {report['test_summary']['before']['success_rate']}% β†’ {report['test_summary']['after']['success_rate']}%")
    print(f"π”§ μμ •λ μ—”λ“ν¬μΈνΈ: {len(report['improvements']['fixed_endpoints'])}κ°")
    print(f"π“ μƒμ„Έ λ¦¬ν¬νΈ:")
    print(f"   - JSON: api_test_before_after_results.json")
    print(f"   - Markdown: API_TEST_BEFORE_AFTER.md")

if __name__ == "__main__":
    asyncio.run(main())