#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ” ì´ìƒíƒì§€ ë° ì˜ˆì¸¡ ë¦¬í¬íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (í•œêµ­ì–´ ì£¼ì„ í¬í•¨)

ëª©ì : ë©”íŠ¸ë¦­ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê³  í–¥í›„ 7ì¼ê°„ì˜ ìœ„í—˜ êµ¬ê°„ì„ ì˜ˆì¸¡
ì…ë ¥: reports/metrics/*.json, logs/*.log, reports/ci_reports/*.json
ì¶œë ¥: Markdown/JSON í˜•ì‹ì˜ ì¢…í•© ë¦¬í¬íŠ¸

ì£¼ìš” ê¸°ëŠ¥:
- ë°ì´í„° ì „ì²˜ë¦¬: ëˆ„ë½ê°’/ì´ìƒê°’ ì²˜ë¦¬, ì´ë™í‰ê· (EWMA) ìŠ¤ë¬´ë”©
- ì´ìƒíƒì§€: ë¡¤ë§ ìœˆë„ìš° ê¸°ë°˜ Z-score ë¶„ì„ (ì„ê³„ì¹˜ ê¸°ë³¸ 3.0)
- ì˜ˆì¸¡: ì„ í˜•íšŒê·€ ì¶”ì„¸ ë¶„ì„ + EWMAë¥¼ í†µí•œ 7ì¼ ì „ë§ì¹˜ ìƒì„±
- ìœ„í—˜ë„ í‰ê°€: ì§€í‘œë³„ ìœ„í—˜ ë“±ê¸‰ ë° í–¥í›„ ì˜ˆìƒ ì´ìƒì¹˜ êµ¬ê°„ ì œê³µ

ì‘ì„±ì: MCP Map Company ìš´ì˜íŒ€
ìƒì„±ì¼: 2024ë…„ ì—°ê°„ ìš´ì˜ ë¦¬í¬íŠ¸ ìë™í™” í”„ë¡œì íŠ¸
"""

import os
import sys
import json
import argparse
import logging
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional, Union
import warnings

# ê³¼í•™ì  ê³„ì‚° ë° ë°ì´í„° ì²˜ë¦¬
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ (numpy/pandas í˜¸í™˜ì„±)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# ë¡œê¹… ì„¤ì • (í•œêµ­ì–´ ë©”ì‹œì§€)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/tmp/anomaly_detect.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class AnomalyDetector:
    """
    ì´ìƒíƒì§€ ë° ì˜ˆì¸¡ ë¶„ì„ ì—”ì§„ (í•œêµ­ì–´ ì£¼ì„ í¬í•¨)

    ì£¼ìš” ì•Œê³ ë¦¬ì¦˜:
    1. ì „ì²˜ë¦¬: ëˆ„ë½ê°’ ë³´ê°„, ì´ìƒê°’ ì œê±°, EWMA ìŠ¤ë¬´ë”©
    2. ì´ìƒíƒì§€: ë¡¤ë§ ìœˆë„ìš° Z-score (|z| > threshold)
    3. ì˜ˆì¸¡: Linear Regression + EWMA ì¡°í•©ìœ¼ë¡œ 7ì¼ ì „ë§
    """

    def __init__(self,
                 window_size: int = 7,
                 threshold: float = 3.0,
                 forecast_days: int = 7,
                 ewma_alpha: float = 0.3):
        """
        ì´ìƒíƒì§€ ì—”ì§„ ì´ˆê¸°í™”

        Args:
            window_size: ë¡¤ë§ ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ 7ì¼)
            threshold: Z-score ì„ê³„ì¹˜ (ê¸°ë³¸ 3.0, 99.7% ì‹ ë¢°êµ¬ê°„)
            forecast_days: ì˜ˆì¸¡ ê¸°ê°„ (ê¸°ë³¸ 7ì¼)
            ewma_alpha: EWMA í‰í™œí™” ê³„ìˆ˜ (0.1~0.5 ê¶Œì¥)
        """
        self.window_size = window_size
        self.threshold = threshold
        self.forecast_days = forecast_days
        self.ewma_alpha = ewma_alpha

        # ë‚´ë¶€ ìƒíƒœ ë³€ìˆ˜
        self.metrics_data = {}
        self.anomalies = {}
        self.forecasts = {}
        self.risk_levels = {}

        logger.info(f"ğŸ” ì´ìƒíƒì§€ ì—”ì§„ ì´ˆê¸°í™”: window={window_size}, threshold={threshold}, forecast={forecast_days}ì¼")

    def load_metric_sources(self,
                           metrics_dir: str = "reports/metrics",
                           logs_dir: str = "logs",
                           ci_reports_dir: str = "reports/ci_reports") -> Dict[str, pd.DataFrame]:
        """
        ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í†µí•©

        Args:
            metrics_dir: ë©”íŠ¸ë¦­ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            logs_dir: ë¡œê·¸ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            ci_reports_dir: CI ë¦¬í¬íŠ¸ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬

        Returns:
            Dict[str, pd.DataFrame]: ì§€í‘œëª…ë³„ ì‹œê³„ì—´ ë°ì´í„°
        """
        logger.info("ğŸ“Š ë©”íŠ¸ë¦­ ë°ì´í„° ì†ŒìŠ¤ ë¡œë”© ì‹œì‘")

        all_data = {}

        try:
            # 1. ë©”íŠ¸ë¦­ JSON íŒŒì¼ë“¤ ì²˜ë¦¬
            metrics_path = Path(metrics_dir)
            if metrics_path.exists():
                for json_file in metrics_path.glob("*.json"):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)

                        # JSON êµ¬ì¡°ì— ë”°ë¼ ì‹œê³„ì—´ ë°ì´í„° ì¶”ì¶œ
                        df = self._parse_metrics_json(data, json_file.stem)
                        if df is not None and not df.empty:
                            all_data[f"metrics_{json_file.stem}"] = df
                            logger.info(f"âœ… ë©”íŠ¸ë¦­ íŒŒì¼ ë¡œë“œ: {json_file.name} ({len(df)} ë ˆì½”ë“œ)")

                    except Exception as e:
                        logger.warning(f"âš ï¸ ë©”íŠ¸ë¦­ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {json_file.name} - {str(e)}")

            # 2. ë¡œê·¸ íŒŒì¼ë“¤ì—ì„œ ìˆ˜ì¹˜ ë©”íŠ¸ë¦­ ì¶”ì¶œ
            logs_path = Path(logs_dir)
            if logs_path.exists():
                for log_file in logs_path.glob("*.log"):
                    try:
                        df = self._parse_log_file(log_file)
                        if df is not None and not df.empty:
                            all_data[f"logs_{log_file.stem}"] = df
                            logger.info(f"âœ… ë¡œê·¸ íŒŒì¼ ë¶„ì„: {log_file.name} ({len(df)} ë©”íŠ¸ë¦­)")

                    except Exception as e:
                        logger.warning(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {log_file.name} - {str(e)}")

            # 3. CI ë¦¬í¬íŠ¸ JSON íŒŒì¼ë“¤ ì²˜ë¦¬
            ci_path = Path(ci_reports_dir)
            if ci_path.exists():
                for ci_file in ci_path.glob("*.json"):
                    try:
                        with open(ci_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)

                        df = self._parse_ci_report_json(data, ci_file.stem)
                        if df is not None and not df.empty:
                            all_data[f"ci_{ci_file.stem}"] = df
                            logger.info(f"âœ… CI ë¦¬í¬íŠ¸ ë¡œë“œ: {ci_file.name} ({len(df)} ë©”íŠ¸ë¦­)")

                    except Exception as e:
                        logger.warning(f"âš ï¸ CI ë¦¬í¬íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {ci_file.name} - {str(e)}")

            logger.info(f"ğŸ“Š ì´ {len(all_data)}ê°œ ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ ì™„ë£Œ")
            self.metrics_data = all_data
            return all_data

        except Exception as e:
            logger.error(f"âŒ ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {}

    def _parse_metrics_json(self, data: Dict, filename: str) -> Optional[pd.DataFrame]:
        """
        ë©”íŠ¸ë¦­ JSON íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ì‹œê³„ì—´ DataFrameìœ¼ë¡œ ë³€í™˜

        Args:
            data: JSON ë°ì´í„°
            filename: íŒŒì¼ëª… (ë©”íŠ¸ë¦­ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©)

        Returns:
            pd.DataFrame: timestamp, value ì»¬ëŸ¼ì„ ê°€ì§„ ì‹œê³„ì—´ ë°ì´í„°
        """
        try:
            records = []

            # ë‹¤ì–‘í•œ JSON êµ¬ì¡° ì§€ì›
            if isinstance(data, list):
                # [{"timestamp": "...", "value": ...}, ...] í˜•íƒœ
                for item in data:
                    if isinstance(item, dict) and 'timestamp' in item:
                        timestamp = pd.to_datetime(item['timestamp'])

                        # ì—¬ëŸ¬ ê°’ í•„ë“œ ì§€ì›
                        for key, value in item.items():
                            if key != 'timestamp' and isinstance(value, (int, float)):
                                records.append({
                                    'timestamp': timestamp,
                                    'metric': f"{filename}_{key}",
                                    'value': float(value)
                                })

            elif isinstance(data, dict):
                # {"metric_name": {"2024-01-01": value, ...}} í˜•íƒœ
                for metric_name, time_series in data.items():
                    if isinstance(time_series, dict):
                        for date_str, value in time_series.items():
                            try:
                                timestamp = pd.to_datetime(date_str)
                                if isinstance(value, (int, float)):
                                    records.append({
                                        'timestamp': timestamp,
                                        'metric': f"{filename}_{metric_name}",
                                        'value': float(value)
                                    })
                            except:
                                continue

            if records:
                df = pd.DataFrame(records)
                df = df.sort_values('timestamp')
                return df

            return None

        except Exception as e:
            logger.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜ ({filename}): {str(e)}")
            return None

    def _parse_log_file(self, log_file: Path) -> Optional[pd.DataFrame]:
        """
        ë¡œê·¸ íŒŒì¼ì—ì„œ ìˆ˜ì¹˜ ë©”íŠ¸ë¦­ì„ ì¶”ì¶œ

        Args:
            log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ

        Returns:
            pd.DataFrame: ì¶”ì¶œëœ ìˆ˜ì¹˜ ë©”íŠ¸ë¦­ë“¤
        """
        try:
            import re

            records = []
            metric_patterns = [
                # ì¼ë°˜ì ì¸ ë©”íŠ¸ë¦­ íŒ¨í„´ë“¤
                r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}).*?cpu[_\s]*usage[:\s]*(\d+\.?\d*)%?',
                r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}).*?memory[_\s]*usage[:\s]*(\d+\.?\d*)%?',
                r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}).*?disk[_\s]*usage[:\s]*(\d+\.?\d*)%?',
                r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}).*?response[_\s]*time[:\s]*(\d+\.?\d*)(ms|s)?',
                r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}).*?blocked[_\s]*ips?[:\s]*(\d+)',
                r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}).*?requests?[_\s]*count[:\s]*(\d+)',
            ]

            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line_num, line in enumerate(f, 1):
                    if line_num > 50000:  # ì„±ëŠ¥ì„ ìœ„í•´ 50K ë¼ì¸ìœ¼ë¡œ ì œí•œ
                        break

                    for pattern in metric_patterns:
                        matches = re.findall(pattern, line, re.IGNORECASE)
                        for match in matches:
                            try:
                                timestamp_str = match[0]
                                value = float(match[1])

                                timestamp = pd.to_datetime(timestamp_str)
                                metric_name = f"log_{log_file.stem}_{pattern.split('.*?')[1].split('[')[0]}"

                                records.append({
                                    'timestamp': timestamp,
                                    'metric': metric_name,
                                    'value': value
                                })
                            except:
                                continue

            if records:
                df = pd.DataFrame(records)
                df = df.sort_values('timestamp')
                return df

            return None

        except Exception as e:
            logger.warning(f"ë¡œê·¸ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ ({log_file.name}): {str(e)}")
            return None

    def _parse_ci_report_json(self, data: Dict, filename: str) -> Optional[pd.DataFrame]:
        """
        CI ë¦¬í¬íŠ¸ JSONì—ì„œ ì‹œê³„ì—´ ë©”íŠ¸ë¦­ ì¶”ì¶œ

        Args:
            data: CI ë¦¬í¬íŠ¸ JSON ë°ì´í„°
            filename: íŒŒì¼ëª…

        Returns:
            pd.DataFrame: CI ê´€ë ¨ ì‹œê³„ì—´ ë©”íŠ¸ë¦­
        """
        try:
            records = []

            # CI ë¦¬í¬íŠ¸ì—ì„œ ì¶”ì¶œí•  ë©”íŠ¸ë¦­ë“¤
            ci_metrics = [
                'build_duration_seconds', 'test_duration_seconds', 'success_rate',
                'test_coverage_percent', 'build_size_mb', 'deployment_time_seconds'
            ]

            if isinstance(data, dict):
                # íƒ€ì„ìŠ¤íƒ¬í”„ ì°¾ê¸°
                timestamp = None
                for key in ['timestamp', 'created_at', 'build_time', 'date']:
                    if key in data:
                        try:
                            timestamp = pd.to_datetime(data[key])
                            break
                        except:
                            continue

                if timestamp is None:
                    # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                    import re
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', filename)
                    if date_match:
                        timestamp = pd.to_datetime(date_match.group(1))
                    else:
                        timestamp = datetime.now()

                # ë©”íŠ¸ë¦­ ê°’ë“¤ ì¶”ì¶œ
                for metric in ci_metrics:
                    value = self._extract_nested_value(data, metric)
                    if value is not None and isinstance(value, (int, float)):
                        records.append({
                            'timestamp': timestamp,
                            'metric': f"ci_{filename}_{metric}",
                            'value': float(value)
                        })

            if records:
                df = pd.DataFrame(records)
                return df

            return None

        except Exception as e:
            logger.warning(f"CI ë¦¬í¬íŠ¸ íŒŒì‹± ì˜¤ë¥˜ ({filename}): {str(e)}")
            return None

    def _extract_nested_value(self, data: Dict, key: str) -> Optional[Union[int, float]]:
        """
        ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ì—ì„œ í‚¤ ê°’ì„ ì¬ê·€ì ìœ¼ë¡œ ì¶”ì¶œ

        Args:
            data: ë”•ì…”ë„ˆë¦¬ ë°ì´í„°
            key: ì°¾ì„ í‚¤

        Returns:
            ì°¾ì€ ê°’ ë˜ëŠ” None
        """
        if isinstance(data, dict):
            if key in data:
                return data[key]

            for value in data.values():
                if isinstance(value, dict):
                    result = self._extract_nested_value(value, key)
                    if result is not None:
                        return result

        return None

    def preprocess_data(self) -> Dict[str, pd.DataFrame]:
        """
        ë¡œë“œëœ ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬

        ì „ì²˜ë¦¬ ë‹¨ê³„:
        1. ëˆ„ë½ê°’ ë³´ê°„ (ì„ í˜• ë³´ê°„)
        2. ì´ìƒê°’ ì œê±° (IQR ë°©ë²•)
        3. EWMA ìŠ¤ë¬´ë”© ì ìš©
        4. í‘œì¤€í™” (Z-score ê³„ì‚°ìš©)

        Returns:
            Dict[str, pd.DataFrame]: ì „ì²˜ë¦¬ëœ ë©”íŠ¸ë¦­ ë°ì´í„°
        """
        logger.info("ğŸ”§ ë©”íŠ¸ë¦­ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")

        processed_data = {}

        for source_name, df in self.metrics_data.items():
            try:
                if df.empty:
                    continue

                # ë©”íŠ¸ë¦­ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì²˜ë¦¬
                for metric_name in df['metric'].unique():
                    metric_df = df[df['metric'] == metric_name].copy()
                    metric_df = metric_df.sort_values('timestamp')

                    # 1. ëˆ„ë½ê°’ ì²˜ë¦¬ (ì„ í˜• ë³´ê°„)
                    metric_df['value'] = metric_df['value'].interpolate(method='linear')
                    metric_df = metric_df.dropna()

                    if len(metric_df) < self.window_size:
                        continue  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ

                    # 2. ì´ìƒê°’ ì œê±° (IQR ë°©ë²•)
                    Q1 = metric_df['value'].quantile(0.25)
                    Q3 = metric_df['value'].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR

                    # ê·¹ë‹¨ì  ì´ìƒê°’ë§Œ ì œê±° (90% ì´ìƒ ë°ì´í„° ë³´ì¡´)
                    outlier_mask = (metric_df['value'] < lower_bound - IQR) | (metric_df['value'] > upper_bound + IQR)
                    metric_df = metric_df[~outlier_mask]

                    # 3. EWMA ìŠ¤ë¬´ë”© ì ìš©
                    metric_df['ewma'] = metric_df['value'].ewm(alpha=self.ewma_alpha).mean()

                    # 4. ë¡¤ë§ í†µê³„ ê³„ì‚° (Z-scoreìš©)
                    metric_df['rolling_mean'] = metric_df['ewma'].rolling(window=self.window_size, min_periods=1).mean()
                    metric_df['rolling_std'] = metric_df['ewma'].rolling(window=self.window_size, min_periods=1).std()

                    # 5. Z-score ê³„ì‚°
                    metric_df['z_score'] = (metric_df['ewma'] - metric_df['rolling_mean']) / (metric_df['rolling_std'] + 1e-8)

                    if len(metric_df) > 0:
                        processed_data[metric_name] = metric_df
                        logger.debug(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {metric_name} ({len(metric_df)} í¬ì¸íŠ¸)")

            except Exception as e:
                logger.warning(f"âš ï¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨ ({source_name}): {str(e)}")

        logger.info(f"ğŸ”§ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_data)}ê°œ ë©”íŠ¸ë¦­")
        return processed_data

    def detect_anomalies(self, processed_data: Dict[str, pd.DataFrame]) -> Dict[str, List[Dict]]:
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ íƒì§€

        Args:
            processed_data: ì „ì²˜ë¦¬ëœ ë©”íŠ¸ë¦­ ë°ì´í„°

        Returns:
            Dict[str, List[Dict]]: ë©”íŠ¸ë¦­ë³„ ì´ìƒì¹˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” ì´ìƒì¹˜ íƒì§€ ì‹œì‘ (ì„ê³„ì¹˜: {self.threshold})")

        anomalies = {}

        for metric_name, df in processed_data.items():
            try:
                metric_anomalies = []

                # Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
                anomaly_mask = np.abs(df['z_score']) > self.threshold
                anomaly_points = df[anomaly_mask]

                for idx, row in anomaly_points.iterrows():
                    anomaly = {
                        'timestamp': row['timestamp'].isoformat(),
                        'value': float(row['value']),
                        'ewma': float(row['ewma']),
                        'z_score': float(row['z_score']),
                        'severity': self._classify_anomaly_severity(abs(row['z_score'])),
                        'direction': 'increase' if row['z_score'] > 0 else 'decrease'
                    }
                    metric_anomalies.append(anomaly)

                if metric_anomalies:
                    anomalies[metric_name] = metric_anomalies
                    logger.info(f"ğŸ” {metric_name}: {len(metric_anomalies)}ê°œ ì´ìƒì¹˜ ë°œê²¬")

            except Exception as e:
                logger.warning(f"âš ï¸ ì´ìƒì¹˜ íƒì§€ ì‹¤íŒ¨ ({metric_name}): {str(e)}")

        self.anomalies = anomalies
        total_anomalies = sum(len(anomaly_list) for anomaly_list in anomalies.values())
        logger.info(f"ğŸ” ì´ìƒì¹˜ íƒì§€ ì™„ë£Œ: ì´ {total_anomalies}ê°œ ë°œê²¬")

        return anomalies

    def _classify_anomaly_severity(self, abs_z_score: float) -> str:
        """
        Z-score ì ˆëŒ“ê°’ì— ë”°ë¥¸ ì´ìƒì¹˜ ì‹¬ê°ë„ ë¶„ë¥˜

        Args:
            abs_z_score: Z-score ì ˆëŒ“ê°’

        Returns:
            str: ì‹¬ê°ë„ ('low', 'medium', 'high', 'critical')
        """
        if abs_z_score >= 5.0:
            return 'critical'  # ë§¤ìš° ë“œë¬¸ ì‚¬ê±´ (0.0001% í™•ë¥ )
        elif abs_z_score >= 4.0:
            return 'high'      # ë“œë¬¸ ì‚¬ê±´ (0.01% í™•ë¥ )
        elif abs_z_score >= 3.5:
            return 'medium'    # ì¼ë°˜ì ì´ì§€ ì•Šì€ ì‚¬ê±´ (0.05% í™•ë¥ )
        else:
            return 'low'       # ë¹„êµì  í”í•œ ì´ìƒì¹˜ (0.3% í™•ë¥ )

    def generate_forecasts(self, processed_data: Dict[str, pd.DataFrame]) -> Dict[str, Dict]:
        """
        ì„ í˜•íšŒê·€ + EWMAë¥¼ ì´ìš©í•œ 7ì¼ ì˜ˆì¸¡ ìƒì„±

        Args:
            processed_data: ì „ì²˜ë¦¬ëœ ë©”íŠ¸ë¦­ ë°ì´í„°

        Returns:
            Dict[str, Dict]: ë©”íŠ¸ë¦­ë³„ ì˜ˆì¸¡ ê²°ê³¼
        """
        logger.info(f"ğŸ“ˆ {self.forecast_days}ì¼ ì˜ˆì¸¡ ìƒì„± ì‹œì‘")

        forecasts = {}

        for metric_name, df in processed_data.items():
            try:
                if len(df) < 14:  # ìµœì†Œ 2ì£¼ ë°ì´í„° í•„ìš”
                    continue

                # ìµœê·¼ 30ì¼ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
                recent_data = df.tail(min(30, len(df))).copy()
                recent_data = recent_data.reset_index(drop=True)

                # 1. ì„ í˜•íšŒê·€ë¡œ ì¥ê¸° ì¶”ì„¸ íŒŒì•…
                X = np.arange(len(recent_data)).reshape(-1, 1)
                y = recent_data['ewma'].values

                lr_model = LinearRegression()
                lr_model.fit(X, y)

                # 2. EWMAì˜ ìµœê·¼ ì¶”ì„¸ ë°˜ì˜
                ewma_trend = recent_data['ewma'].iloc[-3:].mean() - recent_data['ewma'].iloc[-7:-4].mean()

                # 3. ë¯¸ë˜ 7ì¼ ì˜ˆì¸¡
                future_X = np.arange(len(recent_data), len(recent_data) + self.forecast_days).reshape(-1, 1)
                linear_forecast = lr_model.predict(future_X)

                # EWMA ì¶”ì„¸ ë°˜ì˜í•œ ì¡°ì •
                ewma_adjustment = np.array([ewma_trend * (i + 1) * 0.1 for i in range(self.forecast_days)])
                adjusted_forecast = linear_forecast + ewma_adjustment

                # 4. ì‹ ë¢°êµ¬ê°„ ê³„ì‚° (ìµœê·¼ ë°ì´í„°ì˜ ë³€ë™ì„± ê¸°ë°˜)
                recent_std = recent_data['ewma'].std()
                confidence_interval = 1.96 * recent_std  # 95% ì‹ ë¢°êµ¬ê°„

                # 5. ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±
                base_date = recent_data['timestamp'].iloc[-1]
                forecast_dates = [base_date + timedelta(days=i+1) for i in range(self.forecast_days)]

                forecast_points = []
                for i, (date, value) in enumerate(zip(forecast_dates, adjusted_forecast)):
                    forecast_points.append({
                        'date': date.isoformat(),
                        'predicted_value': float(value),
                        'lower_bound': float(value - confidence_interval),
                        'upper_bound': float(value + confidence_interval),
                        'confidence': 0.95
                    })

                # 6. ìœ„í—˜ êµ¬ê°„ ì˜ˆì¸¡ (ì„ê³„ì¹˜ ì´ˆê³¼ ê°€ëŠ¥ì„±)
                risk_periods = self._identify_risk_periods(
                    recent_data, forecast_points, confidence_interval
                )

                forecasts[metric_name] = {
                    'forecast_points': forecast_points,
                    'trend_direction': 'increasing' if lr_model.coef_[0] > 0 else 'decreasing',
                    'trend_strength': abs(float(lr_model.coef_[0])),
                    'confidence_level': 0.95,
                    'model_accuracy': self._calculate_model_accuracy(recent_data, lr_model),
                    'risk_periods': risk_periods
                }

                logger.debug(f"ğŸ“ˆ ì˜ˆì¸¡ ì™„ë£Œ: {metric_name} (ì •í™•ë„: {forecasts[metric_name]['model_accuracy']:.2%})")

            except Exception as e:
                logger.warning(f"âš ï¸ ì˜ˆì¸¡ ì‹¤íŒ¨ ({metric_name}): {str(e)}")

        self.forecasts = forecasts
        logger.info(f"ğŸ“ˆ ì˜ˆì¸¡ ì™„ë£Œ: {len(forecasts)}ê°œ ë©”íŠ¸ë¦­")

        return forecasts

    def _identify_risk_periods(self,
                              recent_data: pd.DataFrame,
                              forecast_points: List[Dict],
                              confidence_interval: float) -> List[Dict]:
        """
        ì˜ˆì¸¡ëœ ê°’ì´ ì´ìƒì¹˜ ì„ê³„ì¹˜ë¥¼ ì´ˆê³¼í•  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìœ„í—˜ ê¸°ê°„ ì‹ë³„

        Args:
            recent_data: ìµœê·¼ ë°ì´í„°
            forecast_points: ì˜ˆì¸¡ í¬ì¸íŠ¸ë“¤
            confidence_interval: ì‹ ë¢°êµ¬ê°„

        Returns:
            List[Dict]: ìœ„í—˜ ê¸°ê°„ ëª©ë¡
        """
        risk_periods = []

        try:
            # ìµœê·¼ ë°ì´í„°ì˜ í†µê³„ ê³„ì‚°
            recent_mean = recent_data['ewma'].mean()
            recent_std = recent_data['ewma'].std()

            # ì´ìƒì¹˜ ì„ê³„ì¹˜ ê³„ì‚°
            upper_threshold = recent_mean + self.threshold * recent_std
            lower_threshold = recent_mean - self.threshold * recent_std

            for point in forecast_points:
                # ì‹ ë¢°êµ¬ê°„ ìƒí•œ/í•˜í•œì´ ì„ê³„ì¹˜ë¥¼ ë„˜ì„ í™•ë¥  ê³„ì‚°
                upper_risk = point['upper_bound'] > upper_threshold
                lower_risk = point['lower_bound'] < lower_threshold

                if upper_risk or lower_risk:
                    risk_type = []
                    if upper_risk:
                        risk_type.append('high_value')
                    if lower_risk:
                        risk_type.append('low_value')

                    risk_periods.append({
                        'date': point['date'],
                        'risk_type': risk_type,
                        'risk_probability': min(0.95, abs(point['predicted_value'] - recent_mean) / (recent_std * self.threshold)),
                        'predicted_value': point['predicted_value'],
                        'threshold_exceeded': upper_risk or lower_risk
                    })

        except Exception as e:
            logger.warning(f"ìœ„í—˜ ê¸°ê°„ ì‹ë³„ ì˜¤ë¥˜: {str(e)}")

        return risk_periods

    def _calculate_model_accuracy(self, data: pd.DataFrame, model: LinearRegression) -> float:
        """
        ëª¨ë¸ì˜ ì •í™•ë„ ê³„ì‚° (ë°±í…ŒìŠ¤íŒ…)

        Args:
            data: í•™ìŠµ ë°ì´í„°
            model: í›ˆë ¨ëœ ëª¨ë¸

        Returns:
            float: ì •í™•ë„ (0~1)
        """
        try:
            if len(data) < 7:
                return 0.5

            # ë§ˆì§€ë§‰ 7ì¼ì„ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‚¬ìš©
            train_data = data.iloc[:-7]
            test_data = data.iloc[-7:]

            if len(train_data) < 7:
                return 0.5

            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
            X_test = np.arange(len(train_data), len(data)).reshape(-1, 1)
            y_test = test_data['ewma'].values
            y_pred = model.predict(X_test)

            # MAPE (Mean Absolute Percentage Error) ê³„ì‚°
            mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-8))) * 100
            accuracy = max(0, min(1, (100 - mape) / 100))

            return accuracy

        except Exception:
            return 0.5

    def calculate_risk_levels(self) -> Dict[str, str]:
        """
        ê° ë©”íŠ¸ë¦­ì˜ ì „ë°˜ì ì¸ ìœ„í—˜ë„ ìˆ˜ì¤€ ê³„ì‚°

        Returns:
            Dict[str, str]: ë©”íŠ¸ë¦­ë³„ ìœ„í—˜ë„ ('low', 'medium', 'high', 'critical')
        """
        risk_levels = {}

        for metric_name in self.metrics_data:
            total_score = 0
            factors = 0

            # 1. ì´ìƒì¹˜ ë¹ˆë„ ì ìˆ˜
            if metric_name in self.anomalies:
                anomaly_count = len(self.anomalies[metric_name])
                anomaly_score = min(10, anomaly_count)  # ìµœëŒ€ 10ì 
                total_score += anomaly_score
                factors += 1

            # 2. ì˜ˆì¸¡ ìœ„í—˜ë„ ì ìˆ˜
            if metric_name in self.forecasts:
                risk_periods = self.forecasts[metric_name].get('risk_periods', [])
                risk_score = len(risk_periods) * 2  # ìœ„í—˜ ê¸°ê°„ë‹¹ 2ì 
                total_score += risk_score
                factors += 1

                # 3. ëª¨ë¸ ì •í™•ë„ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ìœ„í—˜)
                accuracy = self.forecasts[metric_name].get('model_accuracy', 0.5)
                accuracy_score = (1 - accuracy) * 5  # ìµœëŒ€ 5ì 
                total_score += accuracy_score
                factors += 1

            # í‰ê·  ì ìˆ˜ë¡œ ìœ„í—˜ë„ ê²°ì •
            if factors > 0:
                avg_score = total_score / factors
                if avg_score >= 7:
                    risk_level = 'critical'
                elif avg_score >= 5:
                    risk_level = 'high'
                elif avg_score >= 3:
                    risk_level = 'medium'
                else:
                    risk_level = 'low'
            else:
                risk_level = 'low'

            risk_levels[metric_name] = risk_level

        self.risk_levels = risk_levels
        return risk_levels

    def generate_report(self, output_format: str = 'json') -> str:
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            output_format: ì¶œë ¥ í˜•ì‹ ('json' ë˜ëŠ” 'markdown')

        Returns:
            str: ìƒì„±ëœ ë¦¬í¬íŠ¸ ë‚´ìš©
        """
        logger.info(f"ğŸ“‹ {output_format.upper()} ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")

        # ìš”ì•½ í†µê³„ ê³„ì‚°
        total_anomalies = sum(len(anomaly_list) for anomaly_list in self.anomalies.values())
        high_risk_metrics = sum(1 for level in self.risk_levels.values() if level in ['high', 'critical'])
        total_risk_periods = sum(len(forecast['risk_periods']) for forecast in self.forecasts.values())

        # ìƒìœ„ ì´ìƒì¹˜ êµ¬ê°„ ì¶”ì¶œ (Top N)
        top_anomalies = self._get_top_anomalies(limit=10)

        if output_format.lower() == 'markdown':
            return self._generate_markdown_report(
                total_anomalies, high_risk_metrics, total_risk_periods, top_anomalies
            )
        else:
            return self._generate_json_report(
                total_anomalies, high_risk_metrics, total_risk_periods, top_anomalies
            )

    def _get_top_anomalies(self, limit: int = 10) -> List[Dict]:
        """
        Z-scoreê°€ ë†’ì€ ìƒìœ„ ì´ìƒì¹˜ë“¤ ì¶”ì¶œ

        Args:
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜

        Returns:
            List[Dict]: ìƒìœ„ ì´ìƒì¹˜ ëª©ë¡
        """
        all_anomalies = []

        for metric_name, anomaly_list in self.anomalies.items():
            for anomaly in anomaly_list:
                anomaly_copy = anomaly.copy()
                anomaly_copy['metric'] = metric_name
                all_anomalies.append(anomaly_copy)

        # Z-score ì ˆëŒ“ê°’ìœ¼ë¡œ ì •ë ¬
        all_anomalies.sort(key=lambda x: abs(x['z_score']), reverse=True)

        return all_anomalies[:limit]

    def _generate_markdown_report(self,
                                total_anomalies: int,
                                high_risk_metrics: int,
                                total_risk_periods: int,
                                top_anomalies: List[Dict]) -> str:
        """
        Markdown í˜•ì‹ ë¦¬í¬íŠ¸ ìƒì„±
        """
        report_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        md_content = f"""# ğŸ” ì´ìƒíƒì§€ ë° ì˜ˆì¸¡ ë¶„ì„ ë¦¬í¬íŠ¸

**ìƒì„±ì¼ì‹œ**: {report_time}
**ë¶„ì„ ê¸°ê°„**: ìµœê·¼ {self.window_size}ì¼ ë¡¤ë§ ìœˆë„ìš°
**ì„ê³„ì¹˜**: Z-score Â± {self.threshold}
**ì˜ˆì¸¡ ê¸°ê°„**: {self.forecast_days}ì¼

## ğŸ“Š ìš”ì•½ í†µê³„

| ì§€í‘œ | ê°’ |
|------|-----|
| ğŸš¨ ì´ ì´ìƒì¹˜ ìˆ˜ | {total_anomalies}ê°œ |
| âš ï¸ ê³ ìœ„í—˜ ë©”íŠ¸ë¦­ ìˆ˜ | {high_risk_metrics}ê°œ |
| ğŸ“ˆ ì˜ˆì¸¡ ìœ„í—˜ êµ¬ê°„ | {total_risk_periods}ê°œ |
| ğŸ“Š ë¶„ì„ëœ ë©”íŠ¸ë¦­ ìˆ˜ | {len(self.metrics_data)}ê°œ |

## ğŸ” ìƒìœ„ ì´ìƒì¹˜ êµ¬ê°„ (Top {len(top_anomalies)})

"""

        for i, anomaly in enumerate(top_anomalies, 1):
            severity_emoji = {
                'critical': 'ğŸš¨', 'high': 'âŒ', 'medium': 'âš ï¸', 'low': 'â„¹ï¸'
            }
            direction_emoji = 'ğŸ“ˆ' if anomaly['direction'] == 'increase' else 'ğŸ“‰'

            md_content += f"""### {i}. {severity_emoji.get(anomaly['severity'], 'ğŸ”')} {anomaly['metric']}

- **ì‹œê°„**: {anomaly['timestamp'][:19]}
- **ê°’**: {anomaly['value']:.2f} (EWMA: {anomaly['ewma']:.2f})
- **Z-score**: {anomaly['z_score']:.2f} {direction_emoji}
- **ì‹¬ê°ë„**: {anomaly['severity']}

"""

        # ë©”íŠ¸ë¦­ë³„ ìœ„í—˜ë„ í˜„í™©
        md_content += "\n## ğŸ“Š ë©”íŠ¸ë¦­ë³„ ìœ„í—˜ë„ í˜„í™©\n\n"
        risk_emoji = {'critical': 'ğŸš¨', 'high': 'âŒ', 'medium': 'âš ï¸', 'low': 'âœ…'}

        for metric, risk_level in sorted(self.risk_levels.items()):
            emoji = risk_emoji.get(risk_level, 'â“')
            anomaly_count = len(self.anomalies.get(metric, []))
            risk_periods = len(self.forecasts.get(metric, {}).get('risk_periods', []))

            md_content += f"- {emoji} **{metric}**: {risk_level} "
            md_content += f"(ì´ìƒì¹˜ {anomaly_count}ê°œ, ìœ„í—˜ì˜ˆì¸¡ {risk_periods}ê°œ)\n"

        # í–¥í›„ ìœ„í—˜ ì˜ˆì¸¡
        md_content += "\n## ğŸ“ˆ í–¥í›„ 7ì¼ ìœ„í—˜ ì˜ˆì¸¡\n\n"

        future_risks = []
        for metric, forecast in self.forecasts.items():
            for risk in forecast.get('risk_periods', []):
                risk_copy = risk.copy()
                risk_copy['metric'] = metric
                future_risks.append(risk_copy)

        future_risks.sort(key=lambda x: x['risk_probability'], reverse=True)

        if future_risks:
            for risk in future_risks[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                date = risk['date'][:10]
                prob = risk['risk_probability'] * 100
                risk_types = ', '.join(risk['risk_type'])

                md_content += f"- **{date}**: {risk['metric']} ({risk_types}, ìœ„í—˜ë„ {prob:.1f}%)\n"
        else:
            md_content += "- âœ… í–¥í›„ 7ì¼ê°„ íŠ¹ë³„í•œ ìœ„í—˜ ìš”ì†Œê°€ ì˜ˆì¸¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"

        # ê¶Œì¥ì‚¬í•­
        md_content += f"""

## ğŸ’¡ ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”
"""
        critical_metrics = [m for m, r in self.risk_levels.items() if r == 'critical']
        if critical_metrics:
            for metric in critical_metrics:
                md_content += f"- ğŸš¨ **{metric}**: ì¦‰ì‹œ ì ê²€ ë° ëŒ€ì‘ í•„ìš”\n"
        else:
            md_content += "- âœ… ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•œ ìœ„í—˜ ìš”ì†Œ ì—†ìŒ\n"

        md_content += """
### ëª¨ë‹ˆí„°ë§ ê°•í™”
"""
        high_risk_metrics_list = [m for m, r in self.risk_levels.items() if r == 'high']
        if high_risk_metrics_list:
            for metric in high_risk_metrics_list:
                md_content += f"- âŒ **{metric}**: ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ í•„ìš”\n"
        else:
            md_content += "- âœ… íŠ¹ë³„í•œ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•œ ë©”íŠ¸ë¦­ ì—†ìŒ\n"

        md_content += f"""

## ğŸ”§ ë¶„ì„ ì„¤ì •

- **ë¡¤ë§ ìœˆë„ìš°**: {self.window_size}ì¼
- **Z-score ì„ê³„ì¹˜**: Â±{self.threshold}
- **EWMA í‰í™œí™” ê³„ìˆ˜**: {self.ewma_alpha}
- **ì˜ˆì¸¡ ê¸°ê°„**: {self.forecast_days}ì¼
- **ì‹ ë¢°êµ¬ê°„**: 95%

---
*ë³¸ ë¦¬í¬íŠ¸ëŠ” MCP Map Company ì´ìƒíƒì§€ ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""

        return md_content

    def _generate_json_report(self,
                            total_anomalies: int,
                            high_risk_metrics: int,
                            total_risk_periods: int,
                            top_anomalies: List[Dict]) -> str:
        """
        JSON í˜•ì‹ ë¦¬í¬íŠ¸ ìƒì„±
        """
        report_data = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "analysis_window_days": self.window_size,
                "z_score_threshold": self.threshold,
                "forecast_days": self.forecast_days,
                "ewma_alpha": self.ewma_alpha,
                "total_metrics_analyzed": len(self.metrics_data)
            },
            "summary": {
                "total_anomalies": total_anomalies,
                "high_risk_metrics": high_risk_metrics,
                "total_future_risk_periods": total_risk_periods,
                "metrics_by_risk_level": {
                    "critical": len([r for r in self.risk_levels.values() if r == 'critical']),
                    "high": len([r for r in self.risk_levels.values() if r == 'high']),
                    "medium": len([r for r in self.risk_levels.values() if r == 'medium']),
                    "low": len([r for r in self.risk_levels.values() if r == 'low'])
                }
            },
            "top_anomalies": top_anomalies,
            "risk_levels": self.risk_levels,
            "detailed_anomalies": self.anomalies,
            "forecasts": self.forecasts,
            "recommendations": self._generate_recommendations()
        }

        return json.dumps(report_data, indent=2, ensure_ascii=False)

    def _generate_recommendations(self) -> List[Dict]:
        """
        ìœ„í—˜ë„ ìˆ˜ì¤€ì— ë”°ë¥¸ ê¶Œì¥ì‚¬í•­ ìƒì„±
        """
        recommendations = []

        # Critical ë©”íŠ¸ë¦­ ê¶Œì¥ì‚¬í•­
        critical_metrics = [m for m, r in self.risk_levels.items() if r == 'critical']
        for metric in critical_metrics:
            recommendations.append({
                "priority": "critical",
                "metric": metric,
                "action": "ì¦‰ì‹œ ì ê²€ ë° ëŒ€ì‘ í•„ìš”",
                "description": f"{metric}ì—ì„œ ì‹¬ê°í•œ ì´ìƒì¹˜ê°€ ë‹¤ìˆ˜ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì‹œìŠ¤í…œ ì ê²€ì„ ì‹¤ì‹œí•˜ì‹­ì‹œì˜¤."
            })

        # High ë©”íŠ¸ë¦­ ê¶Œì¥ì‚¬í•­
        high_metrics = [m for m, r in self.risk_levels.items() if r == 'high']
        for metric in high_metrics:
            recommendations.append({
                "priority": "high",
                "metric": metric,
                "action": "ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ê°•í™”",
                "description": f"{metric}ì˜ íŒ¨í„´ ë³€í™”ë¥¼ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ê³  ì˜ˆë°©ì  ì¡°ì¹˜ë¥¼ ì¤€ë¹„í•˜ì‹­ì‹œì˜¤."
            })

        # í–¥í›„ ìœ„í—˜ ì˜ˆì¸¡ ê¶Œì¥ì‚¬í•­
        high_risk_forecasts = []
        for metric, forecast in self.forecasts.items():
            risk_periods = forecast.get('risk_periods', [])
            if len(risk_periods) >= 2:
                high_risk_forecasts.append(metric)

        for metric in high_risk_forecasts:
            recommendations.append({
                "priority": "medium",
                "metric": metric,
                "action": "ì˜ˆë°©ì  ì¡°ì¹˜ ì¤€ë¹„",
                "description": f"{metric}ì—ì„œ í–¥í›„ 7ì¼ê°„ ìœ„í—˜ êµ¬ê°„ì´ ì˜ˆì¸¡ë©ë‹ˆë‹¤. ì‚¬ì „ ëŒ€ì‘ ê³„íšì„ ìˆ˜ë¦½í•˜ì‹­ì‹œì˜¤."
            })

        return recommendations


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬ ë° ë¶„ì„ ì‹¤í–‰
    """
    parser = argparse.ArgumentParser(
        description="ğŸ” ì´ìƒíƒì§€ ë° ì˜ˆì¸¡ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  %(prog)s                                    # ê¸°ë³¸ ì‹¤í–‰ (30ì¼, JSON ì¶œë ¥)
  %(prog)s --days 7 --md                      # 7ì¼ ë°ì´í„°, Markdown ì¶œë ¥
  %(prog)s --threshold 2.5 --window 14        # ì„ê³„ì¹˜ 2.5, ìœˆë„ìš° 14ì¼
  %(prog)s --json --verbose                   # JSON ì¶œë ¥, ìƒì„¸ ë¡œê·¸
  %(prog)s --dry-run                          # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ê°€ì§œ ë°ì´í„°)
        """
    )

    # ëª…ë ¹í–‰ ì¸ì ì •ì˜
    parser.add_argument('--days', type=int, default=30,
                       help='ë¶„ì„í•  ë°ì´í„° ê¸°ê°„ (ì¼) [ê¸°ë³¸ê°’: 30]')
    parser.add_argument('--threshold', type=float, default=3.0,
                       help='Z-score ì´ìƒì¹˜ ì„ê³„ì¹˜ [ê¸°ë³¸ê°’: 3.0]')
    parser.add_argument('--window', type=int, default=7,
                       help='ë¡¤ë§ ìœˆë„ìš° í¬ê¸° (ì¼) [ê¸°ë³¸ê°’: 7]')
    parser.add_argument('--forecast', type=int, default=7,
                       help='ì˜ˆì¸¡ ê¸°ê°„ (ì¼) [ê¸°ë³¸ê°’: 7]')
    parser.add_argument('--json', action='store_true',
                       help='JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥')
    parser.add_argument('--md', action='store_true',
                       help='Markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥')
    parser.add_argument('--verbose', action='store_true',
                       help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    parser.add_argument('--dry-run', action='store_true',
                       help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê°€ì§œ ë°ì´í„° ì‚¬ìš©)')
    parser.add_argument('--output', type=str,
                       help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ stdout)')

    args = parser.parse_args()

    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        logger.info("ğŸ” ì´ìƒíƒì§€ ë° ì˜ˆì¸¡ ë¶„ì„ ì‹œì‘")
        logger.info(f"ì„¤ì •: days={args.days}, threshold={args.threshold}, window={args.window}")

        # ì´ìƒíƒì§€ ì—”ì§„ ì´ˆê¸°í™”
        detector = AnomalyDetector(
            window_size=args.window,
            threshold=args.threshold,
            forecast_days=args.forecast
        )

        if args.dry_run:
            logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê°€ì§œ ë°ì´í„° ìƒì„±")
            detector.metrics_data = _generate_fake_data(args.days)
        else:
            # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
            detector.load_metric_sources()

        if not detector.metrics_data:
            logger.error("âŒ ë¶„ì„í•  ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            sys.exit(1)

        # ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = detector.preprocess_data()

        if not processed_data:
            logger.error("âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            sys.exit(1)

        # ì´ìƒì¹˜ íƒì§€
        anomalies = detector.detect_anomalies(processed_data)

        # ì˜ˆì¸¡ ìƒì„±
        forecasts = detector.generate_forecasts(processed_data)

        # ìœ„í—˜ë„ ê³„ì‚°
        risk_levels = detector.calculate_risk_levels()

        # ì¶œë ¥ í˜•ì‹ ê²°ì •
        if args.md:
            output_format = 'markdown'
        else:
            output_format = 'json'

        # ë¦¬í¬íŠ¸ ìƒì„±
        report = detector.generate_report(output_format)

        # ì¶œë ¥
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {args.output}")
        else:
            print(report)

        logger.info("âœ… ì´ìƒíƒì§€ ë° ì˜ˆì¸¡ ë¶„ì„ ì™„ë£Œ")

    except KeyboardInterrupt:
        logger.info("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if args.verbose:
            logger.error(traceback.format_exc())
        sys.exit(1)


def _generate_fake_data(days: int) -> Dict[str, pd.DataFrame]:
    """
    í…ŒìŠ¤íŠ¸ìš© ê°€ì§œ ë©”íŠ¸ë¦­ ë°ì´í„° ìƒì„±

    Args:
        days: ìƒì„±í•  ë°ì´í„° ê¸°ê°„ (ì¼)

    Returns:
        Dict[str, pd.DataFrame]: ê°€ì§œ ë©”íŠ¸ë¦­ ë°ì´í„°
    """
    logger.info(f"ğŸ§ª {days}ì¼ê°„ì˜ ê°€ì§œ ë°ì´í„° ìƒì„±")

    fake_data = {}

    # ë©”íŠ¸ë¦­ ì¢…ë¥˜ ì •ì˜
    metrics_config = [
        {"name": "cpu_usage", "base": 45, "noise": 10, "trend": 0.1, "anomaly_prob": 0.05},
        {"name": "memory_usage", "base": 60, "noise": 15, "trend": -0.05, "anomaly_prob": 0.03},
        {"name": "disk_usage", "base": 75, "noise": 5, "trend": 0.2, "anomaly_prob": 0.02},
        {"name": "response_time", "base": 200, "noise": 50, "trend": 0.5, "anomaly_prob": 0.08},
        {"name": "error_rate", "base": 2, "noise": 1, "trend": 0, "anomaly_prob": 0.1},
    ]

    # ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    base_date = datetime.now() - timedelta(days=days)

    for config in metrics_config:
        records = []
        current_value = config["base"]

        for i in range(days * 24):  # ì‹œê°„ë‹¹ ë°ì´í„° í¬ì¸íŠ¸
            timestamp = base_date + timedelta(hours=i)

            # ê¸°ë³¸ ì¶”ì„¸ + ë…¸ì´ì¦ˆ
            trend_component = config["trend"] * i / 24  # ì¼ë³„ ì¶”ì„¸
            noise_component = np.random.normal(0, config["noise"])

            # ì´ìƒì¹˜ ìƒì„± (í™•ë¥ ì )
            if np.random.random() < config["anomaly_prob"]:
                anomaly_factor = np.random.choice([-1, 1]) * np.random.uniform(3, 6)
                noise_component += anomaly_factor * config["noise"]

            current_value = max(0, config["base"] + trend_component + noise_component)

            records.append({
                'timestamp': timestamp,
                'metric': f"fake_{config['name']}",
                'value': current_value
            })

        df = pd.DataFrame(records)
        fake_data[f"fake_metrics_{config['name']}"] = df

    logger.info(f"ğŸ§ª ê°€ì§œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(fake_data)}ê°œ ë©”íŠ¸ë¦­")
    return fake_data


if __name__ == "__main__":
    main()