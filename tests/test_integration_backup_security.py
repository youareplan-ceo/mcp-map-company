import os
import json
import pytest
import subprocess
import tempfile
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from unittest.mock import patch

# ğŸ§ª í†µí•© í…ŒìŠ¤íŠ¸: ë³´ì•ˆ ë¡œê·¸ + ë°±ì—… ê²€ì¦ + CI/CD ì—°ë™ (í•œêµ­ì–´ ì£¼ì„ í¬í•¨)
#
# ì£¼ìš” ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤:
# âœ… ë³´ì•ˆ ì´ë²¤íŠ¸ ë°œìƒ â†’ security.log ê¸°ë¡ ê²€ì¦
# âœ… backup_verifier.sh ì‹¤í–‰ ê²°ê³¼ ê²€ì¦
# âœ… cleanup_old_backups.sh ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ê²€ì¦
# âœ… ëª¨ë“  ê²°ê³¼ë¥¼ JSON í¬ë§·ìœ¼ë¡œ ì¶œë ¥ í™•ì¸
# âœ… CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸
#
# í…ŒìŠ¤íŠ¸ í™˜ê²½:
# - ì„ì‹œ ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
# - Mock ë³´ì•ˆ ì´ë²¤íŠ¸ ìƒì„±
# - ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ê²€ì¦
# - JSON ì¶œë ¥ í˜•ì‹ ê²€ì¦

class TestIntegrationBackupSecurity:
    """ğŸ” ë³´ì•ˆ ë¡œê·¸ + ë°±ì—… í†µí•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    @pytest.fixture(autouse=True)
    def setup_test_environment(self, tmp_path):
        """ğŸ› ï¸ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì´ˆê¸°í™”"""
        # ì„ì‹œ ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_dir = tmp_path / "test_backups"
        self.backup_dir.mkdir()

        # ì„ì‹œ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        self.log_dir = tmp_path / "logs"
        self.log_dir.mkdir()
        self.security_log = self.log_dir / "security.log"

        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ["SECURITY_LOG_PATH"] = str(self.security_log)
        os.environ["TEST_BACKUP_DIR"] = str(self.backup_dir)

        # í…ŒìŠ¤íŠ¸ìš© ë°±ì—… íŒŒì¼ ìƒì„±
        self._create_test_backup_files()

    def _create_test_backup_files(self):
        """ğŸ“¦ í…ŒìŠ¤íŠ¸ìš© ë°±ì—… íŒŒì¼ ìƒì„±"""
        # ìµœì‹  ë°±ì—… íŒŒì¼ (ì˜¤ëŠ˜)
        recent_backup = self.backup_dir / f"backup_{datetime.now().strftime('%Y%m%d')}.tar.gz"
        recent_backup.write_text("recent backup content")

        # ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ (35ì¼ ì „)
        old_date = datetime.now() - timedelta(days=35)
        old_backup = self.backup_dir / f"backup_{old_date.strftime('%Y%m%d')}.tar.gz"
        old_backup.write_text("old backup content")

        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ ì¡°ì • (macOS í˜¸í™˜)
        old_timestamp = old_date.timestamp()
        os.utime(old_backup, (old_timestamp, old_timestamp))

    def test_security_event_logging(self):
        """ğŸ” ë³´ì•ˆ ì´ë²¤íŠ¸ ë°œìƒ â†’ security.log ê¸°ë¡ ê²€ì¦"""
        # Mock ë³´ì•ˆ ë¡œê±°ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê°„ë‹¨í•œ ë¡œê¹…
        try:
            from mcp.security_logger import log_security_event
            log_security_event("BLOCKED_IP", "192.168.1.100 - Rate Limit ì´ˆê³¼ë¡œ ì°¨ë‹¨")
        except ImportError:
            # Mock ë¡œê¹… êµ¬í˜„
            with open(self.security_log, "a", encoding="utf-8") as f:
                log_entry = {
                    "timestamp": datetime.now().isoformat(),
                    "event": "BLOCKED_IP",
                    "message": "192.168.1.100 - Rate Limit ì´ˆê³¼ë¡œ ì°¨ë‹¨",
                    "level": "WARNING"
                }
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

        # ë¡œê·¸ íŒŒì¼ ìƒì„± í™•ì¸
        assert self.security_log.exists(), "âŒ ë³´ì•ˆ ë¡œê·¸ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        # ë¡œê·¸ ë‚´ìš© ê²€ì¦
        with open(self.security_log, "r", encoding="utf-8") as f:
            log_content = f.read()

        assert "BLOCKED_IP" in log_content, "âŒ ë³´ì•ˆ ì´ë²¤íŠ¸ê°€ ë¡œê·¸ì— ê¸°ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        assert "192.168.1.100" in log_content, "âŒ IP ì£¼ì†Œê°€ ë¡œê·¸ì— ê¸°ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        # JSON í˜•ì‹ ê²€ì¦
        log_lines = log_content.strip().split('\n')
        last_log = json.loads(log_lines[-1])
        assert "event" in last_log, "âŒ ë¡œê·¸ JSON í˜•ì‹ì— event í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"
        assert "message" in last_log, "âŒ ë¡œê·¸ JSON í˜•ì‹ì— message í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"

    def test_backup_verifier_execution(self):
        """ğŸ“‹ backup_verifier.sh ì‹¤í–‰ ê²°ê³¼ ê²€ì¦"""
        script_path = Path("scripts/backup_verifier.sh")

        # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        assert script_path.exists(), "âŒ backup_verifier.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"

        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (JSON ëª¨ë“œ)
        result = subprocess.run([
            str(script_path),
            "--dir", str(self.backup_dir),
            "--json"
        ], capture_output=True, text=True, cwd=".")

        # ì‹¤í–‰ ì„±ê³µ í™•ì¸
        assert result.returncode == 0, f"âŒ backup_verifier.sh ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"

        # JSON ì¶œë ¥ ê²€ì¦
        try:
            output_data = json.loads(result.stdout.strip())
            assert "file" in output_data, "âŒ JSON ì¶œë ¥ì— file í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "size" in output_data, "âŒ JSON ì¶œë ¥ì— size í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "modified" in output_data, "âŒ JSON ì¶œë ¥ì— modified í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"

            # íŒŒì¼ í¬ê¸°ê°€ 0ë³´ë‹¤ í°ì§€ í™•ì¸
            assert output_data["size"] > 0, "âŒ ë°±ì—… íŒŒì¼ í¬ê¸°ê°€ 0ì…ë‹ˆë‹¤"

        except json.JSONDecodeError:
            pytest.fail(f"âŒ backup_verifier.sh JSON ì¶œë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {result.stdout}")

    def test_cleanup_script_dry_run(self):
        """ğŸ—‘ï¸ cleanup_old_backups.sh ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ê²€ì¦"""
        script_path = Path("scripts/cleanup_old_backups.sh")

        # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        assert script_path.exists(), "âŒ cleanup_old_backups.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"

        # ì •ë¦¬ ì „ íŒŒì¼ ê°œìˆ˜ í™•ì¸
        files_before = list(self.backup_dir.glob("*.tar.gz"))

        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ + JSON ì¶œë ¥)
        result = subprocess.run([
            str(script_path),
            "--dir", str(self.backup_dir),
            "--days", "30",
            "--dry-run",
            "--json"
        ], capture_output=True, text=True, cwd=".")

        # ì‹¤í–‰ ì„±ê³µ í™•ì¸
        assert result.returncode == 0, f"âŒ cleanup_old_backups.sh ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"

        # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œì—ì„œëŠ” íŒŒì¼ì´ ì‚­ì œë˜ì§€ ì•Šì•„ì•¼ í•¨
        files_after = list(self.backup_dir.glob("*.tar.gz"))
        assert len(files_after) == len(files_before), "âŒ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œì—ì„œ íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"

        # JSON ì¶œë ¥ ê²€ì¦
        try:
            output_data = json.loads(result.stdout.strip())
            assert "deleted_count" in output_data, "âŒ JSON ì¶œë ¥ì— deleted_count í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "total_size_bytes" in output_data, "âŒ JSON ì¶œë ¥ì— total_size_bytes í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "dry_run" in output_data, "âŒ JSON ì¶œë ¥ì— dry_run í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"
            assert output_data["dry_run"] is True, "âŒ dry_run í”Œë˜ê·¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤"

            # ì˜¤ë˜ëœ íŒŒì¼ì´ ê°ì§€ë˜ì—ˆëŠ”ì§€ í™•ì¸ (35ì¼ ì „ íŒŒì¼)
            assert output_data["deleted_count"] > 0, "âŒ ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        except json.JSONDecodeError:
            pytest.fail(f"âŒ cleanup_old_backups.sh JSON ì¶œë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {result.stdout}")

    def test_makefile_integration(self):
        """ğŸ”§ Makefile ëª…ë ¹ì–´ í†µí•© í…ŒìŠ¤íŠ¸"""
        # Makefile ì¡´ì¬ í™•ì¸
        makefile_path = Path("Makefile")
        assert makefile_path.exists(), "âŒ Makefileì´ ì—†ìŠµë‹ˆë‹¤"

        # Makefile ë‚´ìš©ì—ì„œ ë°±ì—… ê´€ë ¨ ëª…ë ¹ì–´ í™•ì¸
        with open(makefile_path, "r", encoding="utf-8") as f:
            makefile_content = f.read()

        assert "verify-backups:" in makefile_content, "âŒ Makefileì— verify-backups ëª…ë ¹ì–´ê°€ ì—†ìŠµë‹ˆë‹¤"
        assert "clean-backups:" in makefile_content, "âŒ Makefileì— clean-backups ëª…ë ¹ì–´ê°€ ì—†ìŠµë‹ˆë‹¤"
        assert "backup-maintenance:" in makefile_content, "âŒ Makefileì— backup-maintenance ëª…ë ¹ì–´ê°€ ì—†ìŠµë‹ˆë‹¤"

    def test_ci_cd_pipeline_simulation(self):
        """ğŸš€ CI/CD íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜"""
        # 1ë‹¨ê³„: ë³´ì•ˆ ì´ë²¤íŠ¸ ë¡œê¹…
        try:
            from mcp.security_logger import log_security_event
            log_security_event("CI_TEST", "CI/CD íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        except ImportError:
            # Mock ë¡œê¹…
            with open(self.security_log, "a", encoding="utf-8") as f:
                log_entry = {
                    "timestamp": datetime.now().isoformat(),
                    "event": "CI_TEST",
                    "message": "CI/CD íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰",
                    "level": "INFO"
                }
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

        # 2ë‹¨ê³„: ë°±ì—… ê²€ì¦ ì‹¤í–‰
        verify_result = subprocess.run([
            "scripts/backup_verifier.sh",
            "--dir", str(self.backup_dir),
            "--json"
        ], capture_output=True, text=True, cwd=".")

        # 3ë‹¨ê³„: ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹œë®¬ë ˆì´ì…˜
        cleanup_result = subprocess.run([
            "scripts/cleanup_old_backups.sh",
            "--dir", str(self.backup_dir),
            "--dry-run",
            "--json"
        ], capture_output=True, text=True, cwd=".")

        # ëª¨ë“  ë‹¨ê³„ ì„±ê³µ í™•ì¸
        assert verify_result.returncode == 0, "âŒ ë°±ì—… ê²€ì¦ ë‹¨ê³„ ì‹¤íŒ¨"
        assert cleanup_result.returncode == 0, "âŒ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ë‹¨ê³„ ì‹¤íŒ¨"

        # í†µí•© ê²°ê³¼ JSON ìƒì„±
        ci_report = {
            "timestamp": datetime.now().isoformat(),
            "pipeline_status": "SUCCESS",
            "steps": {
                "security_logging": "PASSED",
                "backup_verification": "PASSED" if verify_result.returncode == 0 else "FAILED",
                "cleanup_simulation": "PASSED" if cleanup_result.returncode == 0 else "FAILED"
            },
            "backup_verification_output": json.loads(verify_result.stdout.strip()) if verify_result.stdout.strip() else {},
            "cleanup_simulation_output": json.loads(cleanup_result.stdout.strip()) if cleanup_result.stdout.strip() else {}
        }

        # CI ë³´ê³ ì„œ JSON í˜•ì‹ ê²€ì¦
        assert ci_report["pipeline_status"] == "SUCCESS", "âŒ CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
        assert all(status == "PASSED" for status in ci_report["steps"].values()), "âŒ ì¼ë¶€ ë‹¨ê³„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"

    def test_json_output_schema_validation(self):
        """ğŸ“Š JSON ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        # backup_verifier.sh JSON ìŠ¤í‚¤ë§ˆ ê²€ì¦
        verify_result = subprocess.run([
            "scripts/backup_verifier.sh",
            "--dir", str(self.backup_dir),
            "--json"
        ], capture_output=True, text=True, cwd=".")

        verify_data = json.loads(verify_result.stdout.strip())

        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_verify_fields = ["file", "size", "modified"]
        for field in required_verify_fields:
            assert field in verify_data, f"âŒ backup_verifier JSONì— {field} í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"

        # ë°ì´í„° íƒ€ì… ê²€ì¦
        assert isinstance(verify_data["file"], str), "âŒ file í•„ë“œëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤"
        assert isinstance(verify_data["size"], int), "âŒ size í•„ë“œëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
        assert isinstance(verify_data["modified"], str), "âŒ modified í•„ë“œëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤"

        # cleanup_old_backups.sh JSON ìŠ¤í‚¤ë§ˆ ê²€ì¦
        cleanup_result = subprocess.run([
            "scripts/cleanup_old_backups.sh",
            "--dir", str(self.backup_dir),
            "--dry-run",
            "--json"
        ], capture_output=True, text=True, cwd=".")

        cleanup_data = json.loads(cleanup_result.stdout.strip())

        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_cleanup_fields = ["timestamp", "deleted_count", "total_size_bytes", "backup_dir", "days_keep", "dry_run", "deleted_files"]
        for field in required_cleanup_fields:
            assert field in cleanup_data, f"âŒ cleanup JSONì— {field} í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤"

        # ë°ì´í„° íƒ€ì… ê²€ì¦
        assert isinstance(cleanup_data["deleted_count"], int), "âŒ deleted_count í•„ë“œëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
        assert isinstance(cleanup_data["total_size_bytes"], int), "âŒ total_size_bytes í•„ë“œëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
        assert isinstance(cleanup_data["dry_run"], bool), "âŒ dry_run í•„ë“œëŠ” ë¶ˆë¦°ì´ì–´ì•¼ í•©ë‹ˆë‹¤"

    def test_error_handling_and_logging(self):
        """ğŸš¨ ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹… í…ŒìŠ¤íŠ¸"""
        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë°±ì—… ë””ë ‰í† ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
        invalid_dir = "/nonexistent/backup/dir"

        # backup_verifier.sh ì—ëŸ¬ ì²˜ë¦¬
        verify_result = subprocess.run([
            "scripts/backup_verifier.sh",
            "--dir", invalid_dir
        ], capture_output=True, text=True, cwd=".")

        assert verify_result.returncode != 0, "âŒ ì˜ëª»ëœ ë””ë ‰í† ë¦¬ì— ëŒ€í•´ ì—ëŸ¬ê°€ ë°œìƒí•´ì•¼ í•©ë‹ˆë‹¤"
        assert "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in verify_result.stdout or "not found" in verify_result.stdout.lower(), "âŒ ì ì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        # cleanup_old_backups.sh ì—ëŸ¬ ì²˜ë¦¬
        cleanup_result = subprocess.run([
            "scripts/cleanup_old_backups.sh",
            "--dir", invalid_dir,
            "--json"
        ], capture_output=True, text=True, cwd=".")

        assert cleanup_result.returncode != 0, "âŒ ì˜ëª»ëœ ë””ë ‰í† ë¦¬ì— ëŒ€í•´ ì—ëŸ¬ê°€ ë°œìƒí•´ì•¼ í•©ë‹ˆë‹¤"

    def teardown_method(self):
        """ğŸ§¹ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ í›„ ì •ë¦¬"""
        # í™˜ê²½ ë³€ìˆ˜ ì •ë¦¬
        if "SECURITY_LOG_PATH" in os.environ:
            del os.environ["SECURITY_LOG_PATH"]
        if "TEST_BACKUP_DIR" in os.environ:
            del os.environ["TEST_BACKUP_DIR"]

# ğŸ¯ ì„±ëŠ¥ ë° ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
class TestPerformanceAndStress:
    """âš¡ ì„±ëŠ¥ ë° ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    @pytest.fixture(autouse=True)
    def setup_performance_test(self, tmp_path):
        """ğŸ› ï¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì´ˆê¸°í™”"""
        self.backup_dir = tmp_path / "perf_backups"
        self.backup_dir.mkdir()

        # ëŒ€ìš©ëŸ‰ ë°±ì—… íŒŒì¼ ì‹œë®¬ë ˆì´ì…˜ (100ê°œ íŒŒì¼)
        for i in range(100):
            file_date = datetime.now() - timedelta(days=i)
            backup_file = self.backup_dir / f"backup_{file_date.strftime('%Y%m%d')}_{i:03d}.tar.gz"
            backup_file.write_text(f"backup content {i}" * 1000)  # ì•½ 13KB per file

    def test_large_scale_cleanup_performance(self):
        """ğŸ“ˆ ëŒ€ê·œëª¨ ë°±ì—… ì •ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        import time

        start_time = time.time()

        # ëŒ€ìš©ëŸ‰ ì •ë¦¬ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
        result = subprocess.run([
            "scripts/cleanup_old_backups.sh",
            "--dir", str(self.backup_dir),
            "--days", "30",
            "--dry-run",
            "--json"
        ], capture_output=True, text=True, cwd=".")

        execution_time = time.time() - start_time

        # ì„±ëŠ¥ ê²€ì¦ (10ì´ˆ ì´ë‚´ ì™„ë£Œ)
        assert execution_time < 10.0, f"âŒ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ë¯¸ë‹¬: {execution_time:.2f}ì´ˆ ì†Œìš”"
        assert result.returncode == 0, "âŒ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì¤‘ ì‹¤íŒ¨"

        # ê²°ê³¼ ê²€ì¦
        output_data = json.loads(result.stdout.strip())
        assert output_data["deleted_count"] > 50, "âŒ ì˜ˆìƒë³´ë‹¤ ì ì€ íŒŒì¼ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤"

if __name__ == "__main__":
    # ğŸƒâ€â™‚ï¸ í…ŒìŠ¤íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ
    pytest.main([__file__, "-v", "--tb=short"])