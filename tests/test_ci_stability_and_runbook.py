# tests/test_ci_stability_and_runbook.py
"""
CI ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜ ë° ëŸ°ë¶ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ëª¨ìŒ

ì´ íŒŒì¼ì€ ë‹¤ìŒ ì‹œìŠ¤í…œë“¤ì„ ê²€ì¦í•©ë‹ˆë‹¤:
- scripts/ci_stability_sim.sh CI ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
- scripts/runbook_validator.sh ëŸ°ë¶ ì‹œìŠ¤í…œ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- JSON/Markdown ì¶œë ¥ í˜•ì‹ ê²€ì¦
- ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ì •í™•ì„± ê²€ì‚¬
- ëŸ°ë¶ ë§¤í•‘ ë¬´ê²°ì„± ê²€ì¦

ì‘ì„±ì: Claude AI
ìƒì„±ì¼: $(date '+%Y-%m-%d')
"""
from __future__ import annotations

import os
import json
import tempfile
import subprocess
import shutil
from pathlib import Path
from typing import Dict, Any, List, Optional
import time
import re

import pytest

# í…ŒìŠ¤íŠ¸ ì„¤ì • ìƒìˆ˜
TIMEOUT_SECONDS = 30  # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ
MIN_SIMULATION_RUNS = 10  # ìµœì†Œ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ íšŸìˆ˜
MAX_SIMULATION_RUNS = 100  # ìµœëŒ€ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ íšŸìˆ˜


class TestCIStabilitySimulation:
    """scripts/ci_stability_sim.sh CI ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def temp_workspace(self):
        """ì„ì‹œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì„¤ì • (í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ ì €ì¥ìš©)"""
        with tempfile.TemporaryDirectory() as temp_dir:
            workspace = Path(temp_dir)
            yield workspace

    def test_simulation_script_existence_and_permissions(self):
        """ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì¡´ì¬ ë° ì‹¤í–‰ ê¶Œí•œ í™•ì¸"""
        script_path = Path("scripts/ci_stability_sim.sh")
        assert script_path.exists(), "ci_stability_sim.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
        assert script_path.is_file(), "ci_stability_sim.shê°€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤"

        # ì‹¤í–‰ ê¶Œí•œ í™•ì¸ (Unix ì‹œìŠ¤í…œì—ì„œë§Œ)
        if os.name == 'posix':
            assert os.access(script_path, os.X_OK), "ci_stability_sim.shì— ì‹¤í–‰ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤"

    def test_simulation_help_option(self):
        """ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ë„ì›€ë§ ì˜µì…˜ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/ci_stability_sim.sh")
        if not script_path.exists():
            pytest.skip("ci_stability_sim.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        try:
            result = subprocess.run(
                ["bash", str(script_path), "--help"],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            assert result.returncode == 0, f"ë„ì›€ë§ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"
            assert "ì‚¬ìš©ë²•" in result.stdout or "Usage" in result.stdout, "ë„ì›€ë§ì— ì‚¬ìš©ë²•ì´ ì—†ìŠµë‹ˆë‹¤"
            assert "CI ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜" in result.stdout, "ë„ì›€ë§ì— ìŠ¤í¬ë¦½íŠ¸ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail("ë„ì›€ë§ ì‹¤í–‰ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")
        except FileNotFoundError:
            pytest.skip("bashê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    def test_simulation_dry_run_mode(self):
        """ì‹œë®¬ë ˆì´ì…˜ ë“œë¼ì´ëŸ° ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/ci_stability_sim.sh")
        if not script_path.exists():
            pytest.skip("ci_stability_sim.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        try:
            result = subprocess.run(
                [
                    "bash", str(script_path),
                    "--dry-run",
                    "--fail-rate", "10",
                    "--flaky-rate", "5",
                    "--runs", "50",
                    "--verbose"
                ],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            assert result.returncode == 0, f"ë“œë¼ì´ëŸ° ëª¨ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"
            assert "ë“œë¼ì´ëŸ° ëª¨ë“œ" in result.stdout, "ë“œë¼ì´ëŸ° ëª¨ë“œ í‘œì‹œê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "ì„¤ì • í™•ì¸" in result.stdout, "ì„¤ì • í™•ì¸ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail("ë“œë¼ì´ëŸ° ëª¨ë“œ ì‹¤í–‰ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

    @pytest.mark.parametrize("fail_rate,flaky_rate,runs", [
        (5, 2, 20),    # ë‚®ì€ ì‹¤íŒ¨ìœ¨
        (15, 5, 30),   # ì¤‘ê°„ ì‹¤íŒ¨ìœ¨
        (25, 10, 25),  # ë†’ì€ ì‹¤íŒ¨ìœ¨
    ])
    def test_simulation_execution_with_parameters(self, fail_rate, flaky_rate, runs, temp_workspace):
        """ë‹¤ì–‘í•œ ë§¤ê°œë³€ìˆ˜ë¡œ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/ci_stability_sim.sh")
        if not script_path.exists():
            pytest.skip("ci_stability_sim.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        try:
            result = subprocess.run(
                [
                    "bash", str(script_path),
                    "--fail-rate", str(fail_rate),
                    "--flaky-rate", str(flaky_rate),
                    "--runs", str(runs)
                ],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS,
                cwd=temp_workspace
            )

            assert result.returncode == 0, f"ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"
            assert "ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ" in result.stdout, "ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "ì„±ê³µë¥ " in result.stdout, "ì„±ê³µë¥  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "ì‹¤íŒ¨ìœ¨" in result.stdout, "ì‹¤íŒ¨ìœ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail(f"ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤ (ë§¤ê°œë³€ìˆ˜: {fail_rate}%, {flaky_rate}%, {runs}íšŒ)")

    def test_simulation_json_output(self, temp_workspace):
        """ì‹œë®¬ë ˆì´ì…˜ JSON ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/ci_stability_sim.sh")
        if not script_path.exists():
            pytest.skip("ci_stability_sim.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        output_file = temp_workspace / "simulation_result.json"

        try:
            result = subprocess.run(
                [
                    "bash", str(script_path),
                    "--fail-rate", "10",
                    "--flaky-rate", "5",
                    "--runs", "20",
                    "--json",
                    "--output", str(output_file)
                ],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS,
                cwd=temp_workspace
            )

            assert result.returncode == 0, f"JSON ì¶œë ¥ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"
            assert output_file.exists(), "JSON ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

            # JSON íŒŒì¼ íŒŒì‹± ë° êµ¬ì¡° ê²€ì¦
            with open(output_file, 'r', encoding='utf-8') as f:
                json_data = json.load(f)

            # JSON êµ¬ì¡° ê²€ì¦
            required_fields = [
                "simulation_config",
                "results",
                "execution_stats",
                "generated_at"
            ]

            for field in required_fields:
                assert field in json_data, f"JSON ì¶œë ¥ì— í•„ìˆ˜ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤: {field}"

            # ì‹œë®¬ë ˆì´ì…˜ ì„¤ì • ê²€ì¦
            config = json_data["simulation_config"]
            assert config["fail_rate_target"] == 10, "ì‹¤íŒ¨ìœ¨ ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤"
            assert config["flaky_rate_target"] == 5, "í”Œë˜í‚¤ìœ¨ ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤"
            assert config["total_runs"] == 20, "ì‹¤í–‰ íšŸìˆ˜ ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤"

            # ê²°ê³¼ ê²€ì¦
            results = json_data["results"]
            assert "successful_runs" in results, "ì„±ê³µ ì‹¤í–‰ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "failed_runs" in results, "ì‹¤íŒ¨ ì‹¤í–‰ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "flaky_runs" in results, "í”Œë˜í‚¤ ì‹¤í–‰ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "success_rate" in results, "ì„±ê³µë¥ ì´ ì—†ìŠµë‹ˆë‹¤"

            # ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦
            total_runs = results["successful_runs"] + results["failed_runs"]
            assert total_runs <= 20, "ì „ì²´ ì‹¤í–‰ ìˆ˜ê°€ ì„¤ì •ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail("JSON ì¶œë ¥ ì‹œë®¬ë ˆì´ì…˜ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")
        except json.JSONDecodeError as e:
            pytest.fail(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")

    def test_simulation_markdown_output(self, temp_workspace):
        """ì‹œë®¬ë ˆì´ì…˜ Markdown ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/ci_stability_sim.sh")
        if not script_path.exists():
            pytest.skip("ci_stability_sim.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        output_file = temp_workspace / "simulation_result.md"

        try:
            result = subprocess.run(
                [
                    "bash", str(script_path),
                    "--fail-rate", "15",
                    "--flaky-rate", "8",
                    "--runs", "25",
                    "--markdown",
                    "--output", str(output_file)
                ],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS,
                cwd=temp_workspace
            )

            assert result.returncode == 0, f"Markdown ì¶œë ¥ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"
            assert output_file.exists(), "Markdown ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

            # Markdown íŒŒì¼ ë‚´ìš© ê²€ì¦
            with open(output_file, 'r', encoding='utf-8') as f:
                markdown_content = f.read()

            # Markdown êµ¬ì¡° ê²€ì¦
            required_sections = [
                "# ğŸ§ª CI ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼",
                "## ğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •",
                "## ğŸ“ˆ ì‹¤í–‰ ê²°ê³¼",
                "## ğŸ¯ ì„±ëŠ¥ ë©”íŠ¸ë¦­",
                "## ğŸ’¡ ê¶Œì¥ì‚¬í•­"
            ]

            for section in required_sections:
                assert section in markdown_content, f"Markdownì— í•„ìˆ˜ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤: {section}"

            # í…Œì´ë¸” í˜•ì‹ ê²€ì¦
            assert "|" in markdown_content, "Markdown í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤"
            assert "---" in markdown_content, "Markdown í…Œì´ë¸” êµ¬ë¶„ìê°€ ì—†ìŠµë‹ˆë‹¤"

            # ì„¤ì •ê°’ ê²€ì¦
            assert "15%" in markdown_content, "ì‹¤íŒ¨ìœ¨ ì„¤ì •ì´ Markdownì— ì—†ìŠµë‹ˆë‹¤"
            assert "8%" in markdown_content, "í”Œë˜í‚¤ìœ¨ ì„¤ì •ì´ Markdownì— ì—†ìŠµë‹ˆë‹¤"
            assert "25íšŒ" in markdown_content, "ì‹¤í–‰ íšŸìˆ˜ê°€ Markdownì— ì—†ìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail("Markdown ì¶œë ¥ ì‹œë®¬ë ˆì´ì…˜ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

    def test_simulation_parameter_validation(self):
        """ì‹œë®¬ë ˆì´ì…˜ ë§¤ê°œë³€ìˆ˜ ìœ íš¨ì„± ê²€ì‚¬ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/ci_stability_sim.sh")
        if not script_path.exists():
            pytest.skip("ci_stability_sim.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        # ì˜ëª»ëœ ë§¤ê°œë³€ìˆ˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        invalid_cases = [
            (["--fail-rate", "-5"], "ìŒìˆ˜ ì‹¤íŒ¨ìœ¨"),
            (["--fail-rate", "150"], "100% ì´ˆê³¼ ì‹¤íŒ¨ìœ¨"),
            (["--flaky-rate", "-2"], "ìŒìˆ˜ í”Œë˜í‚¤ìœ¨"),
            (["--flaky-rate", "120"], "100% ì´ˆê³¼ í”Œë˜í‚¤ìœ¨"),
            (["--runs", "0"], "0íšŒ ì‹¤í–‰"),
            (["--runs", "-10"], "ìŒìˆ˜ ì‹¤í–‰ íšŸìˆ˜"),
            (["--fail-rate", "60", "--flaky-rate", "50"], "í•©ê³„ 100% ì´ˆê³¼"),
        ]

        for invalid_params, description in invalid_cases:
            try:
                result = subprocess.run(
                    ["bash", str(script_path)] + invalid_params,
                    capture_output=True,
                    text=True,
                    timeout=TIMEOUT_SECONDS
                )

                assert result.returncode != 0, f"{description} ì¼€ì´ìŠ¤ì—ì„œ ì˜¤ë¥˜ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
                assert "ì˜¤ë¥˜" in result.stderr or "error" in result.stderr.lower(), f"{description} ì¼€ì´ìŠ¤ì—ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤"

            except subprocess.TimeoutExpired:
                pytest.fail(f"{description} ìœ íš¨ì„± ê²€ì‚¬ê°€ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

    def test_simulation_statistical_accuracy(self, temp_workspace):
        """ì‹œë®¬ë ˆì´ì…˜ í†µê³„ì  ì •í™•ì„± í…ŒìŠ¤íŠ¸ (ì¶©ë¶„í•œ ì‹¤í–‰ íšŸìˆ˜ë¡œ)"""
        script_path = Path("scripts/ci_stability_sim.sh")
        if not script_path.exists():
            pytest.skip("ci_stability_sim.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        # í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” í…ŒìŠ¤íŠ¸ (ë” ë§ì€ ì‹¤í–‰ íšŸìˆ˜)
        fail_rate = 20
        flaky_rate = 10
        runs = 100  # ì¶©ë¶„í•œ í‘œë³¸ í¬ê¸°

        output_file = temp_workspace / "statistical_test.json"

        try:
            result = subprocess.run(
                [
                    "bash", str(script_path),
                    "--fail-rate", str(fail_rate),
                    "--flaky-rate", str(flaky_rate),
                    "--runs", str(runs),
                    "--json",
                    "--output", str(output_file)
                ],
                capture_output=True,
                text=True,
                timeout=60  # ë” ê¸´ íƒ€ì„ì•„ì›ƒ
            )

            assert result.returncode == 0, f"í†µê³„ í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"

            with open(output_file, 'r', encoding='utf-8') as f:
                json_data = json.load(f)

            results = json_data["results"]
            actual_fail_rate = results["actual_fail_rate"]
            flaky_reproduce_rate = results["flaky_reproduce_rate"]

            # í†µê³„ì  í—ˆìš© ì˜¤ì°¨ (í‘œë³¸ í¬ê¸°ë¥¼ ê³ ë ¤í•œ ì˜¤ì°¨ ë²”ìœ„)
            # ì‹¤ì œë¡œëŠ” ëœë¤ ì‹œë®¬ë ˆì´ì…˜ì´ë¯€ë¡œ ì–´ëŠ ì •ë„ ì˜¤ì°¨ëŠ” í—ˆìš©
            fail_rate_tolerance = 15  # 15% ì˜¤ì°¨ í—ˆìš©
            flaky_rate_tolerance = 20  # 20% ì˜¤ì°¨ í—ˆìš©

            fail_rate_diff = abs(actual_fail_rate - fail_rate)
            flaky_rate_diff = abs(flaky_reproduce_rate - flaky_rate)

            # ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šì€ ê²€ì¦ (ì‹œë®¬ë ˆì´ì…˜ì˜ ëœë¤ì„± ê³ ë ¤)
            assert fail_rate_diff <= fail_rate_tolerance, \
                f"ì‹¤íŒ¨ìœ¨ ì˜¤ì°¨ê°€ í—ˆìš© ë²”ìœ„ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤: ì˜ˆìƒ {fail_rate}%, ì‹¤ì œ {actual_fail_rate}%, ì˜¤ì°¨ {fail_rate_diff}%"

            assert flaky_rate_diff <= flaky_rate_tolerance, \
                f"í”Œë˜í‚¤ìœ¨ ì˜¤ì°¨ê°€ í—ˆìš© ë²”ìœ„ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤: ì˜ˆìƒ {flaky_rate}%, ì‹¤ì œ {flaky_reproduce_rate}%, ì˜¤ì°¨ {flaky_rate_diff}%"

        except subprocess.TimeoutExpired:
            pytest.fail("í†µê³„ ì •í™•ì„± í…ŒìŠ¤íŠ¸ê°€ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")


class TestRunbookValidator:
    """scripts/runbook_validator.sh ëŸ°ë¶ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def temp_workspace(self):
        """ì„ì‹œ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì„¤ì •"""
        with tempfile.TemporaryDirectory() as temp_dir:
            workspace = Path(temp_dir)
            yield workspace

    def test_validator_script_existence_and_permissions(self):
        """ëŸ°ë¶ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì¡´ì¬ ë° ì‹¤í–‰ ê¶Œí•œ í™•ì¸"""
        script_path = Path("scripts/runbook_validator.sh")
        assert script_path.exists(), "runbook_validator.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
        assert script_path.is_file(), "runbook_validator.shê°€ íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤"

        # ì‹¤í–‰ ê¶Œí•œ í™•ì¸ (Unix ì‹œìŠ¤í…œì—ì„œë§Œ)
        if os.name == 'posix':
            assert os.access(script_path, os.X_OK), "runbook_validator.shì— ì‹¤í–‰ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤"

    def test_validator_help_option(self):
        """ëŸ°ë¶ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ë„ì›€ë§ ì˜µì…˜ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/runbook_validator.sh")
        if not script_path.exists():
            pytest.skip("runbook_validator.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        try:
            result = subprocess.run(
                ["bash", str(script_path), "--help"],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            assert result.returncode == 0, f"ë„ì›€ë§ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"
            assert "ì‚¬ìš©ë²•" in result.stdout or "Usage" in result.stdout, "ë„ì›€ë§ì— ì‚¬ìš©ë²•ì´ ì—†ìŠµë‹ˆë‹¤"
            assert "ëŸ°ë¶ ì‹œìŠ¤í…œ ê²€ì¦" in result.stdout, "ë„ì›€ë§ì— ìŠ¤í¬ë¦½íŠ¸ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail("ë„ì›€ë§ ì‹¤í–‰ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

    def test_validator_required_files_check(self):
        """ëŸ°ë¶ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ì˜ í•„ìˆ˜ íŒŒì¼ í™•ì¸ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/runbook_validator.sh")
        if not script_path.exists():
            pytest.skip("runbook_validator.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        # í•„ìˆ˜ íŒŒì¼ë“¤ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        required_files = [
            "mcp/utils/runbook.py",
            "scripts/ci_autoremediate.sh",
            "scripts/hooks"  # ë””ë ‰í† ë¦¬
        ]

        missing_files = []
        for file_path in required_files:
            path = Path(file_path)
            if not path.exists():
                missing_files.append(file_path)

        if missing_files:
            pytest.skip(f"í•„ìˆ˜ íŒŒì¼ì´ ì—†ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {missing_files}")

        # ëŸ°ë¶ ê²€ì¦ ì‹¤í–‰
        try:
            result = subprocess.run(
                ["bash", str(script_path), "--verbose"],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            # ê²€ì¦ ìì²´ëŠ” ì„±ê³µí•˜ê±°ë‚˜ ì‹¤íŒ¨í•  ìˆ˜ ìˆì§€ë§Œ, ìŠ¤í¬ë¦½íŠ¸ëŠ” ì •ìƒ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
            assert result.returncode in [0, 1], f"ëŸ°ë¶ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {result.stderr}"
            assert "ëŸ°ë¶ ì‹œìŠ¤í…œ" in result.stdout, "ëŸ°ë¶ ê²€ì¦ ê²°ê³¼ì— ì‹œìŠ¤í…œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail("ëŸ°ë¶ ê²€ì¦ ì‹¤í–‰ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

    def test_validator_json_output(self, temp_workspace):
        """ëŸ°ë¶ ê²€ì¦ JSON ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/runbook_validator.sh")
        if not script_path.exists():
            pytest.skip("runbook_validator.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
        required_files = ["mcp/utils/runbook.py", "scripts/ci_autoremediate.sh"]
        for file_path in required_files:
            if not Path(file_path).exists():
                pytest.skip(f"í•„ìˆ˜ íŒŒì¼ì´ ì—†ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {file_path}")

        output_file = temp_workspace / "validation_result.json"

        try:
            result = subprocess.run(
                [
                    "bash", str(script_path),
                    "--json",
                    "--output", str(output_file)
                ],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            # JSON ì¶œë ¥ì€ ê²€ì¦ ì„±ê³µ/ì‹¤íŒ¨ì™€ ê´€ê³„ì—†ì´ ì •ìƒ ë™ì‘í•´ì•¼ í•¨
            assert result.returncode in [0, 1], f"JSON ì¶œë ¥ ëŸ°ë¶ ê²€ì¦ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"
            assert output_file.exists(), "JSON ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

            # JSON íŒŒì¼ íŒŒì‹± ë° êµ¬ì¡° ê²€ì¦
            with open(output_file, 'r', encoding='utf-8') as f:
                json_data = json.load(f)

            # JSON êµ¬ì¡° ê²€ì¦
            required_fields = [
                "validation_status",
                "timestamp",
                "summary",
                "validation_results"
            ]

            for field in required_fields:
                assert field in json_data, f"JSON ì¶œë ¥ì— í•„ìˆ˜ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤: {field}"

            # ê²€ì¦ ìƒíƒœ í™•ì¸
            validation_status = json_data["validation_status"]
            assert validation_status in ["PASSED", "FAILED"], f"ì˜ëª»ëœ ê²€ì¦ ìƒíƒœ: {validation_status}"

            # ìš”ì•½ ì •ë³´ í™•ì¸
            summary = json_data["summary"]
            assert "total_runbook_templates" in summary, "ëŸ°ë¶ í…œí”Œë¦¿ ìˆ˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"
            assert "issues_found" in summary, "ë°œê²¬ëœ ë¬¸ì œ ìˆ˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail("JSON ì¶œë ¥ ëŸ°ë¶ ê²€ì¦ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")
        except json.JSONDecodeError as e:
            pytest.fail(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")

    def test_validator_text_output_format(self):
        """ëŸ°ë¶ ê²€ì¦ í…ìŠ¤íŠ¸ ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/runbook_validator.sh")
        if not script_path.exists():
            pytest.skip("runbook_validator.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not Path("mcp/utils/runbook.py").exists():
            pytest.skip("mcp/utils/runbook.pyê°€ ì—†ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")

        try:
            result = subprocess.run(
                ["bash", str(script_path), "--verbose"],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            # í…ìŠ¤íŠ¸ ì¶œë ¥ì€ ì„±ê³µ/ì‹¤íŒ¨ì™€ ê´€ê³„ì—†ì´ ì •ìƒ ë™ì‘í•´ì•¼ í•¨
            assert result.returncode in [0, 1], f"í…ìŠ¤íŠ¸ ì¶œë ¥ ëŸ°ë¶ ê²€ì¦ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}"

            # í…ìŠ¤íŠ¸ ì¶œë ¥ êµ¬ì¡° ê²€ì¦
            required_sections = [
                "ëŸ°ë¶ ì‹œìŠ¤í…œ ê²€ì¦ ê²°ê³¼",
                "ê²€ì¦ ìš”ì•½",
                "ê²€ì¦ ìƒíƒœ"
            ]

            for section in required_sections:
                assert section in result.stdout, f"í…ìŠ¤íŠ¸ ì¶œë ¥ì— í•„ìˆ˜ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤: {section}"

            # ê²€ì¦ ìƒíƒœ í™•ì¸
            assert ("í†µê³¼" in result.stdout or "ì‹¤íŒ¨" in result.stdout), "ê²€ì¦ ìƒíƒœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail("í…ìŠ¤íŠ¸ ì¶œë ¥ ëŸ°ë¶ ê²€ì¦ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

    def test_validator_runbook_template_detection(self):
        """ëŸ°ë¶ í…œí”Œë¦¿ ê°ì§€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/runbook_validator.sh")
        runbook_module = Path("mcp/utils/runbook.py")

        if not script_path.exists():
            pytest.skip("runbook_validator.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
        if not runbook_module.exists():
            pytest.skip("mcp/utils/runbook.pyê°€ ì—†ìŠµë‹ˆë‹¤")

        # ëŸ°ë¶ ëª¨ë“ˆì—ì„œ ì‹¤ì œ í…œí”Œë¦¿ í™•ì¸
        try:
            import sys
            sys.path.insert(0, str(Path("mcp").absolute()))
            from utils.runbook import RUNBOOK_TEMPLATES

            expected_templates = list(RUNBOOK_TEMPLATES.keys())
            assert len(expected_templates) > 0, "ëŸ°ë¶ í…œí”Œë¦¿ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        except ImportError:
            pytest.skip("ëŸ°ë¶ ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰í•˜ì—¬ í…œí”Œë¦¿ ê°ì§€ í™•ì¸
        try:
            result = subprocess.run(
                ["bash", str(script_path), "--json"],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            # ê²€ì¦ ê²°ê³¼ íŒŒì‹±
            if result.stdout.strip():
                try:
                    json_data = json.loads(result.stdout)
                    detected_templates = json_data.get("runbook_templates", [])

                    # ìµœì†Œí•œì˜ í•„ìˆ˜ í…œí”Œë¦¿ë“¤ì´ ê°ì§€ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    essential_templates = [
                        "dependency_install_failed",
                        "test_timeout",
                        "build_timeout"
                    ]

                    for template in essential_templates:
                        if template in expected_templates:
                            assert template in detected_templates, f"í•„ìˆ˜ í…œí”Œë¦¿ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {template}"

                except json.JSONDecodeError:
                    pytest.skip("JSON ì¶œë ¥ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

        except subprocess.TimeoutExpired:
            pytest.fail("ëŸ°ë¶ í…œí”Œë¦¿ ê°ì§€ í…ŒìŠ¤íŠ¸ê°€ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

    def test_validator_error_mapping_detection(self):
        """ì—ëŸ¬ ë§¤í•‘ ê°ì§€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        script_path = Path("scripts/runbook_validator.sh")
        autoremediate_script = Path("scripts/ci_autoremediate.sh")

        if not script_path.exists():
            pytest.skip("runbook_validator.sh ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
        if not autoremediate_script.exists():
            pytest.skip("scripts/ci_autoremediate.shê°€ ì—†ìŠµë‹ˆë‹¤")

        # ìë™ ì™„í™” ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì—ëŸ¬ ë§¤í•‘ í™•ì¸
        try:
            with open(autoremediate_script, 'r', encoding='utf-8') as f:
                script_content = f.read()

            # ERROR_TO_HOOK_MAPì´ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            assert "ERROR_TO_HOOK_MAP" in script_content, "ERROR_TO_HOOK_MAPì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        except UnicodeDecodeError:
            pytest.skip("ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë§¤í•‘ ê°ì§€ í™•ì¸
        try:
            result = subprocess.run(
                ["bash", str(script_path), "--json"],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            if result.stdout.strip():
                try:
                    json_data = json.loads(result.stdout)
                    hook_error_mappings = json_data.get("hook_error_mappings", [])

                    # ë§¤í•‘ì´ ê°ì§€ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë¹ˆ ë°°ì—´ë„ ìœ íš¨)
                    assert isinstance(hook_error_mappings, list), "í›…-ì—ëŸ¬ ë§¤í•‘ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤"

                except json.JSONDecodeError:
                    pytest.skip("JSON ì¶œë ¥ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

        except subprocess.TimeoutExpired:
            pytest.fail("ì—ëŸ¬ ë§¤í•‘ ê°ì§€ í…ŒìŠ¤íŠ¸ê°€ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")


class TestSystemIntegration:
    """ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜ + ëŸ°ë¶ ê²€ì¦)"""

    def test_both_scripts_coexistence(self):
        """ë‘ ìŠ¤í¬ë¦½íŠ¸ì˜ ê³µì¡´ì„± ë° ìƒí˜¸ ì˜ì¡´ì„± í…ŒìŠ¤íŠ¸"""
        sim_script = Path("scripts/ci_stability_sim.sh")
        validator_script = Path("scripts/runbook_validator.sh")

        # ë‘ ìŠ¤í¬ë¦½íŠ¸ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        scripts_status = {
            "ci_stability_sim.sh": sim_script.exists(),
            "runbook_validator.sh": validator_script.exists()
        }

        missing_scripts = [name for name, exists in scripts_status.items() if not exists]

        if missing_scripts:
            pytest.skip(f"ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ì–´ í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {missing_scripts}")

        # ë‘ ìŠ¤í¬ë¦½íŠ¸ê°€ ëª¨ë‘ ì •ìƒ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸
        try:
            # CI ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜ ë„ì›€ë§
            sim_result = subprocess.run(
                ["bash", str(sim_script), "--help"],
                capture_output=True,
                text=True,
                timeout=10
            )
            assert sim_result.returncode == 0, "CI ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ë„ì›€ë§ ì‹¤í–‰ ì‹¤íŒ¨"

            # ëŸ°ë¶ ê²€ì¦ ë„ì›€ë§
            validator_result = subprocess.run(
                ["bash", str(validator_script), "--help"],
                capture_output=True,
                text=True,
                timeout=10
            )
            assert validator_result.returncode == 0, "ëŸ°ë¶ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ë„ì›€ë§ ì‹¤í–‰ ì‹¤íŒ¨"

        except subprocess.TimeoutExpired:
            pytest.fail("í†µí•© í…ŒìŠ¤íŠ¸ì—ì„œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

    def test_json_output_consistency(self, temp_workspace=None):
        """ë‘ ìŠ¤í¬ë¦½íŠ¸ì˜ JSON ì¶œë ¥ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
        if temp_workspace is None:
            temp_workspace = Path(tempfile.mkdtemp())

        sim_script = Path("scripts/ci_stability_sim.sh")
        validator_script = Path("scripts/runbook_validator.sh")

        if not sim_script.exists() or not validator_script.exists():
            pytest.skip("ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ì–´ JSON ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")

        sim_output = temp_workspace / "sim_output.json"
        validator_output = temp_workspace / "validator_output.json"

        try:
            # CI ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜ JSON ì¶œë ¥
            sim_result = subprocess.run(
                [
                    "bash", str(sim_script),
                    "--fail-rate", "10",
                    "--flaky-rate", "5",
                    "--runs", "20",
                    "--json",
                    "--output", str(sim_output)
                ],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            # ëŸ°ë¶ ê²€ì¦ JSON ì¶œë ¥
            validator_result = subprocess.run(
                [
                    "bash", str(validator_script),
                    "--json",
                    "--output", str(validator_output)
                ],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            # ë‘ JSON íŒŒì¼ì´ ëª¨ë‘ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if sim_result.returncode == 0:
                assert sim_output.exists(), "ì‹œë®¬ë ˆì´ì…˜ JSON ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

                with open(sim_output, 'r', encoding='utf-8') as f:
                    sim_json = json.load(f)

                # JSON êµ¬ì¡° ê¸°ë³¸ ê²€ì¦
                assert "generated_at" in sim_json or "timestamp" in sim_json, "ì‹œë®¬ë ˆì´ì…˜ JSONì— íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìŠµë‹ˆë‹¤"

            if validator_result.returncode in [0, 1]:  # ê²€ì¦ì€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ
                if validator_output.exists():
                    with open(validator_output, 'r', encoding='utf-8') as f:
                        validator_json = json.load(f)

                    # JSON êµ¬ì¡° ê¸°ë³¸ ê²€ì¦
                    assert "timestamp" in validator_json, "ê²€ì¦ JSONì— íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìŠµë‹ˆë‹¤"

        except subprocess.TimeoutExpired:
            pytest.fail("JSON ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ì—ì„œ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
        except json.JSONDecodeError as e:
            pytest.fail(f"JSON ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ì—ì„œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if temp_workspace != Path(tempfile.mkdtemp()):
                shutil.rmtree(temp_workspace, ignore_errors=True)

    def test_comprehensive_workflow(self):
        """í¬ê´„ì ì¸ ì›Œí¬í”Œë¡œ í…ŒìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜ â†’ ê²€ì¦)"""
        sim_script = Path("scripts/ci_stability_sim.sh")
        validator_script = Path("scripts/runbook_validator.sh")

        if not sim_script.exists() or not validator_script.exists():
            pytest.skip("í¬ê´„ì  ì›Œí¬í”Œë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

        # 1ë‹¨ê³„: ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        try:
            sim_result = subprocess.run(
                [
                    "bash", str(sim_script),
                    "--fail-rate", "20",
                    "--flaky-rate", "10",
                    "--runs", "15",
                    "--dry-run"  # ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ ë“œë¼ì´ëŸ° ì‚¬ìš©
                ],
                capture_output=True,
                text=True,
                timeout=15
            )

            assert sim_result.returncode == 0, f"1ë‹¨ê³„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤íŒ¨: {sim_result.stderr}"

        except subprocess.TimeoutExpired:
            pytest.fail("1ë‹¨ê³„ ì‹œë®¬ë ˆì´ì…˜ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

        # 2ë‹¨ê³„: ëŸ°ë¶ ê²€ì¦ ì‹¤í–‰
        try:
            validator_result = subprocess.run(
                ["bash", str(validator_script)],
                capture_output=True,
                text=True,
                timeout=TIMEOUT_SECONDS
            )

            # ê²€ì¦ì€ ì„±ê³µí•˜ê±°ë‚˜ ì‹¤íŒ¨í•  ìˆ˜ ìˆì§€ë§Œ, ìŠ¤í¬ë¦½íŠ¸ëŠ” ì •ìƒ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
            assert validator_result.returncode in [0, 1], f"2ë‹¨ê³„ ëŸ°ë¶ ê²€ì¦ ì˜¤ë¥˜: {validator_result.stderr}"

        except subprocess.TimeoutExpired:
            pytest.fail("2ë‹¨ê³„ ëŸ°ë¶ ê²€ì¦ì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤")

        # 3ë‹¨ê³„: í†µí•© ê²°ê³¼ í™•ì¸
        # ë‘ ë‹¨ê³„ê°€ ëª¨ë‘ ì™„ë£Œë˜ë©´ ì„±ê³µ
        # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŸ°ë¶ ì‹œìŠ¤í…œì„ ê°œì„ í•˜ëŠ” í”¼ë“œë°± ë£¨í”„ê°€ ìˆì„ ê²ƒì„


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ pytest ì„¤ì •
if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])